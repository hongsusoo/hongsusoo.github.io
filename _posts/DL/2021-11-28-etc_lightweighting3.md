---
defaults:
  - scope:
      path: ""
      type: posts
    values:
      layout: single
      author_profile: true
      comments: true
      share: true
      related: true

title: "ê²½ëŸ‰í™” ë°©ë²• : Pruning"
excerpt: "about : ê²½ëŸ‰í™”"
toc: true
toc_sticky: true
toc_label: "Label"
categories:
  - DL etc
tags:
  - [ê²½ëŸ‰í™”, pruning]
date: 2021-11-28
last_modified_at: 2021-11-28
---

<br>

# ê²½ëŸ‰í™” - Pruning

- Pruning : ì¤‘ìš”ë„ê°€ ë‚®ì€ íŒŒë¼ë¯¸í„°ë¥¼ ì œê±°í•˜ëŠ” ë°©ë²•

## Pruning ì¢…ë¥˜

- Pruning ë‹¨ìœ„ì— ë”°ë¼ í¬ê²Œ ë‘ ê°€ì§€(Structured, UnStructured)ë¡œ êµ¬ë¶„ ê°€ëŠ¥í•¨
- Structured : Group ë‹¨ìœ„
- UnStructured : fine grained ë‹¨ìœ„

### Structured Pruning
 
- íŒŒë¼ë¯¸í„°ë¥¼ ê·¸ë£¹ ë‹¨ìœ„ë¡œ Pruning ì§„í–‰
- ê·¸ë£¹ ë‹¨ìœ„ : Channel, Filter, Layer Level ë“±ìœ¼ë¡œ ì •ì˜í•˜ì—¬ ì§„í–‰ ê°€ëŠ¥í•¨
- Masked (0ìœ¼ë¡œ Pruning ëœ)ëœ ê·¸ë£¹ìœ¼ë¡œ ì‹¤ì§ˆì ì¸ ì—°ì‚° íšŸìˆ˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆì–´ ì†ë„ í–¥ìƒì— ì§ì ‘ì ì¸ ì˜í–¥ì´ ìˆìŒ

#### **ğŸ”ê´€ë ¨ ë…¼ë¬¸**

1. Learning Efficient Convolutional Networks Through Network Slimming(ICCV 2017)
    - ë‹¨ìœ„ : Structured
    - ê¸°ì¤€ : BatchNorm Scaling Factor $\gamma$
    - ì ìš© : Global
    - Phase : í•™ìŠµëœ ëª¨ë¸
    - ê° filterì— BNì˜ Scaling Factor $\gamma$ê°€ ê³±í•´ì§€ê¸° ë•Œë¬¸ì—, Scaling Factor $\gamma$ê°€ ì»¤ì§€ë©´, filter weight í¬ê¸°ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì»¤ì§ˆ ê²ƒìœ¼ë¡œ ì˜ˆìƒ
    - Scaling Factor $\gamma$ í¬ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ p%ì˜ ì±„ë„ì„ pruning ì§„í–‰
    - Scaling Factor $\gamma$ì— L1-normì„ Regularizeë¡œ ì‚¬ìš©í•˜ì—¬ Insignificant Channelë“¤ì„ ìì—°ìŠ¤ëŸ½ê²Œ Prune ë˜ë„ë¡ ìœ ë„í•¨
    - ImageNet ê¸°ì¤€, VGG11,  50% Pruning, FlOPS 30.4%, íŒŒë¼ë¯¸í„° ìˆ˜ 82.5% ê°ì†Œ
    
<img src="https://user-images.githubusercontent.com/77658029/143731866-35e7121f-7509-4e5c-93bf-3312c9f60878.png"  width="70%" height="70%"/>
<img src="https://user-images.githubusercontent.com/77658029/143732381-50dc4a96-1a9c-46f8-9a7b-da903de94e8d.png"  width="50%" height="50%"/>

2. HRank:Filter Pruning using High-Rank Feature Map(CVPR 2020)
    - ë‹¨ìœ„ : Structured
    - ê¸°ì¤€ : Feature map filter SVD Rank
    - ì ìš© : Local
    - Phase : í•™ìŠµëœ ëª¨ë¸
    - Weightê°€ ì•„ë‹Œ Feature map outputì„ ë³´ì
    - Feature map outputì˜ ê° Filter SVDë¥¼ ì ìš©í•˜ì—¬ Rankë¥¼ ê³„ì‚°í•¨
    - **But, Feature map outputì€ ì´ë¯¸ì§€ì— ë”°ë¼ ë‹¬ë¼ì§€ì§€ ì•Šë‚˜?**
    - í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” **Randomí•œ imageë“¤ì˜ Feature map output**ì„ ê³„ì‚°í•˜ì—¬ Rankë¥¼ êµ¬í•´ë³´ë‹ˆ ì‹¤í—˜ì ìœ¼ë¡œ **ì°¨ì´ê°€ ì—†ìŒ**ì„ í™•ì¸í•¨    
    - ResNet50, ImageNet ê¸°ì¤€ ì•½ 44%ì˜ FLOPS ê°ì†Œ, 1.17% ì„±ëŠ¥ í•˜ë½
    <img src="https://user-images.githubusercontent.com/77658029/143770018-0334d6a9-b69a-43c5-b748-f9f9b25e7e2e.png"  width="70%" height="70%"/>
    - ê³„ì‚°ì€ torchì— matrix rank ê³„ì‚° í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ì‚¬ìš© ê°€ëŠ¥í•¨
    
### UnStructured Pruning

- Parameter í•˜ë‚˜í•˜ë‚˜ë¥¼ ë…ë¦½ì ìœ¼ë¡œ Pruning ì§„í–‰
- í–‰ë ¬ì´ ì ì  Sparse í•´ì§€ê²Œ ë˜ì§€ë§Œ Filterì˜ ìˆ˜ê°€ ì—†ì–´ì§€ëŠ”ê²Œ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ì‹¤ì§ˆì ì¸ ì†ë„ê°€ ê³„ì„ ë˜ì§„ ì•ŠìŒ
- í•˜ì§€ë§Œ Sparse Representationì— ëŒ€í•´ì„œ ì—°ì‚° ê°€ì†ì´ ê°€ëŠ¥í•˜ë©´ ì†ë„ê°€ ê³„ì„ ë  ìˆ˜ë„ ìˆìŒ
<img src="https://user-images.githubusercontent.com/77658029/143770659-54e61a09-1726-4c30-8400-389e43520fce.png"  width="70%" height="70%"/>

#### **ğŸ”ê´€ë ¨ ë…¼ë¬¸**

1. The Lottery Ticket Hypothesis : Finding Sparse, Trainable Neural Networks(ICLR 2019)
    - ë‹¨ìœ„ : Unstructured(fine grained)
    - ê¸°ì¤€ : L1 Norm
    - ì ìš© : Global
    - Phase : í•™ìŠµëœ ëª¨ë¸
    - ë°©ë²• : ì•½ê°„ì˜ Prune â†’ ì¬í•™ìŠµ â†’ ì•½ê°„ì˜ Prune â†’ ì¬í•™ìŠµ ...(iterative pruning)
    - Pruneëœ Sparseí•œ ëª¨ë¸ë“¤ì€ ì¼ë°˜ì ìœ¼ë¡œ í•™ìŠµì´ ì–´ë ¤ì›€
    - Lottery Ticket Hypothesis : Denseí•˜ê³ , Randomly-initializedëœ feed-forward Netì€ ê¸°ì¡´ original Networkì™€ í•„ì í•˜ëŠ” ì„±ëŠ¥ì„ ê°–ëŠ” sub-networks(winning tickets)ì„ ê°–ëŠ”ë‹¤ëŠ” ê°€ì •(Pruning-obtained networkë“¤ ì¤‘ original networkë§Œí¼ í•™ìŠµì´ ì˜ë˜ê³ , ë†’ì€ accuracyë¥¼ ê°–ëŠ” sub-networksê°€ ìˆì„ ê²ƒì„ ê°€ì •)
    - ì´ëŸ° Lottery Ticket(sub network)ë¥¼ ì°¾ê¸° ìœ„í•œ ë°©ë²•ë¡ ì„ ì œì‹œí•˜ì˜€ê³ , 10~20%ì˜ Weightë§Œìœ¼ë¡œ ì› networkì™€ ë™ì¼ ì„±ëŠ¥ì„ ëƒ„
    - Identifying Winning Tickets êµ¬í•˜ëŠ” ë°©ë²•
        1. ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” $f(x;\theta_{0})$
        2. ë„¤íŠ¸ì›Œí¬ë¥¼ í•™ìŠµí•˜ì—¬ parameter $\theta$ë¥¼ êµ¬í•¨(jë²ˆì§¸ í•™ìŠµí•˜ë©´ $\theta_{j}$)
        3. parameter $\theta_{j}$ë¥¼ $p\%$ ë§Œí¼ pruning ì§„í–‰ â†’ mask $m$ë¥¼ ìƒì„±(pëŠ” ë³´í†µ 20%)
        4. ì‚´ì•„ë‚¨ì€ Parameter(Maskë˜ì§€ ì•Šì€ ì• ë“¤)ë¥¼ ì´ˆê¸° Parameter($\theta_{0}$)ë¡œ ë˜ëŒë¦¼, ì´ë•Œ ë‚˜ì˜¨ ê²°ê³¼ë¥¼ Winning Ticket($f(x;m\odot\theta_{0})$, maskëœ ì˜ì—­ì€ 0ìœ¼ë¡œ ë‚˜ë¨¸ì§€ëŠ” $\theta_{0}$(ì´ˆê¸°í™”)ë¡œ ì •ì˜í•¨)ì´ë¼ê³  í•¨
        5. Target Sparsity(e.g. 90%)ì— ë„ë‹¬í• ë•Œê¹Œì§€ B - D ë°˜ë³µ ì§„í–‰(10íšŒ ì§„í–‰í•  ê²½ìš° $(1-0.2)^{10} \approx 10.7\%$)

    - **ê°„ë‹¨í•œ Ideaë¡œ ì¢‹ì€ ì„±ëŠ¥**ì„ ë½‘ì•„ë‚´ê³ , Pruning í›„ Fine tuningí•˜ëŠ” ë°©ì‹ì„ ê³ ì§‘í•˜ë˜ ê¸°ì¡´ í‹€ì„ ê¹¨ê³  **ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„**ì„ ì œì‹œí•¨
    - **í•˜ì§€ë§Œ**, **ë§¤ìš° ëŠë¦° í•™ìŠµì‹œê°„**

2. Stabilizing the Lottery Ticket Hypothesis(arXiv 2019) ; Weight Rewinding 
    - LTH(Lottery Ticket Hypothesis)ì˜ ê²½ìš° ë°ì´í„°ì…‹ì´ë‚˜, ë„¤íŠ¸ì›Œí¬ì˜ í¬ê¸°ê°€ ì»¤ì¡Œì„ë•Œ ë¶ˆì•ˆì •í•œ ëª¨ìŠµì´ ìˆì—ˆìŒ
    - ê·¸ë˜ì„œ ê¸°ì¡´ LTHì—ì„œ Parameterë¥¼ **ì´ˆê¸°í™” í•˜ì—¬ ì¬í•™ìŠµí•˜ëŠ” ê²ƒì´ ì•„ë‹Œ** **kë²ˆì§¸ Epochì˜ í•™ìŠµ Parameterë¥¼ ì‚¬ìš©**í•˜ì—¬ ì•ˆì •í™” ì‹œí‚´
    - ë…¼ë¬¸ì—ì„œëŠ” ì´ Iterationì˜ 0.1~7% ì§€ì ì„ Rewinding pointë¡œ ì–¸ê¸‰í•¨
    - Weightì™€ Learning Rate ëª¨ë‘ë¥¼ Rewind(ë˜ëŒë ¤)ì„œ Pruning â†’ Retrain ì§„í–‰í•¨
<img src="https://user-images.githubusercontent.com/77658029/143775056-9d71ff40-82fd-4482-b2a9-c5d0ea9b7f0c.png"  width="85%" height="85%"/>
    
3. Comparing Rewind And Fine-Tuning In Neural Network Pruning(ICLR 2020); Learning Rate Rewinding
    - 2ë²ˆ ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•œ Weight RewindingëŠ” í•˜ì§€ ì•Šê³  Learning Rateë§Œ Rewindingí•˜ì—¬ ì§„í–‰í•¨
<img src="https://user-images.githubusercontent.com/77658029/143775088-bafd9e97-f7d9-4651-99be-28bbf55fde59.png"  width="85%" height="85%"/>    

**ğŸ’¡ LTH** 
- ì™œ ì•„ì§ ì˜ë˜ëŠ”ì§€ ì„¤ëª…ì´ ì•ˆë¨(But, RL, NLP ë“±ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì ìš©ì´ ê°€ëŠ¥í•¨ì´ Reportë˜ê³ ìˆìŒ)
- íŠ¹ì • initializationì— ëŒ€í•´ì„œ, í•™ìŠµ í›„ ë„ë‹¬í•˜ëŠ” ê³µê°„ì´ ìœ ì‚¬? â†’ ë„¤íŠ¸ì›Œí¬ì˜ í•™ìŠµ ë° ìˆ˜ë ´ ê´€ë ¨ ì—°êµ¬ì™€ ì ‘ëª©ë˜ëŠ” ë¶€ë¶„

4. Linear Mode Connectivity and the Lottery Ticket Hypothesis(ICML 2020)
    - ë„¤íŠ¸ì›Œí¬ì˜ í•™ìŠµ ë° ìˆ˜ë ´ ê´€ë ¨ëœ ì‹¤í—˜
    - íŠ¹ì • í•™ìŠµ ì‹œì (epoch at 0,k)ì—ì„œ seedë¥¼ ë³€ê²½í•˜ì—¬ ë‘ ê°œì˜ Netì„ í•™ìŠµ í›„ ë¹„êµí•´ë´„(SGDë¥¼ í†µí•œ ê³„ì‚° ê²°ê³¼ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ë‹¤ë¥¸ ê³³ìœ¼ë¡œ ìˆ˜ë ´)
    - Seedë¥¼ í†µí•´ ë‹¤ë¥¸ Weightë¥¼ í•™ìŠµí•œ í›„ ë‘ Net Weightì˜ Linear interpolationì„ ì‚¬ìš©í•˜ì—¬ weightê°„ì˜ ì–´ë–¤ ê´€ê³„ì„±ì„ í™•ì¸í•´ ë³´ê³ ìí•¨
     
    - $W_0$ë¥¼ ì‚¬ìš©í•˜ì˜€ì„ ê²½ìš°ì—ëŠ” Weightê°’ì´ ìˆ˜ë ´ ìì²´ë¥¼ ì•ˆí•˜ëŠ” ê²½í–¥ì„±ì´ ë³´ì˜€ì§€ë§Œ,
    <img src="https://user-images.githubusercontent.com/77658029/143775480-07976360-68df-4a15-9f3e-cd0a732ac82f.png"  width="85%" height="85%"/>
    - $W_k$ë¥¼ ì‚¬ìš©í•œ ê²½ìš° íŠ¹ì • **kë²ˆì§¸ì—ì„œ ìˆ˜ë ´**í•˜ëŠ” ê²°ê³¼ê°€ ë‚˜ì˜¤ê¸° ì‹œì‘í•¨
    <img src="https://user-images.githubusercontent.com/77658029/143775773-8d49a7a1-e0d3-48c4-8207-7a3b956c1e30.png"  width="85%" height="85%"/>
    - ë‘ ê°€ì§€ ê²°ê³¼ë¥¼ ë¹„êµí•´ë³´ê³  ìƒê°í•´ ë´¤ì„ë•Œ ì–´ëŠ ì •ë„ í•™ìŠµì´ ì§„í–‰ë˜ë©´ í•œ Plateìœ„ì—ì„œ í•™ìŠµë˜ëŠ”ê²Œ ì•„ë‹ê¹Œ?ë¼ëŠ” ì˜ë¬¸ì„ ë‚¨ê²¼ê³ , ì´ ë¶€ë¶„ì´ LTHì™€ ì–´ë–»ê²Œ ê´€ë ¨ì´ ìˆëŠ”ì§€ ë§ì€ ì—°êµ¬ë“¤ì´ ì§„í–‰ë˜ê³  ìˆìŒ
    <img src="https://user-images.githubusercontent.com/77658029/143775822-485fd25a-d3f2-4ddd-9298-fdc188253c9e.png"  width="85%" height="85%"/>
    
5. Deconstructing Lottery Tickets:Zeros, Signs, and the Supermask(NeurIPS 2019)
    - LTHì˜ ê²½ìš° $w_f$ë¥¼ L1 normìœ¼ë¡œ ì¤„ì„¸ì›Œ Masking ì§„í–‰í•˜ê²Œ ë˜ëŠ”ë°, ì´ë•Œ $w_i$ë¥¼ í•¨ê»˜ ë¹„êµí•˜ëŠ” ë°©ì‹ì„ ì œì•ˆí•¨
    <img src="https://user-images.githubusercontent.com/77658029/143776050-7e6717b9-a249-4ed8-8dd4-a2ee50752f8d.png"  width="85%" height="85%"/>



## ì •ë¦¬ 

### ì•„ì‰¬ìš´ì 
 
- Structured/Unstructured ëª¨ë‘ ê°„ë‹¨í•œ Architecture ë²¤ì¹˜ë§ˆí¬ë¥¼ ìˆ˜í–‰í•¨(MobileNet, EfficientNet, NASNet ë“± Moderní•œ ArchitectureëŠ” ì˜ ë‹¤ë£¨ì§€ ì•ŠìŒ)
- ë‹¤ì–‘í•œ Architectureì— ì ìš©í•˜ë ¤ë©´ ì—¬ëŸ¬ê°€ì§€ ì˜ˆì™¸ì²˜ë¦¬, ì¶”ê°€ ê²€ì¦ì„ í•„ìš”ë¡œí•¨(TransformerëŠ” ê·¸ë˜ë„ ëœí•œ í¸)
- íŒŒë¼ë¯¸í„° ìˆ˜, FLOPsë¡œ ë¹„êµë¥¼ ë§ì´ í•˜ì§€ë§Œ, ì‹¤ì§ˆì ìœ¼ë¡œ Latency í–¥ìƒ ì •ë„ì™€ëŠ” ê±°ë¦¬ê°€ ìˆìŒ, ì•„ì§ê¹Œì§€ Unstructuredì—ì„œ ì—°ì‚° ê°€ì†ì´ ë˜ëŠ” HWê°€ ë§ì§€ ì•ŠìŒ, ì§€ì›í•˜ëŠ” ì¶”ì„¸ì´ì§€ë§Œ ì•„ì§ ì œì•½ì‚¬í•­ì´ ë§ìŒ
- ì—°êµ¬ ë³µì¡ë„ê°€ ë³µì¡í•´ì§€ëŠ” ì •ë„ì— ë¹„í•´ íš¨ê³¼ëŠ” í¬ì§€ ì•Šì€ ê²ƒ ê°™ìŒ

### ê°•ì 

- ì—¬ì „íˆ ì—°êµ¬ì  ê°€ì¹˜ê°€ ë†’ê³ , ì—¬ëŸ¬ ë¶„ì•¼ì™€ì˜ ì ‘ëª©ì´ ì¼ì–´ë‚˜ëŠ” ì¶”ì„¸
- 

<br>

```
ğŸ’¡ ìˆ˜ì • í•„ìš”í•œ ë‚´ìš©ì€ ëŒ“ê¸€ì´ë‚˜ ë©”ì¼ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤!ğŸ’¡ 
```
