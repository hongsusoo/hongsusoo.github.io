---
defaults:
  - scope:
      path: ""
      type: posts
    values:
      layout: single
      author_profile: true
      comments: true
      share: true
      related: true

title: "[AI basic] ê²½ì‚¬í•˜ê°•ë²•(gradient descent)_ìˆœí•œë§›"
excerpt: "about : python"
toc: true
toc_sticky: true
toc_label: "Label"
categories:
  - AI
tags:
  - [python, AI Math, gradient descent]
date: 2021-08-02
last_modified_at: 2021-08-02
---
<br>

# ê²½ì‚¬ í•˜ê°•/ìƒìŠ¹ë²•

## ë¯¸ë¶„ì— ëŒ€í•œ ì´í•´

- ë¯¸ë¶„(differentiation)ì€ ë³€ìˆ˜ì˜ ì›€ì§ì„ì— ë”°ë¥¸ í•¨ìˆ˜ê°’ì˜ ë³€í™”ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•œ ë„êµ¬
- ìµœì í™”ì—ì„œ ì œì¼ ë§ì´ ì‚¬ìš©í•˜ëŠ” ê¸°ë²•
- ì ‘ì„ ì˜ ê¸°ìš¸ê¸°ë¥¼ ì˜ë¯¸
- í•œ ì ì—ì„œì˜ ì ‘ì„ ì˜ ê¸°ìš¸ê¸°ë¥¼ ì•Œë©´ ì–´ëŠ ë°©í–¥ìœ¼ë¡œ ì›€ì§ì—¬ì•¼ ì¦ê°€ or ê°ì†Œí•˜ëŠ”ì§€ í™•ì¸ ê°€ëŠ¥í•¨
- ì €ì°¨ì›ì—ì„œëŠ” ëˆˆìœ¼ë¡œ í™•ì¸ ê°€ëŠ¥í•˜ì§€ë§Œ, ê³ ì°¨ì›ìœ¼ë¡œ ê°ˆê²½ìš° í•´ì„ì´ ì–´ë ¤ì›Œì§€ê¸° ë•Œë¬¸ì— ë¯¸ë¶„ì„ í™œìš©

![image](https://user-images.githubusercontent.com/77658029/127102561-466ffe3d-a63a-446e-a6cb-633b23afbc0c.png)


### ë¯¸ë¶„ ëª¨ë“ˆ(sympy)

- `import sympy as sym`ìœ¼ë¡œ sympy ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜´
- `from sympy.abc import x` ë³€ìˆ˜ë¥¼ xë¡œ ì§€ì •í•¨

```python
import sympy as sym
from sympy.abc import x

print(sym.diff(sym.poly(x**2 + 2*x + 3),x))
```
```
Poly(2ğ‘¥+2,ğ‘¥,ğ‘‘ğ‘œğ‘šğ‘ğ‘–ğ‘›=â„¤)
```

## ë¯¸ë¶„ì—ì„œ ê²½ì‚¬ìƒìŠ¹ë²•ìœ¼ë¡œ

- íŠ¹ì • ì  $x$ì—ì„œ ë¯¸ë¶„í•œê°’($f'(x)$)ë§Œí¼ ì  $x$ì— ë”í•˜ê±°ë‚˜ ë¹¼ì£¼ëŠ” ì‘ì—…ì„ ì§„í–‰
- ë” ì´ìƒ ë³€í™”ê°€ ì—†ê±°ë‚˜, ë¯¸ë¶„ê°’ì´ ë¯¸ë¦¬ ì •í•´ë‘” epslion ì´í•˜ê°€ ë ë•Œê¹Œì§€ ë°˜ë³µí•œë‹¤(ê·¹ê°’ì— ë„ë‹¬í•˜ê±°ë‚˜, ê·¼ì²˜ì— ë„ì°©í–ˆì„ ê²½ìš°)
- ì í™”ì‹ í‘œí˜„ $x_n=x_{n-1}Â±f'(x_{n-1})$   $for f'(x_n) > Îµ$ (+ : ìƒìŠ¹, - : í•˜ê°•)

ğŸ’¡ ê²½ì‚¬ìƒìŠ¹ë²•
![image](https://user-images.githubusercontent.com/77658029/127119457-6f18c60a-3e0a-4803-9e18-16f959fb794b.png)

ğŸ’¡ ê²½ì‚¬í•˜ê°•ë²•
![image](https://user-images.githubusercontent.com/77658029/127120676-05df0b12-adcb-4d39-ad4f-a94003c8fa4d.png)


## ê²½ì‚¬ë²• algorithm

1. ë¯¸ë¶„ ê³„ì‚° : gradient
2. x_init : ì‹œì‘ì , learning_rate : í•™ìŠµë¥ , epsilon : ì¢…ë£Œì¡°ê±´
â€» í•™ìŠµë¥ ì˜ ê²½ìš°ëŠ” ë¯¸ë¶„ì„ í†µí•´ ì—…ë°ì´íŠ¸ í•˜ëŠ” ì†ë„ë¥¼ ì¡°ì ˆí•˜ëŠ”ë°, ê°’ì„ ì˜ëª» ì„¤ì •í•˜ë©´ ìˆ˜ë ´ê°’ì— ë„ë‹¬í•˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆìŒ

```python
import numpy as np
import sympy as sym
from sympy.abc import x

def gradient_ascent(func, x_init, lr=0.3,epsilon=1e-2): #ê²½ì‚¬ìƒìŠ¹ë²•
    cnt=0
    val = x_init
    f_diff = sym.diff(func, x)
    diff = f_diff.subs(x,val)
    while np.abs(diff) > epsilon:
        val = val + lr*diff
        f_diff = sym.diff(func, x)
        diff = f_diff.subs(x,val)
        cnt += 1
#         print("ì—°ì‚°íšŸìˆ˜ : {}, ì¢Œí‘œ : ({:0.2f}, {:0.2f})".format(cnt,float(val),float(func.subs(x,val))))
    print("ê²½ì‚¬ìƒìŠ¹ - í•¨ìˆ˜ : {}, ìµœëŒ€ì  : ({:0.2f}, {:0.2f})".format(func,val,func.subs(x,val)))

def gradient_descent(func, x_init, lr=0.3,epsilon=1e-2): #ê²½ì‚¬í•˜ê°•ë²•
    cnt=0
    val = x_init
    f_diff = sym.diff(func, x)
    diff = f_diff.subs(x,val)
    while np.abs(diff) > epsilon:
        val = val - lr*diff
        f_diff = sym.diff(func, x)
        diff = f_diff.subs(x,val)
        cnt += 1
#         print("ì—°ì‚°íšŸìˆ˜ : {}, ì¢Œí‘œ : ({:0.2f}, {:0.2f})".format(cnt,float(val),float(func.subs(x,val))))
    print("ê²½ì‚¬í•˜ê°• - í•¨ìˆ˜ : {}, ìµœì†Œì  : ({:0.2f}, {:0.2f})".format(func,val,func.subs(x,val)))
    
gradient_ascent(func=sym.poly(-x**2), x_init=10) #ê²½ì‚¬ìƒìŠ¹ë²•
gradient_descent(func=sym.poly(x**2), x_init=10) #ê²½ì‚¬í•˜ê°•ë²•
```
```
ê²½ì‚¬ìƒìŠ¹ - í•¨ìˆ˜ : Poly(-x**2, x, domain='ZZ'), ìµœëŒ€ì  : (0.00, -0.00)
ê²½ì‚¬í•˜ê°• - í•¨ìˆ˜ : Poly(x**2, x, domain='ZZ'), ìµœì†Œì  : (0.00, 0.00)
```
  
## í¸ë¯¸ë¶„(partial differentiation)ì— ëŒ€í•œ ì´í•´ 

- ê¸°ì¡´ ë¯¸ë¶„ì˜ ê²½ìš° ë°©í–¥ì€ ì¦ê°€ or ê°ì†Œ ë‘ ê°€ì§€ ë°©í–¥ë§Œ ê°€ì§€ê³  ìˆìŒ
- ë§Œì•½ ë²¡í„°ê°€ ì…ë ¥ë˜ëŠ” ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ê²½ìš°ëŠ” ê¸°ì¡´ ë¯¸ë¶„ìœ¼ë¡œ ëŒ€ì‘ì´ ì–´ë ¤ì›€

ğŸ”‘ í¸ë¯¸ë¶„ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°ê°€ëŠ¥!

- í¸ë¯¸ë¶„ì€ ë²¡í„°ë¡œ ì…ë ¥ë˜ëŠ” ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì—ì„œ í•œ ì°¨ì›ì„ ê¸°ì¤€ìœ¼ë¡œ ì¡ê³  ê·¸ ë°©í–¥ìœ¼ë¡œì˜ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ëŠ” ì‘ì—…
- ëª¨ë“  ì°¨ì›ì— ëŒ€í•´ì„œ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ê²Œ ë˜ë©´, í•˜ë‚˜ì˜ ë²¡í„°ë¥¼ êµ¬í•  ìˆ˜ ìˆëŠ”ë° ê·¸ ë²¡í„°ë¥¼ ê·¸ë ˆë””ì–¸íŠ¸(gradient)ë¼ê³ í•¨
- ê·¸ë ˆë””ì–¸íŠ¸ ë²¡í„°ë¥¼ ì´ìš©í•˜ì—¬ ë¯¸ë¶„ê³¼ ë™ì¼í•˜ê²Œ ê²½ì‚¬ë²•ì„ ì´ìš©í•  ìˆ˜ ìˆìŒ

![image](https://user-images.githubusercontent.com/77658029/127163925-b2643b4e-cebe-45d0-ac23-c3507dd9af8f.png)
  $e_i$ëŠ” ië²ˆì§¸ ê°’ë§Œ 1ì´ê³  ë‚˜ë¨¸ì§€ëŠ” 0ì¸ ë‹¨ìœ„ë²¡í„°


## ê·¸ë ˆë””ì–¸íŠ¸(gradient)

- ëª¨ë“  ì°¨ì›ì— ëŒ€í•´ì„œ í¸ë¯¸ë¶„ì„ í•˜ì—¬ ë²¡í„°ë¡œ ë§Œë“  ê²ƒ
- ê·¸ë ˆë””ì–¸íŠ¸ ë²¡í„°ëŠ” í•œ ì ì—ì„œ ê¸°ìš¸ê¸°ê°€ ì¦ê°€í•˜ëŠ” ë°©í–¥ ë²¡í„°ë¥¼ ì˜ë¯¸í•¨
![image](https://user-images.githubusercontent.com/77658029/127168928-11e9ddf8-46d9-461a-b476-a6b497a53685.png)
- $\nabla f$ë¡œ í‘œí˜„í•¨
- $\nabla$ëŠ” nabla(ë‚˜ë¸”ë¼) í˜¹ì€ del(ë¸)ì´ë¼ê³  ì½ìŒ

ğŸ’¡ gradient vector direction
![image](https://user-images.githubusercontent.com/77658029/127168616-c8440173-99e1-4ff3-8834-62c20f3bd947.png)

**ğŸ“Œreference**
- boostcourse AI tech pre-course
- https://ko.wikipedia.org/wiki/%EA%B8%B0%EC%9A%B8%EA%B8%B0_(%EB%B2%A1%ED%84%B0)


## ê²½ì‚¬ë²• algorithm(ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ gradient)

1. ë¯¸ë¶„ ê³„ì‚° : gradient(x,y ê° ë³€ìˆ˜ë¡œ í¸ë¯¸ë¶„í•˜ì—¬ gradient ê³„ì‚°)
2. x_init : ì‹œì‘ì , learning_rate : í•™ìŠµë¥ , epsilon : ì¢…ë£Œì¡°ê±´
â€» í•™ìŠµë¥ ì˜ ê²½ìš°ëŠ” ë¯¸ë¶„ì„ í†µí•´ ì—…ë°ì´íŠ¸ í•˜ëŠ” ì†ë„ë¥¼ ì¡°ì ˆí•˜ëŠ”ë°, ê°’ì„ ì˜ëª» ì„¤ì •í•˜ë©´ ìˆ˜ë ´ê°’ì— ë„ë‹¬í•˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆìŒ

```python
import numpy as np
import sympy as sym
from sympy.abc import x,y

def gradient_descent(func, x_init, lr=0.3,epsilon=1e-2): #ê²½ì‚¬í•˜ê°•ë²•
    cnt=0
    val = x_init
    f_x_diff = sym.diff(func, x) # xë¡œ í¸ë¯¸ë¶„
    f_y_diff = sym.diff(func, y) # yë¡œ í¸ë¯¸ë¶„
    nabla_f = np.array([f_x_diff.subs(x,val[0]).subs(y,val[1]),f_y_diff.subs(x,val[0]).subs(y,val[1])], dtype = float) # grad ë²¡í„°ê°’
    while np.linalg.norm(nabla_f) > epsilon: # grad ë²¡í„°ê°’ì˜ í¬ê¸°ë¥¼ ì‘ê²Œí•˜ì!
        val = val - lr*nabla_f # ê²½ì‚¬í•˜ê°•ë²•
        f_x_diff = sym.diff(func, x) # xë¡œ í¸ë¯¸ë¶„
        f_y_diff = sym.diff(func, y) # yë¡œ í¸ë¯¸ë¶„
        nabla_f = np.array([f_x_diff.subs(x,val[0]).subs(y,val[1]),f_y_diff.subs(x,val[0]).subs(y,val[1])], dtype = float)
        cnt += 1
        print("ì—°ì‚°íšŸìˆ˜ : {},ìµœì†Œì  : ({:0.2f}, {:0.2f}), ìµœì†Œê°’ : {:0.2f}".format(cnt,val[0],val[1],func.subs(x,val[0]).subs(y,val[1])))
    print("ê²½ì‚¬í•˜ê°• - í•¨ìˆ˜ : {}, ìµœì†Œì  : ({:0.2f}, {:0.2f}), ìµœì†Œê°’ : {:0.2f}".format(func,val[0],val[1],func.subs(x,val[0]).subs(y,val[1])))

gradient_descent(func=sym.poly(x**2+2*y**2), x_init=np.array([2,2])) #ê²½ì‚¬í•˜ê°•ë²•
```
```
ì—°ì‚°íšŸìˆ˜ : 1,ìµœì†Œì  : (0.80, -0.40), ìµœì†Œê°’ : 0.96
ì—°ì‚°íšŸìˆ˜ : 2,ìµœì†Œì  : (0.32, 0.08), ìµœì†Œê°’ : 0.12
ì—°ì‚°íšŸìˆ˜ : 3,ìµœì†Œì  : (0.13, -0.02), ìµœì†Œê°’ : 0.02
ì—°ì‚°íšŸìˆ˜ : 4,ìµœì†Œì  : (0.05, 0.00), ìµœì†Œê°’ : 0.00
ì—°ì‚°íšŸìˆ˜ : 5,ìµœì†Œì  : (0.02, -0.00), ìµœì†Œê°’ : 0.00
ì—°ì‚°íšŸìˆ˜ : 6,ìµœì†Œì  : (0.01, 0.00), ìµœì†Œê°’ : 0.00
ì—°ì‚°íšŸìˆ˜ : 7,ìµœì†Œì  : (0.00, -0.00), ìµœì†Œê°’ : 0.00
ê²½ì‚¬í•˜ê°• - í•¨ìˆ˜ : Poly(x**2 + 2*y**2, x, y, domain='ZZ'), ìµœì†Œì  : (0.00, -0.00), ìµœì†Œê°’ : 0.00
```
**ğŸ“Œreference**
- boostcourse AI tech pre-course
- https://ko.wikipedia.org/wiki/%EA%B8%B0%EC%9A%B8%EA%B8%B0_(%EB%B2%A1%ED%84%B0)

<br>
```
ğŸ’¡ ìˆ˜ì • í•„ìš”í•œ ë‚´ìš©ì€ ëŒ“ê¸€ì´ë‚˜ ë©”ì¼ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤!ğŸ’¡ 
```
