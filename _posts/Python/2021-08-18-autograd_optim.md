---
defaults:
  - scope:
      path: ""
      type: posts
    values:
      layout: single
      author_profile: true
      comments: true
      share: true
      related: true

title: "[DL Baic] PyTorch nn.Module(Autograd & Optimizer)"
excerpt: "about : python"
toc: true
toc_sticky: true
toc_label: "Label"
categories:
  - AI
tags:
  - [python, AI Tool, PyTorch, nn.Module]
date: 2021-08-18
last_modified_at: 2021-08-18
---

<br>  

# Autograd & Optimizer

<br>

## Torch.nn.Module

- ë”¥ëŸ¬ë‹ì„ êµ¬ì„±í•˜ëŠ” layerì˜ base class
- input, output, forward, backward(autograd,weight)
- í•™ìŠµì´ ëŒ€ìƒì´ ë˜ëŠ” parameter(tensor) ì •ì˜

![image](https://user-images.githubusercontent.com/77658029/129821395-4f9db686-a2a6-401e-a36d-41e80008047c.png)

<br>

### nn.Moduleì˜ êµ¬ì¡°

- __init__ : ê°ì¢… parameterë“¤ ì •ì˜
- forward : Networkì— ì…ë ¥ì„ ë„£ì–´ outputì„ ë½‘ì•„ëƒ„
- sigmoid : activation function, ë‹¤ë¥¸ í•¨ìˆ˜ë„ êµ¬í˜„í•´ì„œ ì‚¬ìš© ê°€ëŠ¥í•¨
- backward : forwardì—ì„œ ì§„í–‰í•œ ì‹ì— ëŒ€í•´ gradientê°’ì„ êµ¬í•¨(weight, biasì— ê´€í•´ì„œ)
- optimizer : backwardì—ì„œ ê³„ì‚°í•œ ê°’ì„ Networkì— ì—…ë°ì´íŠ¸ ì‹œì¼œì¤Œ(weight, biasë¥¼ ì—…ë°ì´íŠ¸í•¨)


```python
class Example(nn.Module):
    def __init__(self, dim, lr=torch.scalar_tensor(0.01)): #dim = (inputsize,outputsize) - weightì— ë“¤ì–´ê°ˆ 
        super(Template, self).__init__()
        # intialize parameters
        self.W = torch.zeros(dim, 1, dtype=torch.float)
        self.b = torch.scalar_tensor(0)
        self.grads = {"dW": torch.zeros(dim, 1, dtype=torch.float), "db": torch.scalar_tensor(0)}
        self.lr = lr
        
    def forward(self, x):
        ## compute forward
        linear = torch.addmm(self.b, x, self.w)
        output = self.sigmoid(linear)
        return output
        
    def sigmoid(self, z):
        return 1/(1 + torch.exp(-z))
    
    def backward(self, x, yhat, y):
        ## compute backward
        self.grads["dw"] = (1/x.shape[1]) * torch.mm(x, (yhat - y).T) #gradient ê³„ì‚°
        self.grads["db"] = (1/x.shape[1]) * torch.sum(yhat - y) #gradient ê³„ì‚°
        
    def optimize(self):
        ## optimization step
        self.w = self.w - self.lr * self.grads["dw"] #gradient update
        self.b = self.b - self.lr * self.grads["db"] #gradient update
```



<br>

## Fowardì™€ parameter

- Tensor ê°ì²´ì˜ ìƒì† ê°ì²´ 
- nn.Moduleë‚´ì— attributeê°€ ë  ë•ŒëŠ” required_grad = Trueë¡œ ì§€ì •ë˜ì–´ í•™ìŠµ ëŒ€ìƒì´ ë˜ëŠ” Tensor
- required_grad = Trueê°€ ë˜ì–´ ìˆì–´ì•¼ Autogradê°€ ì§„í–‰ì´ë¨
- Linear, conv ë“±ì˜ layerë“¤ì— ì´ë¯¸ parameter(weight)ê°€ ì§€ì •ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ì§ì ‘ ì§€ì •í•  ì¼ì€ ë§ì§€ ì•ŠìŒ
- ì•„ë˜ì™€ `nn.parameter`ë¥¼ í™œìš©í•˜ì—¬ weightë¥¼ êµ¬ì„±í•œ ì´í›„ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë™ì‘ì‹œí‚¤ê²Œ ë˜ë©´ forwardpassì´ ì¼ì–´ë‚˜ê²Œ ëœë‹¤

**ğŸ“°code**
```python
import torch
from torch import nn
from torch import Tensor

class MyLinear(nn.Module):
    def __init__(self, in_features, out_features, bias=True):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        
        self.weights = nn.Parameter(torch.randn(in_features, out_features))
        self.bias = nn.Parameter(torch.randn(out_features))

    def forward(self, x : Tensor):
        return x @ self.weights + self.bias
    
x = torch.randn(5, 7) # batch 5, input dim 7
print(x)
layer = MyLinear(7, 12) # input 7, output 12
print(layer(x).shape) # batch 5, output dim 12
```
**ğŸ”result**
```
tensor([[ 0.4152,  2.4837,  0.7766, -1.4975, -0.6413, -0.0836,  1.0998],
        [ 2.0921,  0.6759, -0.5452, -1.0992, -0.3953, -0.2154,  0.6453],
        [-0.3645, -0.1517, -0.0193, -1.8220,  1.7140, -1.0767, -0.5035],
        [-0.6115,  2.9012, -1.4191, -1.2334, -1.2555, -0.4280, -1.7653],
        [ 0.1985, -1.1522,  0.2816,  0.9674, -0.0682,  0.9396, -0.7299]])
torch.Size([5, 12])
```

- ìœ„ ì½”ë“œì—ì„œ MyLinearì˜ ê°ì²´ì¸ layerì˜ parameterì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤
- `layer.parameters()`í˜•ì‹ìœ¼ë¡œ ë½‘ìœ¼ë©´ generator ê°ì²´ë¡œ ë½‘í•˜ê¸° ë•Œë¬¸ì— forë¬¸ì„ ì‚¬ìš©í•´ì„œ ì¶œë ¥í•´ ì¤„ ìˆ˜ ìˆë‹¤
- ìœ„ ì½”ë“œì—ì„œ parameterëŠ” weightì™€ bias ë‘ ì¢…ë¥˜ê°€ ìˆê¸° ë•Œë¬¸ì—, ì¶œë ¥ ë˜í•œ 2ê°œì˜ paramterë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤

**ğŸ“°code**
```python
for i,weight in enumerate(layer.parameters()):
    print(i)
    print(weight)
```
**ğŸ”result**
```
0
Parameter containing:
tensor([[-0.9773, -0.8269, -1.3366,  0.3635,  1.2686,  0.6571, -1.0801, -0.0366,
         -1.1278, -1.1030, -0.3572,  0.6733],
        [ 1.4786,  0.8860,  2.9108,  1.1033, -0.1592,  0.4273,  1.9332, -0.5062,
         -1.1353,  0.5759,  0.3554,  0.9182],
        [-1.5364,  0.6698, -1.4138, -1.1673, -0.6144, -0.2622,  0.1079,  1.5369,
         -0.7240, -0.9971,  1.1517, -0.3620],
        [ 0.3360, -0.8670, -1.4483, -0.1183,  0.4558,  0.0648, -1.7267, -1.2206,
         -0.9050,  1.2235, -1.2723, -0.6792],
        [-0.0449, -0.5937, -0.0282,  0.0487,  1.5190, -1.9477, -1.9266, -0.5821,
          0.1365, -0.9889, -0.5429,  0.0134],
        [ 0.2352,  0.3291, -0.8454,  0.5034, -1.3333, -2.3747,  0.1558,  1.0028,
         -0.1426,  1.7190,  0.3017, -0.3314],
        [-0.3310,  0.4655, -0.3544, -1.0033, -0.8208, -0.4913,  0.7759, -1.9471,
         -0.6976,  0.4412, -1.0636, -0.0432]], requires_grad=True)
1
Parameter containing:
tensor([-1.3176,  1.0029,  0.7482, -2.1537,  0.8717, -1.0003, -0.2203,  0.4989,
        -0.7009,  1.0871,  1.2747, -1.3107], requires_grad=True)
```


<br>

## Backward

- forwardë¥¼ í†µí•´ ë§Œë“¤ì–´ì§„ parameterë“¤ë¡œ output(ì˜ˆì¸¡ì¹˜)ë¥¼ ë½‘ì•„ë‚´ëŠ”ë°, ì´ë•Œì˜ ì˜ˆì¸¡ì¹˜ì™€ ì‹¤ì œê°’ê°„ì˜ ì°¨ì´(loss)ì— ëŒ€í•´ ë¯¸ë¶„ ìˆ˜í–‰
- lossë¥¼ ìµœì†Œí™” í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ paratmeterë¥¼ ì—…ë°ì´íŠ¸ ì‹œì¼œì¤Œ

- ëŒ€ë¶€ë¶„ epochë§ˆë‹¤ backwardë¥¼ ì§„í–‰ì‹œì¼œì¤Œ
- backwardì—ì„œëŠ” í¬ê²Œ 4ê°€ì§€ ë¶€ë¶„ì— ëŒ€í•´ì„œ í•„ìˆ˜ì ìœ¼ë¡œ ì§„í–‰í•´ì•¼í•˜ëŠ” ëŒ€ìƒ

    1. optimizer.zero_grad() : ì´ì „ ë¯¸ë¶„í•œ gradient ê°’ì´ ì˜í–¥ì„ ì£¼ì§€ ì•Šë„ë¡ ì´ˆê¸°í™” ì‹œì¼œì£¼ëŠ” ì‘ì—…
    2. loss ì§€ì • : modelì˜ ì˜ˆì¸¡ì— ê¸°ì—¬í•˜ëŠ” lossë¥¼ ì„¤ì •í•˜ì—¬ ì •ì˜í•´ì¤˜ì•¼í•¨
    3. loss.backward() : lossì— ëŒ€í•œ ë¯¸ë¶„ê°’ì„ ê³„ì‚°í•˜ëŠ” ë‹¨ê³„
    4. optimizer.step() : backwardì—ì„œ ê³„ì‚°í•œ ë¯¸ë¶„ê°’ì„ parameterì— ì ìš©ì‹œì¼œì£¼ëŠ” ë‹¨ê³„
    
- ì‹¤ì œ BackwardëŠ” module ë‹¨ê³„ì—ì„œ ì§ì ‘ ì§€ì •ê°€ëŠ¥í•¨
- Moduleì—ì„œ backwardì™€ optimizerë¥¼ ì˜¤ë²„ë¼ì´ë”© í–ˆê¸° ë•Œë¬¸ì— autogradê°€ ê°€ëŠ¥í•¨
- ë§Œì•½ ì§ì ‘ ì§€ì •í•œë‹¤ê³  í•˜ë©´, ì§ì ‘ ì—¬ëŸ¬ layerì— ëŒ€í•œ ë¯¸ë¶„ì„ ì§„í–‰ì•¼í•¨


**ğŸ“°code**
```python
import numpy as np
import torch
from torch import nn
from torch import Tensor
from torch.autograd import Variable

class LinearRegression(torch.nn.Module):
    def __init__(self, inputSize, outputSize):
        super(LinearRegression,self).__init__()
        self.linear = torch.nn.Linear(inputSize,outputSize)
        
    def forward(self,x):
        out = self.linear(x)
        return out

# create dummy data for training
x_values = [i for i in range(11)]
x_train = np.array(x_values, dtype=np.float32)
x_train = x_train.reshape(-1, 1)

y_values = [2*i + 1 for i in x_values]
y_train = np.array(y_values, dtype=np.float32)
y_train = y_train.reshape(-1, 1)

inputDim = 1
outputDim = 1
learningRate = 0.01 
epochs = 100

model = LinearRegression(inputDim, outputDim)

criterion = torch.nn.MSELoss() 
optimizer = torch.optim.SGD(model.parameters(), lr=learningRate) 

for epoch in range(epochs):
    inputs = Variable(torch.from_numpy(x_train))
    labels = Variable(torch.from_numpy(y_train))

    optimizer.zero_grad()
    
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()

#     print('epoch {}, loss {}'.format(epoch, loss.item()))
    
with torch.no_grad():
    predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()

for train, pred in zip(y_train,predicted):
    print(train,pred)
```
**ğŸ”result**
```
[1.] [1.145034]
[3.] [3.124148]
[5.] [5.1032615]
[7.] [7.0823755]
[9.] [9.061489]
[11.] [11.040603]
[13.] [13.019717]
[15.] [14.998831]
[17.] [16.977945]
[19.] [18.95706]
[21.] [20.936172]
```

- ì‹¤ì œ backwardëŠ” Module ë‹¨ê³„ì—ì„œ ì§ì ‘ ì§€ì •í•˜ì—¬ ì§„í–‰í•  ìˆ˜ ìˆì§€ë§Œ,
- ì§ì ‘ backward, optimizer í•¨ìˆ˜ë¥¼ ì˜¤ë²„ë¼ì´ë”© í•´ì„œ ì§ì ‘ codingí•´ì¤˜ì•¼í•¨


<br>

**ğŸ“Œreference**
- boostcourse AI tech
- [toward data science](https://subinium.github.io/pytorch-dataloader/)

<br>

```
ğŸ’¡ ìˆ˜ì • í•„ìš”í•œ ë‚´ìš©ì€ ëŒ“ê¸€ì´ë‚˜ ë©”ì¼ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤!ğŸ’¡ 
```
