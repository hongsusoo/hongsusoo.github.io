---
defaults:
  - scope:
      path: ""
      type: posts
    values:
      layout: single
      author_profile: true
      comments: true
      share: true
      related: true

title: "[AI basic] ê²½ì‚¬í•˜ê°•ë²•(gradient descent)_ë§¤ìš´ë§›"
excerpt: "about : python"
toc: true
toc_sticky: true
toc_label: "Label"
categories:
  - AI basic
tags:
  - [python, AI Math, gradient descent]
date: 2021-08-02
last_modified_at: 2021-08-02
---
<br>

# ê²½ì‚¬í•˜ê°•ë²•ì„ í†µí•œ ì„ í˜• íšŒê·€

- ì—­í–‰ë ¬ê³¼ ë¬´ì–´íœë¡œì¦ˆ(Moore-penrose)ì—­í–‰ë ¬ì„ í†µí•˜ì—¬ ì„ í˜•íšŒê·€ê°€ ê°€ëŠ¥í•˜ì§€ë§Œ, ë‹¤ë¥¸ ë°©ì‹ì˜ ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€

**ğŸ”‘ ê²½ì‚¬í•˜ê°•ë²•ì„ í†µí•œ ì„ í˜•íšŒê·€**

- ì„ í˜•ëª¨ë¸ì„ í¬í•¨í•˜ì—¬ ë‹¤ë¥¸ ëª¨ë¸ë„ ëŒ€ì‘ì´ ê°€ëŠ¥í•¨
- ì—­í–‰ë ¬ì„ í†µí•œ íšŒê·€ë²•ë³´ë‹¤ ë” powerfulí•œ ë°©ë²•

## ì„ í˜•íšŒê·€ì— ëŒ€í•œ ì´í•´ 

- ë§ì€ ìˆ˜ì˜ dataê°€ ìˆì„ ê²½ìš°($ë²¡í„°ì˜ ìˆ˜â‰¥ë²¡í„°ì˜ ê¸¸ì´$), ê·¸ ëª¨ë“  ë²¡í„°ë¥¼ ë§Œì¡±í•˜ëŠ” í•´ë‹µì„ êµ¬í•  ìˆ˜ ì—†ìŒ(ì¼ì§ì„ ì— ìˆëŠ” ê²½ìš°ëŠ” ì œì™¸)
- ë”°ë¼ì„œ ì„ í˜•íšŒê·€ ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ë°ì´í„°ë“¤ì„ ì•„ìš°ë¥¼ ìˆ˜ ìˆëŠ” ìµœì ì˜ ì‹ì„ ì°¾ì•„ì•¼ í•¨

ğŸ’¡ data -> í–‰ë ¬ì‹ìœ¼ë¡œ í‘œí˜„

![image](https://user-images.githubusercontent.com/77658029/127202018-d3dff48d-4964-43e8-8b8a-dea17975253f.png)

- ì°¾ê³ ì í•˜ëŠ” ìµœì ì˜ ì‹ì„ $X\beta = \hat Y â‰ˆ Y$ë¡œ ì •ì˜í•˜ì—¬ $Y-\hat Y$ë¥¼ ìµœì†Œë¡œ ë§Œë“œëŠ” $\beta$ì„ êµ¬í•´ì•¼ í•œë‹¤.
- ë¬´ì–´íœë¡œì¦ˆ ì—­í–‰ë ¬ì„ ì‚¬ìš©í•˜ë©´, ë°”ë¡œ ìµœì†Œê°€ ë˜ëŠ” $\beta$ì„ êµ¬í•  ìˆ˜ ìˆì§€ë§Œ, ì‚¬ìš©ë²”ìœ„ê°€ ë” ë„“ì€ ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ íŠ¸ë¼ì´ í•´ë³´ì
- **ì„ í˜•íšŒê·€ ëª©ì ì‹** $||Y-\hat Y||_2 = ||Y-X\beta||_2$ê°€ ìµœì†Œê°€ ë˜ëŠ” $\beta$ êµ¬í•˜ê¸° ìœ„í•´ gradient vectorë¥¼ êµ¬í•´ë³´ì

ğŸ”‘ gradient ìˆ˜ì‹ í’€ì´

![image](https://user-images.githubusercontent.com/77658029/127207989-1e0c5c33-21ad-42f8-9d4d-0089bc31175e.png)

- ì í™”ì‹ í‘œí˜„ $\beta^{(t+1)} = \beta^{(t)} - \lambda\nabla_\beta||Y-X\beta^{(t)}|| = \beta^{(t)} + \frac{2\lambda}{n}X^T(y-X\beta^{(t)}$


## ì„ í˜•íšŒê·€ algorithm êµ¬í˜„(ê²½ì‚¬í•˜ê°•ë²•)

```python
import numpy as np

#í•œí‰ë©´ ìƒ -> [1,2,3]ì´ ì •ë‹µ
X= np.array([[1,1],[1,2],[2,2],[2,3]]) 
y= np.dot(x,np.array([1,2]))+3

# 0,0 ë§Œ ë‹¤ë¥¸ í‰ë©´ì— ìˆì„ ê²½ìš° -> [0.7333, 1.8666, 0.7333]ì´ ì •ë‹µ
# X= np.array([[1,1],[1,2],[2,2],[2,3],[0,0]]) 
# y= np.array([3,5,6,8,1])
beta_gd=[10.1,15.1,-6.5] 
X_=np.array([np.append(x,[1]) for x in X])

for t in range(10000): 
    error = y - X_@beta_gd
#     error = error/np.linalg.norm(error) #ì´ë¶€ë¶„ì„ ì•ˆí•´ì£¼ë©´ learning_rate:0.01ì— ë°˜ì˜ë¨
    grad = - np.transpose(X_) @ error
    beta_gd = beta_gd - 0.01*grad
    
print(beta_gd)
```
```
[1. 2. 3.]
```

## ê²½ì‚¬í•˜ê°•ë²•ì€ ë§ŒëŠ¥ì¼ê¹Œ?

- ê²½ì‚¬í•˜ê°•ë²•ì€ ê²°êµ­ ë¯¸ë¶„ê°€ëŠ¥ì´ ê°€ëŠ¥í•˜ê³ , ìµœì†Œì ì„ êµ¬í•  ìˆ˜ ìˆëŠ” ë³¼ë¡(convex)í•œ í•¨ìˆ˜ì— ëŒ€í•´ì„œëŠ” ìˆ˜ë ´ì´ ê°€ëŠ¥í•œë‹¤
- ì ì ˆí•œ í•™ìŠµë¥ ê³¼ í•™ìŠµíšŸìˆ˜ë¥¼ ì„ íƒí–ˆì„ ë•Œ ìˆ˜ë ´ì´ ë³´ì¥ë¨
- ì„ í˜•íšŒê·€ì˜ ê²½ìš°ëŠ” ëª©ì ì‹($||Y-X\beta||_2$)ì´ íšŒê·€ê³„ìˆ˜$\beta$ì— ëŒ€í•´ ë³¼ë¡í•¨ìˆ˜ì´ê¸° ë•Œë¬¸ì— ìˆ˜ë ´ê°€ëŠ¥ì„±ì´ ë³´ì¥ëœë‹¤
- í•˜ì§€ë§Œ ê·¹ì†Œê°’ì´ ì—¬ëŸ¬ê°œê±°ë‚˜, ë¯¸ë¶„ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°ëŠ” ì‚¬ìš©í•˜ì§€ ëª»í•œë‹¤(ë¬´ì–´íœë¡œì¦ˆ ì—­í–‰ë ¬ë³´ë‹¤ëŠ” ë” ë§ì€ ëª¨ë¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤)
- ë¹„ì„ í˜•íšŒê·€ ë¬¸ì œì˜ ê²½ìš° ëª©ì ì‹ì´ ë³¼ë¡í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìˆ˜ë ´ì„±ì´ í•­ìƒ ë³´ì¥ë˜ì§€ ì•ŠìŒ

![image](https://user-images.githubusercontent.com/77658029/127268266-add97292-b46f-40a7-a05d-0f94e4d1a011.png)

### ê²½ì‚¬í•˜ê°•ë²• gradient vector ê³„ì‚°




## í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•(Stochastic Gradient Descent)

- ê²½ì‚¬í•˜ê°•ë²•ì€ non-convexí•¨ìˆ˜ì— ëŒ€í•´ì„œ ìµœì†Œê°’ì´ ë³´ì¥ë˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ë¬¸ì œ ìˆìŒ
- ê·¸ê±¸ ë³´ì™„í•˜ê¸° ìœ„í•œ ë°©ë²•ì„
- í•œ ê°œ í˜¹ì€ ì¼ë¶€ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ì—…ë°ì´íŠ¸(ê¸°ì¡´ ê²½ì‚¬í•˜ê°•ë²•ì€ ëª¨ë“  ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ì—…ë°ì´íŠ¸)
- ì¼ë¶€ ë°ì´í„°ë¥¼ í™œìš©í•˜ê¸° ë•Œë¬¸ì— ì—°ì‚°ìì›ì„ íš¨ìœ¨ì ìœ¼ë¡œ í™œìš© ê°€ëŠ¥í•¨($O(d^2n) -> O(d^2b)$ : ì—°ì‚°ëŸ‰ b/në§Œí¼ ê°ì†Œí•¨)
![image](https://user-images.githubusercontent.com/77658029/127457548-7697a65a-4fbc-4485-8230-cd355c57f433.png)

- ë³¼ë¡ì´ ì•„ë‹Œ(non-convex) ëª©ì ì‹ì€ SGDë¥¼ í†µí•´ ìµœì í™” ê°€ëŠ¥í•¨
- mini-batchëŠ” 10~1000ê°œì˜ ë°ì´í„°ë¥¼ ê°€ì§€ê³  í•¨
- ë”¥ëŸ¬ë‹ì—ì„œëŠ” SGDê°€ ì‹¤ì¦ì ìœ¼ë¡œ ë” ë‚«ë‹¤ëŠ” í‰ê°€ë¥¼ ë°›ìŒ

![image](https://user-images.githubusercontent.com/77658029/127513594-67c3d399-52a8-41cc-97c6-a4cb95a2ca8a.png)

ğŸ’¡ mini-batch SGD 

![image](https://user-images.githubusercontent.com/77658029/127514342-237efeb8-6344-4358-a818-e9535045f5b4.png)
![image](https://user-images.githubusercontent.com/77658029/127514249-95da0f34-1a68-4189-9822-6a5c85596d29.png)


## ê²½ì‚¬í•˜ê°•ë²• vs ë¯¸ë‹ˆë°°ì¹˜ SGD

![image](https://user-images.githubusercontent.com/77658029/127515340-fca8d0ed-c7c3-4330-9c2f-f9ef4563bd25.png)

**ğŸ“Œreference**
- boostcourse AI tech pre-course
- https://www.kakaobrain.com/blog/113


<br>
```
ğŸ’¡ ìˆ˜ì • í•„ìš”í•œ ë‚´ìš©ì€ ëŒ“ê¸€ì´ë‚˜ ë©”ì¼ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤!ğŸ’¡ 
```
