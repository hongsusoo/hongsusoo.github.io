---
defaults:
  - scope:
      path: ""
      type: posts
    values:
      layout: single
      author_profile: true
      comments: true
      share: true
      related: true

title: "[DL Basic] PyTorch ê¸°ì´ˆ ë¬¸ë²•"
excerpt: "about : python"
toc: true
toc_sticky: true
toc_label: "Label"
categories:
  - AI
tags:
  - [python, AI Tool, PyTorch]
date: 2021-08-17
last_modified_at: 2021-08-17
---

<br>

# PyTorch Basic grammar

## PyTorch Operations

- TensorëŠ” PyTorchì—ì„œ ë‹¤ì°¨ì› Arraysë¥¼ í‘œí˜„í•˜ëŠ” PyTorchì˜ í´ë˜ìŠ¤
- TensorëŠ” Numpyì— ndarrayì™€ ë™ì¼í•¨(TFì—ì„œì˜ Tensorë„ ë™ì¼í•¨)
- Numpyì™€ í•¨ìˆ˜êµ¬ì„±ì´ ê±°ì˜ ë™ì¼í•¨
- Tensor ìƒì„±ì€ listë„ ê°€ëŠ¥í•¨

<br>

**ğŸ“Œ Numpy ê¸°ëŠ¥**

1. Data Slicing
2. `tensor_1.flatten()` : 1ì°¨ì›ìœ¼ë¡œ í´ì¤Œ
3. `torch.ones_like(tensor_1)` : tensor_1ê³¼ ë™ì¼í•œ ì°¨ì›ì— ì›ì†ŒëŠ” ëª¨ë‘ 1ì¸ Tensor ìƒì„±
4. `tensor_1.numpy()` : tensorë¥¼ numpyë¡œ ë°”ê¿”ì¤Œ
5. `tensor_1.shape` : í˜„ì¬ Tensorì˜ shapeì„ ì¶œë ¥í•¨
6. `tensor_1.dtype` : í˜„ì¬ Tensorì˜ data typeì„ ì¶œë ¥í•¨
7. ì‚¬ì¹™ì—°ì‚° : ë™ì¼ ì°¨ì› Tensorì‚¬ì´ì˜ ì‚¬ì¹™ì—°ì‚° ë° scalar ì—°ì‚° ê°€ëŠ¥í•¨, (ë‚´ì , dotì—°ì‚°ì€ ë‹¤ë¦„)


<br>

### numpy to Tensor

**ğŸ“°code**
```python
import numpy as np
import torch

n_array = np.arange(10).reshape(2,5)

print("numpy :")
print(n_array)
print("ndim :", n_array.ndim, "shape :", n_array.shape)
print()
t_array = torch.FloatTensor(n_array)
print("Tensor :")
print(t_array)
print("ndim :", t_array.ndim, "shape :", t_array.shape)
```
**ğŸ”result**
```
numpy :
[[0 1 2 3 4]
 [5 6 7 8 9]]
ndim : 2 shape : (2, 5)

Tensor :
tensor([[0., 1., 2., 3., 4.],
        [5., 6., 7., 8., 9.]])
ndim : 2 shape : torch.Size([2, 5])
```

<br>

### CUDA

- Numpyì™€ ê°€ì¥ í° ì°¨ì´ì ìœ¼ë¡œ TensorëŠ” GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ
- `tensorëª….device` í˜•ì‹ìœ¼ë¡œ í˜„ì¬ Tensorì˜ ë°ì´í„°ê°€ GPU ë©”ëª¨ë¦¬ìƒì— ìˆëŠ”ì§€ CPUì— ìˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŒ

**ğŸ“°code**
```python
import numpy as np
import torch

n_array = np.arange(10).reshape(2,5)
t_array = torch.FloatTensor(n_array)
t_array.device

# if torch.cuda.is_available(): #GPUê°€ ìˆìœ¼ë©´ GPUì— ë„£ì–´ì¤Œ
#     t_array_cuda = t_array.to('cuda')
#     t_array_cuda.device
```
**ğŸ”result**
```
device(type='cpu')
```

<br>


### Strideì™€ contiguous

- Tensorì˜ ê²½ìš° stride í­ì„ ì´ìš©í•´ì„œ ì°¨ì›ì„ ê±´ë„ˆë›°ë©° ì ‘ê·¼í•¨
- ë§Œì•½ [2,3,3] ì°¨ì›ì˜ tensorì—ì„œ 0 ì°¨ì›ì—ì„œ 1ì°¨ì›ìœ¼ë¡œ ë„˜ì–´ê°ˆë•Œ 9($3*3$)ì˜ ë³´í­ìœ¼ë¡œ ì›€ì§ì—¬ ë‹¤ìŒ ì°¨ì›ì— ì ‘ê·¼í•˜ê²Œë¨
- ì´ëŸ° ê²½ìš°ë¥¼ contiguousí•œ ìƒíƒœë¼ê³ í•˜ëŠ”ë°, ë©”ëª¨ë¦¬ê°€ ì—°ì†ì ìœ¼ë¡œ ë“¤ì–´ ìˆëŠ” ê²½ìš°ë¥¼ ì˜ë¯¸í•¨
- transposeë¥¼ í•˜ê³ ë‚˜ë©´ ì´ëŸ° contiguousí•œ ìƒíƒœê°€ ê¹¨ì§€ê²Œ ëœë‹¤.
- ì´ëŸ´ê²½ìš° `.contiguous()` í•¨ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ ë‹¤ì‹œ ë©”ëª¨ë¦¬ë¥¼ ì—°ì†ì ìœ¼ë¡œ ë§Œë“¤ì–´ ì¤„ ìˆ˜ ìˆë‹¤.

**ğŸ“°code**
```python
import numpy as np
import torch

t_array = torch.FloatTensor(np.arange(10).reshape(2,5))
print(t_array.shape)
print(t_array.stride())
trans_t_array = t_array.T
print(trans_t_array.stride())
print(trans_t_array.contiguous().stride())
```
**ğŸ”result**
```
torch.Size([2, 5])
(5, 1)
(1, 5)
(2, 1)
```

<br>

### View vs Reshape

- viewì™€ reshape ëª¨ë‘ tensorì˜ ì°¨ì›ì„ handlingí•˜ëŠ” í•¨ìˆ˜ì´ë‹¤
- ë‘ í•¨ìˆ˜ì˜ ì°¨ì´ëŠ” contiguousê°€ ê¹¨ì¡Œì„ ë•Œ ì°¨ì´ê°€ ë°œìƒ
- viewëŠ” contiguousê°€ ê¹¨ì¡Œì„ ë•Œ errorë¥¼ ë°œìƒì‹œí‚¤ê²Œ ë˜ê³ , `.contiguous()` í•¨ìˆ˜ë¡œ ì •ìƒí™” í•  ìˆ˜ ìˆìŒ â†’ contiguityë¥¼ ë³´ì¥í•¨
- reshapeì€ contiguousê°€ ê¹¨ì¡Œì„ ë•Œ copyë¥¼ í•˜ì—¬ ë©”ëª¨ë¦¬ë¥¼ ìƒˆë¡œ í• ë‹¹í•˜ì—¬ ì§„í–‰í•˜ê²Œ ë˜ â†’ contiguityë¥¼ ë³´ì¥í•˜ì§€ ëª»í•¨
- contiguityë¥¼ ë³´ì¥í•œë‹¤ í•¨ì€ ê¸°ì¡´ ë©”ëª¨ë¦¬ë¥¼ ê³„ì† ì‚¬ìš©í•˜ëŠ”ê±¸ ì˜ë¯¸í•¨

**ğŸ“°code**
```python
import torch

a= torch.zeros(3,2)
b=a.T.reshape(6)
a.fill_(1)
print(a)
print(b)
```
**ğŸ”result**
```
tensor([[1., 1.],
        [1., 1.],
        [1., 1.]])
tensor([0., 0., 0., 0., 0., 0.])
```

<br>


### squeezeì™€ unsqueeze

![image](https://user-images.githubusercontent.com/77658029/129655872-b5b6cb8a-8bd1-4ea5-ad94-83e42bff1338.png)

**ğŸ“°code**
```python
import numpy as np
import torch

tensor_ex = torch.rand(size=(2,1, 2))
print('1.',tensor_ex.shape)
tensor_ex=tensor_ex.squeeze()
print('2.',tensor_ex.shape)
print('3.',tensor_ex.unsqueeze(0).shape)
print('4.',tensor_ex.unsqueeze(1).shape)
print('5.',tensor_ex.unsqueeze(2).shape)
```
**ğŸ”result**
```
1. torch.Size([2, 1, 2])
2. torch.Size([2, 2])
3. torch.Size([1, 2, 2])
4. torch.Size([2, 1, 2])
5. torch.Size([2, 2, 1])
```

<br>

### mm vs matmul

- matmulì˜ ê²½ìš° broadcasting ì§€ì›ì´ ë˜ê³ , mmì€ ì§€ì›ì´ ì•ˆë¨
- ë§Œì•½ ì•„ë˜ ì½”ë“œì—ì„œ aë¥¼ [[1,2]]ê°€ ì•„ë‹Œ [1,2]ë¡œ ë°”ê¾¸ë©´ error ë°œìƒ($(2)*(2,1)$ì—°ì‚° ì‹œ error ë°œìƒ)

**ğŸ“°code**
```python
a = torch.FloatTensor([[1,2]])
b = torch.FloatTensor([[3],[4]])
print(a.shape,b.shape)
print(a*b)
print(a.mm(b))
print(a.matmul(b))
```
**ğŸ”result**
```
torch.Size([1, 2]) torch.Size([2, 1])
tensor([[3., 6.],
        [4., 8.]])
tensor([[11.]])
tensor([[11.]])
```

<br>

### nn.functional

- ML/DLì—ì„œ ìœ ìš©í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì—¬ëŸ¬ í•¨ìˆ˜ë¥¼ ì§€ì›í•´ì¤Œ
- softmax, argmax, one_hot etc..
- `import torch.nn.functional as F` í˜•ì‹ìœ¼ë¡œ ì‚¬ìš©í•¨

<br>

### Auto Gradient

- ìë™ ë¯¸ë¶„ ê¸°ëŠ¥ìœ¼ë¡œ ì—°ì‡„ë²•ì¹™ë„ ìë™ì„ ì§„í–‰í•´ì¤Œ
- ì•ìœ¼ë¡œëŠ” Linearì— ë‚´ì¥ë˜ì–´ ìˆì–´ì„œ ê·¸ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ê²Œ ë  ì˜ˆì •
- `requires_grad=True`ê°€ ë˜ì–´ ìˆì–´ì•¼ ë¯¸ë¶„ì´ ê°€ëŠ¥í•¨


**ğŸ“°code**
```python
import numpy as np
import torch

w = torch.tensor(1.0, requires_grad=True)
y = w**3
z = 10*y + 2
z.backward()
w.grad
```
**ğŸ”result**
```
tensor(30.)
```

<br>

**ğŸ“Œreference**
- boostcourse AI tech
- [contiguousë€?](https://titania7777.tistory.com/3)
- [stackoverflow](https://stackoverflow.com/questions/61598771/pytorch-squeeze-and-unsqueeze)

<br>

```
ğŸ’¡ ìˆ˜ì • í•„ìš”í•œ ë‚´ìš©ì€ ëŒ“ê¸€ì´ë‚˜ ë©”ì¼ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤!ğŸ’¡ 
```
