---
defaults:
  - scope:
      path: ""
      type: posts
    values:
      layout: single
      author_profile: true
      comments: true
      share: true
      related: true

title: "[DL Basic] Optimization"
excerpt: "about : python"
toc: true
toc_sticky: true
toc_label: "Label"
categories:
  - AI
tags:
  - [python, AI Model, optimization]
date: 2021-08-10
last_modified_at: 2021-08-10
---

<br>

# Optimization

## Optimization ìš©ì–´ ì •ë¦¬ 

### Generalization

- Train dataì—ì„œ errorë¥¼ ì¤„ì¼ ìˆ˜ ìˆì§€ë§Œ 
- Test dataë¥¼ ë„£ì–´ ë°˜ë³µí•˜ë©´ error ê°€ ì»¤ì§ˆ ìˆ˜ ìˆë‹¤ 
- ì´ë•Œ Train data errorì™€ Test data errorì˜ ì°¨ì´ë¥¼ generalizaion gap ì´ë¼ê³  í•œë‹¤
- ì¢‹ì€ Generalizationê°€ ì¢‹ë‹¤ ë¼ê³ í•˜ë©´, Train dataì™€ Test data ì‚¬ì´ì˜ ì°¨ì´ê°€ ì‘ë‹¤

![image](https://user-images.githubusercontent.com/77658029/128792998-954a64ef-72ef-4741-8530-3204d4e258f3.png)

<br>

### Under-fitting vs Over-fitting

- ì´ë¡ ì ì¸ ì–˜ê¸°ë¡œ ì»¨ì…‰ìœ¼ë¡œ ì•Œê³  ê°€ë©´ ì¢‹ìŒ
- train dataê°€ test dataì˜ ì¼ë¶€ë¥¼ ê°€ì¡Œì™”ë‹¤ê³  ê°€ì •í–ˆì„ë•Œ
- Over-fitting : Train dataì˜ ì„¸ì„¸í•œ ë¶€ë¶„ê¹Œì§€ fittingë˜ì–´ test dataë¥¼ ì˜ ì„¤ëª…í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°
- Under-fitting : í•™ìŠµëŸ‰ì´ ë¶€ì¡±í•˜ê±°ë‚˜ ë„ˆë¬´ ì¼ë°˜í™”í•˜ì—¬ Train data ìì²´ë¥¼ ì˜ ë§Œì¡±í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°
- Balanced : ì ì ˆí•œ ê²½ìš°
 
<br>

### Cross Validation

- train dataë¥¼ ì—¬ëŸ¬ê°œë¡œ ë‚˜ëˆ ì„œ, ê·¸ ì¤‘ ì¼ë¶€ë¥¼ í•™ìŠµì‹œí‚¤ê³  ë‚¨ì€ train dataë¡œ validationì„ ì§„í–‰í•œë‹¤
- ë‹¤ìŒ ì‹œë„í•´ì„œëŠ” ë‹¤ë¥¸ ì¼ë¶€ë¥¼ í•™ìŠµì‹œí‚¤ê³  ë‹¤ì‹œ ë‚¨ì€ train dataë¡œ validationì„ ì§„í–‰í•œë‹¤
- ì´ëŸ° ì‘ì—…ë“¤ì„ í†µí•´ Hyperparameterë¥¼ ìµœì í™”
- hyper parameter : learning-rate, loss function, batch size, training íšŸìˆ˜(epoch) ë“± ìš°ë¦¬ê°€ ì •í•˜ëŠ” ê°’
- ì˜ˆì‹œë¡œ k-fold ë°©ë²•ì´ ìˆìŒ
- ìš°ë¦¬ëŠ” ì ˆëŒ€ë¡œ test dataë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤!

![image](https://user-images.githubusercontent.com/77658029/128794259-00507c8a-e44c-471e-b884-cbb3cec0f05c.png)

<br>

### Bias - Variance tradeoff

- variance : ë™ì¼í•œ ì…ë ¥ì„ ë„£ì—ˆì„ ë•Œ ì¶œë ¥ì´ ì–¼ë§ˆë‚˜ ì¼ê´€ì ì¸ê°€
- varianceê°€ ì»¤ì§€ë©´, í•œê³³ìœ¼ë¡œ ëª¨ì´ê³ , ì‘ì•„ì§€ë©´ í¼ì³ì§
- Bias : ë™ì¼í•œ ì…ë ¥ì—ì„œ í‰ê· ì ìœ¼ë¡œ True Targetê³¼ ì–¼ë§ˆë‚˜ ì°¨ì´ê°€ ìˆëŠ”ì§€
- Biasê°€ í¬ë©´ True Targetê³¼ ë©€ë¦¬ ë–¨ì–´ì§„ ìƒí™©ì´ê³ , ì‘ìœ¼ë©´ ê°€ê¹Œìš´ ìƒí™©ì„

![image](https://user-images.githubusercontent.com/77658029/128794533-a3781c71-840d-4775-ab61-42aa241d0e76.png)

**ğŸ’¡ trade off **

- ì–´ë–¤ errorë¥¼ ìµœì†Œí™” í•  ë•Œ, ì´ê²ƒì€ $Bias^2, Variance, Noise$ 3ê°€ì§€ë¡œ êµ¬ì„±í•  ìˆ˜ ìˆëŠ”ë°, 
- í•˜ë‚˜ê°€ ì‘ì•„ì§€ë©´, ë‹¤ë¥¸ í•­ëª©ì´ ì»¤ì§€ëŠ” Trade offê´€ê³„ê°€ ìˆìŒ

![image](https://user-images.githubusercontent.com/77658029/128794917-bd523592-24a0-47ec-9a0e-ab7ff9fb83b7.png)

<br>

### Bootstrapping(in statistics)

- train dataì—ì„œ ì „ì²´ë¥¼ ë‹¤ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì¼ë¶€ë¥¼ ë½‘ì•„ì„œ ì—¬ëŸ¬ê°œì˜ train dataë¥¼ ë§Œë“¤ì–´ì£¼ëŠ”ê±¸ bootstrappingì´ë¼ê³  í•¨
- ì´ë ‡ê²Œ ë§Œë“¤ì–´ì§„ ëª¨ë¸ì˜ consensusë¥¼ í™•ì¸í•˜ì—¬ ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ í™•ì¸í•¨

<br>

### Bagging and Boosting

- Bagging(**B**ootstrapping **agg**regat**ing**)
    - bootstrappingí•œ dataë¡œ ì—¬ëŸ¬ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚´
    - ì´ë ‡ê²Œ ë‚˜ì˜¨ ê²°ê³¼ê°’ì„ íˆ¬í‘œí•˜ê±°ë‚˜, í‰ê· ì„ ë‚´ì„œ ê²°ì •í•˜ê²Œë¨
    - ensembleì´ ì´ëŸ°ê²ƒì´ê³ , ì´ë ‡ê²Œ í–ˆì„ ë•Œ ê²°ê³¼ê°€ ì¢‹ì€ ê²½ìš°ê°€ ìˆìŒ(Kaggleì—ì„œ ë§ì´ ì‚¬ìš©í•¨)
- Boosting 
    - ì•½í•œ learnerë¥¼ ì—¬ëŸ¬ê°œë¥¼ sequentialí•˜ê²Œ ì—°ê²°í•˜ì—¬ í•˜ë‚˜ì˜ ê°•í•œ learnerë¥¼ ë§Œë“¤ì–´ì£¼ëŠ” ê²ƒ
    - í•œ ëª¨ë¸ì„ ë§Œë“¤ì—ˆëŠ”ë°, 100ê°œì¤‘ 60ê°œë§Œ êµ¬ë³„í•  ìˆ˜ ìˆë‹¤ë©´, ë‚˜ë¨¸ì§€ 40ê°œì— ëŒ€í•œ ë‹¤ë¥¸ ëª¨ë¸ì„ ë§Œë“¤ì–´ ì—°ê²°í•´ì£¼ëŠ” ë°©ì‹ì„ ë°˜ë³µí•´ì„œ ì—°ê²°í•¨

![image](https://user-images.githubusercontent.com/77658029/128797271-9ff37893-9857-407b-b476-d4495bfb2dfe.png)

<br>


## Practical Gradient Descent

- Stochastic Gradient Descent : single sample
- Mini Batch Gradient Descent : subset(minibatch : 128,256.. ë“± ì‚¬ìš©)
- Batch Gradient Descent : whole data

<br>

### Batch-Size Matters

- Bathc Sizeì— ë”°ë¼ì„œ training functionì˜ ê²½ì‚¬ë„ê°€ ë‹¬ë¼ì§
- Large batch Methodë¥¼ ì‚¬ìš©í•  ê²½ìš°, Sharp Training Functionì„ ì–»ì„ ìˆ˜ ìˆì–´ ì•½ê°„ì˜ ì˜¤ì°¨ì—ë„ í¬ê²Œ ë°˜ì‘ì„ í•˜ì—¬ ì„±ëŠ¥ì´ Test ìƒí™© ì‹œ ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ
- Small batch Methodë¥¼ ì‚¬ìš©í•  ê²½ìš°, Flat Training Functingì„ ì–»ì„ ìˆ˜ ìˆì–´, ì•½ê°„ì˜ ì˜¤ì°¨ì—ë„ ì‘ê²Œ ë°˜ì‘ì„ í•˜ì—¬ Test ìƒí™© ì‹œì„±ëŠ¥ì— í° ë³€í™”ê°€ ì—†ì„ ìˆ˜ ìˆìŒ

![image](https://user-images.githubusercontent.com/77658029/128798611-7bc50e96-5e3a-45f8-9a1c-341e4cd55af6.png)

<br>

### Gradient Descent

- gradientê°’ì— learning rateë¥¼ ê³±í•´ì¤˜ì„œ ë‹¤ìŒ í•­ìœ¼ë¡œ ë„˜ê²¨ì¤Œ
- learning rateì— ë”°ë¼ì„œ í¬ê²Œ ì¢Œìš°ëœë‹¤ëŠ” ë‹¨ì ì´ ìˆìŒ

![image](https://user-images.githubusercontent.com/77658029/128799663-1328008a-8e9a-4631-987e-d0a5d6603bb5.png)

<br>

### Momentum

- Gradientê°’ì— Momentumì´ë¼ëŠ” ì´ì „ Gradientì— ëŒ€í•œ ë¶€ë¶„ì„ ì¼ì •ë¶€ë¶„ ë°˜ì˜í•˜ì—¬ ë‹¤ìŒí•­ìœ¼ë¡œ ë„˜ê²¨ì¤Œ
- ë§Œì•½ local minimum ë¶€ë¶„ì„ ì§€ë‚˜ì¹˜ë©´, ë°”ë¡œ ë‹¤ì‹œ ëŒì•„ì˜¤ì§€ ëª»í•˜ê³  ì¢€ë” ì˜¬ë¼ ê°”ë‹¤ê°€ ë‚´ë ¤ì™€ì„œ ì¢€ ë” ëŠ¦ê²Œ ìˆ˜ë ´í•˜ê²Œ ë¨

![image](https://user-images.githubusercontent.com/77658029/128800100-e5849494-eab7-4a25-936d-6c846a97197f.png)

<br>

### Nesterov Accelerate

- ì´ë²ˆì— ê³„ì‚°ëœ gradientì™€ ë‹¤ìŒìœ¼ë¡œ ê°ˆ ê³³ì„ ë¯¸ë¦¬ ì—°ì‚°í•˜ì—¬ ë‘ ê°€ì§€ ë²¡í„°ë¥¼ ë”í•´ì¤Œ
- local minimumerì— ìˆ˜ë ´í•˜ëŠ” ì†ë„ê°€ ë¹ ë¦„

![image](https://user-images.githubusercontent.com/77658029/128800345-7aaf111d-0358-434d-95e5-647f3b951071.png)

![image](https://user-images.githubusercontent.com/77658029/128800440-698079d5-b0a1-4f93-bd7f-9b967f5b84a2.png)

<br>

### Adagrad

- ì•„ë‹¤ê·¸ë¼ë“œ
- ìˆ˜ì •ëŸ‰ì´ ìë™ìœ¼ë¡œ ì¡°ì •ë˜ëŠ”ë° ê·¸ ê°’ì´ $G_t$ë¼ëŠ” Gradient ì œê³±í•©ìœ¼ë¡œ ëŠ˜ì–´ë‚˜ê¸° ë•Œë¬¸ì— ì‹œë„ íšŸìˆ˜ê°€ ëŠ˜ì–´ë‚  ìˆ˜ë¡ ë³€í™”ìœ¨ì´ ì ì–´ì§€ê²Œë¨
- adapted learningì´ ì§„í–‰ë¨
- ìë™ìœ¼ë¡œ learning rateì´ ë³€í™”ë˜ëŠ” ì¥ì ì´ ìˆì§€ë§Œ
- íšŸìˆ˜ê°€ ëŠ˜ë©´ì„œ ë³€í™”ëŸ‰ì´ ì¤„ì–´ë“¤ì–´ ìˆ˜ë ´ì´ ì•ˆë˜ëŠ” ê²½ìš°ê°€ ë°œìƒí•¨

![image](https://user-images.githubusercontent.com/77658029/128802006-22ff1bd2-b0da-4d75-a91f-17da303e158f.png)

<br>

### Adadelta

- Adagradì—ì„œ $G_t$ê°’ì´ ê³„ì† ì»¤ì§€ëŠ”ê±¸ ë°©ì§€í•˜ê¸° ìœ„í•´ì„œ ë§Œë“  ëª¨ë¸
- learning rateê°€ ì—†ì–´ì„œ, ì‹¤ì œë¡œ ì¡°ì •í• ê²Œ ë§ì§€ ì•Šì•„ ìì£¼ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ

![image](https://user-images.githubusercontent.com/77658029/128807185-f941c17f-34dd-4005-a1eb-1cf528724971.png)

<br>

### RMSprop

- ì •ì‹ ë…¼ë¬¸ì€ ì—†ì§€ë§Œ Geoffrey Hintonì´ ì½”ì„¸ë¼ ê°•ì˜ì—ì„œ ì œì•ˆ
- ì•„ë‹¤ê·¸ë¼ë“œì—ì„œ learning rateê°€ ì¤„ì–´ë“œëŠ”ê±¸ ë³´ì™„í•´ì¤Œ

![image](https://user-images.githubusercontent.com/77658029/128807282-b041c70c-ec44-4dfa-a5db-fc251e281253.png)

<br>

### Adam(Adaptive Moment Estimation)

- Momentumê³¼ adapted ë‘ ê°€ì§€ë¥¼ ëª¨ë‘ ì‚¬ìš©í•œ ë°©ì‹

![image](https://user-images.githubusercontent.com/77658029/128807647-cdcb69ec-7533-496b-a1bf-df15084e3ed5.png)


## Regularization

**Early Stopping** : Generalization performanceê°€ ë‚˜ë¹ ì§€ê¸° ì „ì— ë©ˆì¶”ì

![image](https://user-images.githubusercontent.com/77658029/128808228-3ab13e3f-f884-471b-89aa-8aa04316bcfe.png)

**parameter Norm Penalty** 

    - ë¶€ë“œëŸ¬ìš´ í•¨ìˆ˜ì¼ ìˆ˜ ë¡ Generalization performaceê°€ ì¢‹ì„ ê²ƒì´ë‹¤ë¼ëŠ” ê°€ì •í•˜ì—ì„œ
    - í•™ìŠµì‹œí‚¬ ë•Œ, weightë„ í•¨ê»˜ ì¤„ì—¬ë‚˜ê°€ëŠ” ë°©ë²•( weight decayed)
    
![image](https://user-images.githubusercontent.com/77658029/128809166-17019e0d-08ad-4c59-8744-aed91cc61c81.png)

**Data Augmentation** 

    - dataëŠ” ë§ì„ ìˆ˜ë¡ ì¢‹ë‹¤
    
![image](https://user-images.githubusercontent.com/77658029/128809285-49c16225-11e3-4815-b77f-e93684c812f9.png)

    - í•˜ì§€ë§Œ ëŒ€ë¶€ë¶„ì˜ ê²½ìš° Training dataëŠ” í•œì •ì ì„
    - ê·¸ë ‡ê¸° ë•Œë¬¸ì— ë°ì´í„°ë¥¼ ëŒë¦¬ê³  ë°”ê¾¸ë©° ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•´ëƒ„
    
![image](https://user-images.githubusercontent.com/77658029/128809357-f4065a9f-f66c-4b99-889f-62b0a00380a7.png)


**Noise Robustness**

    - inputê³¼ weightsì— Noiseë¥¼ ë„£ê³  í•™ìŠµì‹œí‚¤ë©´, ë” ì„±ëŠ¥ì´ ì¢‹ì•„ì§
    
![image](https://user-images.githubusercontent.com/77658029/128809576-8de1f352-34c7-4545-9f12-b3b4656b523c.png)

**Label Smoothing**

    - ë°ì´í„° 2ê°œë¥¼ ë½‘ì•„ì„œ ì„ì–´ì£¼ëŠ” ì‘ì—…
    - ë¶„ë¥˜ë¬¸ì œë¥¼ í’€ë•Œ, decision boundaryë¥¼ ì°¾ê²Œë˜ëŠ”ë° ì´ë•ŒëŠ” ì´ê±° ì•„ë‹ˆë©´ ì´ê±° ì„ ì´ í™•ì‹¤í•˜ê²Œ ê·¸ì–´ì ¸ ìˆëŠ”ë°
    - ì´ decision boundaryë¥¼ ë¶€ë“œëŸ½ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ì‘ì—…

![image](https://user-images.githubusercontent.com/77658029/128809837-23b52bec-0c7c-4bb6-8fef-8843d3f581b0.png)

**Drop Out**

    - ëª‡ëª‡ ë‰´ëŸ°ì˜ ê°’ì„ zero ë°”ê¿”ì£¼ëŠ”ê²ƒ
    - ëª…í™•í•˜ê²Œ ìˆ˜í•™ì ìœ¼ë¡œ ë°í˜€ì§€ì§„ ì•Šì•˜ì§€ë§Œ, ì„±ëŠ¥í–¥ìƒë¨
   
![image](https://user-images.githubusercontent.com/77658029/128810056-d083f0bb-1ec9-4418-9aee-6119edb6fa98.png)

**Batch Normalization**

    - Layerë§ˆë‹¤ ì •ê·œí™”ë¥¼ ì§„í–‰í•¨(í‰ê· ì„ ë¹¼ì„œ, í‘œì¤€í¸ì°¨ë¡œ ë‚˜ëˆ ì¤Œ)
    - Layerê°€ ê¹Šê²Œ ìŒ“ì¼ ìˆ˜ë¡ ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ëŠ” í˜„ìƒì„ ë³´ì„

![image](https://user-images.githubusercontent.com/77658029/128810486-273f6424-8115-433b-8b4a-fa9639becbf1.png)

<br>

**ğŸ“Œreference**
- boostcourse AI tech
- [from the bottom](https://hyeonnii.tistory.com/232)

<br>

```
ğŸ’¡ ìˆ˜ì • í•„ìš”í•œ ë‚´ìš©ì€ ëŒ“ê¸€ì´ë‚˜ ë©”ì¼ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤!ğŸ’¡ 
```
