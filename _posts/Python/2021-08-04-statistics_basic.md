---
defaults:
  - scope:
      path: ""
      type: posts
    values:
      layout: single
      author_profile: true
      comments: true
      share: true
      related: true

title: "[AI basic] 통계론(statistics)"
excerpt: "about : python"
toc: true
toc_sticky: true
toc_label: "Label"
categories:
  - AI basic
tags:
  - [python, AI Math]
date: 2021-08-04
last_modified_at: 2021-08-04
---
<br>

# 통계학

- 통계적 모델링은 
통계학 적절한 가정위에서 data를 표현하는 확률분포를 추정하는것이 목표
정답의 분포를 알 수 없고, 다양한 확률분포가 있다보니, 어떤 확률분포를 사용하냐! 선택의 문제 

유한한 개수의 데이터만 가지고 모집단을 정확하게 알아내는건 불가능함 -> 근사적 확률분포를 추정!

데이터와 추정방법의 불확실성을 고려하고 예측의 위험을 최소화 하는 방향으로!

확률분포를 추정하는것을 목표로

데이터가 특정 확률분포를 따른다고 선험적으로 (a priori) 가정한 이후 그 분포를 결정하는 모수를 추정하는 방법

모수적 방법(parametric)
정규분포를 예로들면
평균,분산 -> 모수, 
추정하는 방법을 통해서 데이터를 학습시키는게 모수적 방법론
평균,분산을 추정하는게 모수적 방법론

비모수적(nonparametric)
모수가 있지만, 유기적으로 변하는것!
가정을 따로 하지 않음

특정 확률분포를 가정하지않고 데이터에 따라서 모델의 구조와 모수의 갯수가 유연하게 바뀔떄




## 모수

- 

## 확률분포 가정하기 

- 우선 히스토그램을 통해 모양을 관찰함
- 만약 데이터가 2개의 데이터만 갖음 -> 베르누이,이항분포
- n개의 이산적인 값을 가지는 경우 -> 카테고리 분포
- 데이터가 [0,1]사이에서 값을 가지는 경우 -> 베타분포
- 데이터가 0이상의 값을 가지는 경우 -> 감마분포, 로그정규분포
- 데이터가 R전체에서 값을 가지는 경우 -> 정규분포, 라플라스분포

💡 기계적으로 확률분포를 가정하면 안되고, 데이터를 생성하는 원리를 먼저 고려하는것이 원칙!

모수 추정후 통계적 검정까지 이뤄져야함

## 데이터로 모수를 추정해보자

- 정규분포의 모수는 평균 μ,분산 $σ^2$
- 표본분산을 구할때 N-1로 나눠주면 불편(unbiased)추정량을 구할 수 있다
- N-1은 자유도와 관련이 있는데, 자유도는 독립변수의 개수
- 불편추정량 -> 편의가 없는 추정량

![image](https://user-images.githubusercontent.com/77658029/128108435-34b53596-6ea3-46b9-9ce1-1f11416c4266.png)

통계량의 확률분포를 표집분포(sampling distribution)라 부르며, 특히 표본평균의 표집분포는 N이 커질수록 정규분포 $\mathscr N(\mu,\sigma^2/N)$를 따른다
- 중심극한정리(Central Limit Theorem)이라 부르며 모집단의 분포가 정규분포를 따르지 않아도 성립함
💡 표본분포(sample distribution)랑은 다름 

베르누이 분포(이항분포)에서 N에 따른 표집분포의 변화
![image](https://user-images.githubusercontent.com/77658029/128109141-413c5190-c62e-4a8b-bc7c-01a7ef532cb3.png)


## 최대가능도 추정법(Maximum likelihood estimation, MLE)

- 표본평균이나 표본분산은 유용한 통계량이지만, 확률분포마다 사용하는 모수가 다르므로 적절한 통계량이 달라지게됨
- 이론적으로 가장 가능성이 높은 모수를 추정하는 방법 중 하나가 최대가능도 추정법
- 가능도(likelihood) 함수는 모수 $\theta$를 따르는 분포가 X를 관찰할 가능성(확률이랑은 다름)

![image](https://user-images.githubusercontent.com/77658029/128110103-39764d71-a8a9-4d60-ba30-9bea1cb5d0c2.png)

- 데이터 집합 X가 독립적으로 추출되었을 경우 로그가능도를 최적화
- 0~1사이의 숫자를 수억번 곱해주게 되면, 컴퓨터 오차가 커지게 된다. 그렇기 때문에 log화 시킨 후 덧셈으로 처리하여 오차를 최소화 시킬수 있다
- 경사하강법으로 가능도를 최적화 할때 미분연산을 사용하게 되는데, 로그 가능도를 사용하면 연산량을 $O(n^2)$에서 $O(n)$으로 줄여줌
- 대부분의 손실함수의 경우 경사하강법을 사용하므로 음의 로그가능도(negative log-likelihood)를 최적화함

![image](https://user-images.githubusercontent.com/77658029/128110058-da01c2e6-a08f-4752-af82-e61991998654.png)


### 최대가능도 추정법 예제: 정규분포

- 정규분포를 따른 확률변수 X로 부터 독립적인 표본 n개를 얻었을 경우
- 


<br>
```
💡 수정 필요한 내용은 댓글이나 메일로 알려주시면 감사하겠습니다!💡 
```
