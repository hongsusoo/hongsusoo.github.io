---
defaults:
  - scope:
      path: ""
      type: posts
    values:
      layout: single
      author_profile: true
      comments: true
      share: true
      related: true

title: "[paper review] ViT(Vision Transformer)_2020"
excerpt: "about : pr"
toc: true
toc_sticky: true
toc_label: "Label"
categories:
  - AI Paper Review
tags:
  - 
date: 2021-11-12
last_modified_at: 2021-11-12
---
<br>

# Vision Transformer Review

![image](https://user-images.githubusercontent.com/77658029/141405793-2660d30b-1a61-45c9-8f22-d6c925f00652.png)

## ABSTRACT

![image](https://user-images.githubusercontent.com/77658029/141405980-39d89dd3-4457-482e-8921-699983135613.png)

Transformer는 NLP Task에서는 표준처럼 사용되고 있지만, CV에서는 아직 제약이 있다. 단순하게 CNN의 큰 틀은 남겨두고 중간에 끼워 넣는 씩으로만 Attention을 사용하고 있는데, 이번 논문에서는 CNN의 요소로써 Attention을 사용하는 것이 아닌, image patch를 이용하여 순수한 Transformer를 적용하여 성능을 향상 시켰다고 한다. 많은 양의 데이터를 Pretrained 시키고 여러가지 recognition benchmark(ImageNet, CIFAR-100, VTAB 등)에 대해 Transfer learning을 수행할 경우 Vivion Transformer(ViT)는 CNN SoTA model과 비교해도 훨씬 적은 computational resource을 가지며 동시에 우수한 결과를 얻을 수 있다


## INTRODUCTION

![image](https://user-images.githubusercontent.com/77658029/141407106-ad223712-d669-4ef9-ae20-ac5ad840ecda.png)

Transformer는 효율적인 연산과 확장성의 특징을 가지고 있고, 모델과 데이터셋이 커지더라도 성능이 Saturation 되지 않는 성질을 가지고 있다. 이런 Transformer는 NLP에서 표준으로 사용되고 있다.

![image](https://user-images.githubusercontent.com/77658029/141407305-ca910c84-a997-4386-8d8b-3e9260a5a79f.png)

CV에서도 이런 Transformer를 이용한 시도들이 있었고, 일부는 CNN을 완전히 대체한 것도 있었지만 이론적으로는 효율적이지만 Specialized Attention Pattern을 사용하여 Hardware 문제로 아직 사용이 어려운 상황이다. Image Size가 커지면 결국 CNN Architecture를 가진 기존 SoTA모델들 이길 수가 없었다

![image](https://user-images.githubusercontent.com/77658029/141407532-3d19e175-9b6f-4510-81cc-11d7c5e918fa.png)

ViT 논문 팀에서는 이런 NLP의 Transformer의 성공에서 영감을 받아 가능한 순수한 Transformer를 그대로 사용하고자 했다. 그러기 위해서 Image를 Patch로 분할하고 이러한 Patch들을 Embedding Vector로 변환하여 Transformer의 입력하는 시도를 한다. 이 경우 Image Patch를 NLP Task의 Word와 동일한 방식으로 본다고 생각할 수 있다.

![image](https://user-images.githubusercontent.com/77658029/141407919-487fcca9-7421-45e6-b4b7-1124ee6d139e.png)
![image](https://user-images.githubusercontent.com/77658029/141407946-0c95f310-0295-4469-9138-668d0e77db68.png)

CNN의 경우 Inductive bias(귀납 편향, 경험에 의한 편향)으로 인해 일반화를 잘 시키지만, Transformer의 경우 이런 CNN의 Inductive bias 성질이 없어 data가 많아야 일반화가 가능하다.

CNN의 경우 물체의 위치가 변하더라도 인지할 수 있음(locality)
-> Feature map도 물체의 움직임에 따라 동일하게 이동하게됨(Translation equivariance)
-> 만약 classification task의 경우 CNN은 Translation invariance 성질을 갖게됨

- 모델의 일반적이 문제 -> inductive bias가 이런 부분들을 완화시켜줌..?
    1. Models are brittle : 모델 자체가 불안정함
        - 데이터가 조금만 바뀌어도 결과가 안좋음
    2. Models are spurious : 겉으로만 그럴듯한 모델
        - 결과와 편향을 학습(이미지의 본래의 모습이 아닌)


![image](https://user-images.githubusercontent.com/77658029/141408679-d0ef04b3-8161-48b4-ac96-a9bf665cfed3.png)

하지만 large-scale dataset(14M-300M Images)에서 모델을 학습하면 inductive bias가 없는 Transformer가 더 효과적인 학습을 하는 것을 알게되었다. inductive bias는 과적합시키는 부분이 있기 때문에 이런 부분이 적은 Transformer는 데이터가 많아질 수록 학습에 유리한 부분이다.

## RELATED WORK

![image](https://user-images.githubusercontent.com/77658029/141408916-0021ad1a-011d-432f-b390-88e5cde89bcd.png)

- Transformers 종류
    - BERT : Self-Supervised Pre-Training Task
    - GPT : 일방향 언어 모델링
![image](https://user-images.githubusercontent.com/77658029/141409112-4af4f278-6075-464f-ac1c-065f590e9fb9.png)

![image](https://user-images.githubusercontent.com/77658029/141408932-f2c3be67-c177-452f-aefe-4df022a675c8.png)

- Naive Application (Self-Attention을 적용하려는 초기 시도)
    - Parmar : 전역적(Globally)으로 적용 X → 쿼리(Query) 픽셀 주변(Local Neighborhood)에만 Self-Attention 적용
    - Weissenborn : 다양한 크기의 Block을 Scale Attention 적용,  But, 효율적인 GPU 사용을 위한 복잡한 엔지니어링 필요


![image](https://user-images.githubusercontent.com/77658029/141409346-ae062ab5-16e7-4c6b-80a8-a1abc110f9c8.png)

ViT model과 연관있는 모델은 Cordonnier model로 2x2 Image Patch를 추출하여, Full Self-attention 사용한 모델이다. 하지만 2x2 Image Patch를 사용하므로 작은 해상도(Resolution)에만 적용가능하단 단점이 있다.


![image](https://user-images.githubusercontent.com/77658029/141409729-f8fd37a2-c5c4-4dd0-8a2d-1b223d50327a.png)

CNN과 Self Attention을 위한 여러 시도들이 있었는데 그중 최신에 나온 Image GPT(iGPT) model이다. Generative Model처럼 학습하고, 해상도와 Color Space를 축소하는 방식을 사용했고, Transformers를 Image Pixel 단위로 적용했다. 

![image](https://user-images.githubusercontent.com/77658029/141410398-a59a07ef-5f34-4e11-be74-c73fdd77c6ea.png)

논문의 저자는 Transformer의 ImageNet-21k, JFT-300M의 Large Scale의 데이터셋의 학습에 대한 경험적 탐색을 수행하는 것에 초점을 두고 있다.


## METHOD

![image](https://user-images.githubusercontent.com/77658029/141410956-3107cb4a-2fc0-480c-9534-d3de0ba07ffe.png)

image patch (Query) → encoder(head(key, value)) → embedding vector?

### 3.1 ViT
![image](https://user-images.githubusercontent.com/77658029/141410669-1671d65d-3100-420f-8b2c-9e1351d7cc38.png)

Transformer는 쉽게 Setup할 수 있고, 확장가능성에 대한 이점이 있기 때문에 가능한한 순수한 Transformer 형식을 유지하기 위해 노력했다. 


![image](https://user-images.githubusercontent.com/77658029/141410841-b3c6db25-3131-4649-a25b-eafa70899a43.png)

기존 Transformer를 사용하기 위해 Image Patch를 Flatten 하여 사용하였고 또한 Transformer에 동일한 크기로 입력해주기위해 D Dimensions로 Linear Projection 진행하였다.(아래 수식)

![image](https://user-images.githubusercontent.com/77658029/141411419-2b14d6f7-b557-4a2d-934c-6985fb7e844f.png)

- reshape : HxWxC → Nx(P^2xC)
- (P,P) : Image Patch split 수
- N : Patch의 수?
- D : Linear Projection(E)을 사용하여 D차원으로 매핑

![image](https://user-images.githubusercontent.com/77658029/141412137-d169f86b-7aa4-4132-91cf-949322ae9371.png)

BERT의 CLASS Token 처럼 학습 가능한 Embedding Patch를 추가하고
Pre-train 및 fine-tuning중에 classification head는 Z^0_L에 추가된다.

![image](https://user-images.githubusercontent.com/77658029/141412365-67c9bfd9-e1c3-41e8-b9d2-78e3172442a4.png)

![image](https://user-images.githubusercontent.com/77658029/141414722-ffc836d2-e71f-4b3b-9faf-4797fe102f06.png)

Patch의 Embedding Position을 유지하기 위해서 Position Embedding를 추가했고, 논문에서는 1D Position Embedding을 사용하였는데, 2D Position Embedding을 사용해도 성능에 큰 차이가 없어 1D로 사용하였다.


### 수식

![image](https://user-images.githubusercontent.com/77658029/141415845-ff1fa485-5c79-4380-a2e7-33735244c829.png)

### FINE TUNING AND HIGHER RESOLUTION

![image](https://user-images.githubusercontent.com/77658029/141415936-39e5f1c0-ffe1-47bc-b2a9-b7ea414bd93b.png)

일반적으로 large-scale dataset에 대해 ViT를 pre-train하고 downstream task에 대해 fine-tuning을 수행한다. 이를 위해 pre-trained prediction head를 제거하고 0으로 초기화된 KxD feedforward layer를 추가한다
Pre-train 보다 높은 resolution으로 fine-tuning하는것은 종종 도움이 된다. 더 높은 resolution의 이미지를 feed할 때 patch 크기를 동일하게 유지하므로 sequence length가 더 길어지게 된다. 이때 PreTrained 모델을 사용하기 위해서는 Sequence Length를 맞춰주기 위해서 2D Interpolation을 수행하게 되고, 이때 ViT에 Inductive Bias가 심어지는 효과가 생긴다. 


## EXPERIMENTS

![image](https://user-images.githubusercontent.com/77658029/141416415-28b9fe76-66c7-4028-a5a1-ea1983ce17a5.png)

pre-training 계산비용을 고려할 때 ViT는 더 낮은 비용으로 대부분의 recognition benchmark에서 SotA를 달성하였다.

### 4.1 SETUP

#### Datasets
- pretrained 
    1. ILSVRC-2012 ImageNet dataset(ImageNet) - 1k 클래스 및 1.3M 이미지
    2. ImageNet-21k - 21,000 클래스 및 14M 이미지
    3. JFT - 18k 클래스 및 303M 이미지
- Transfer Learning dataset
    1. ImageNet 및 ReaL labels
    2. CIFAR 10/100
    3. Oxford-IIIT Pets
    4. Oxford Flowers-102
    5. 19-task VTAB classification suite

#### Model Variants

BERT에 사용되는 구성을 기반으로 하고, 접미사는 “B”(Base), “L”(Large), “H”(Huge) 를 뜻함
예를들어 ViT-L/16 의 경우는 Large 사이즈 이며 16x16 의 패치크기를 가짐

#### Training & Fine-tuning

- Pre-train
    - Adam optimizer, b_1 = 0.9, b_2=0.999, batch size=4096
    - weight decay: 0.1

- Fine-tuning
    - SGD w/ momentum, batch size=512batchsize=512
    - using linear learning rate warmup and decay
    - Higher resolution: 512 for ViT-L/16 and 518 for ViT-H/14


### 4.2  COMPARISON TO STATE OF THE ART

![image](https://user-images.githubusercontent.com/77658029/141417543-5353b442-897f-4e69-b8b2-0afdf78e7c78.png)

![image](https://user-images.githubusercontent.com/77658029/141417498-1eaac578-3423-4140-b242-0f554dad7d53.png)

모든 모델은 TPUv3에서 학습되었으며 pre-train에 소요된 일수를 확인할 수 있다.

ViT-L/16 모델은 모든 dataset에서 BiT-L과 일치하거나 더 좋은 성능을 보여줌과 동시에 훨씬 적은 computational resource를 필요로 한다. 더 큰 모델인 Vit-H/14는 ImageNet 및 CIFAR-100과 VTAB에서 성능이 더욱 향상되었다.

![image](https://user-images.githubusercontent.com/77658029/141417753-c6cd8993-71f8-4f5f-99d6-c5347fa4fb63.png)

VTAB : ImageNet의 GLUE 같은 평가 지표

### 4.3  PRE-TRAINING DATA REQUIREMENT

![image](https://user-images.githubusercontent.com/77658029/141418309-152c5704-4f81-442d-be09-9c105a0da112.png)

Dataset의 크기에 따라 성능이 얼마나 바뀔까?

![image](https://user-images.githubusercontent.com/77658029/141418370-356783c6-e0a3-435f-a6e2-fc3f2459fc49.png)

![image](https://user-images.githubusercontent.com/77658029/141418404-ff9e78b0-68ac-4eb7-aa2e-e0d0d3a258b9.png)

![image](https://user-images.githubusercontent.com/77658029/141418601-5c4bc401-1ba1-4207-8ba7-400be9519b0a.png)

![image](https://user-images.githubusercontent.com/77658029/141418653-1c7fd563-3d34-4d63-9d5d-2eff27cdfd94.png)

가장 작은 dataset에 대해 pre-train을 수행한 경우 ViT-Large 모델은 정규화에도 불구하고 ViT-Base보다 성능이 떨어진다. 그러나 ImageNet-21k dataset을 사용하면 성능이 비슷하다. JFT-300M dataset에서는 ViT가 BiT보다 더 좋은 성능을 보여줌을 확인할 수 있다.

결과적으로 더 작은 dataset의 경우 convolutional inductive bias가 유리하지만 큰 dataset의 경우 관련 패턴을 학습하는 것이 충분하고 이점이 있다는 것을 알수 있다. 


### 4.4 SCALING STUDY

![image](https://user-images.githubusercontent.com/77658029/141419168-cf8b780e-af8e-449c-baeb-2a23cc8eaa22.png)

JFT-300M dataset에서 transfer performance에 대해 다양한 모델의 확장 연구를 수행했다. Dataset 크기는 Transfer Performance에 큰 영향이 없었다.
그래서, 여러 조건에서 여러 모델을 transfer performance vs pre-training compute에 대한 실험을 진행하였다.

![image](https://user-images.githubusercontent.com/77658029/141418898-c32aba45-4f84-4ed6-8541-da2436205d80.png)

연산량 또한 같은 수준의 Accuracy에서 보면 BiT 모델보다 ViT가 적은 수준인 것을 확인할 수 있고, Hybrid도 정확도가 높아질 수록 ViT가 좋은 성능을 가지는 것을 확인 할 수 있다.

### INSPECTING VISION TRANSFORMER

![image](https://user-images.githubusercontent.com/77658029/141419518-3c1378fd-c792-4352-a360-1db26aaca38d.png)

ViT가 이미지를 처리하는 방법을 이해하기위해 internal representations을 분석해보았다. ViT의 Flatten 된 Patch를 2D로 Projection한 Space를 확인하였다.

![image](https://user-images.githubusercontent.com/77658029/141419923-078b4506-d139-43ec-ac1d-63d7d24357fb.png)

학습된 learned Embedding filter는 마치 CNN의 Low dimensional feature map과 동일한 양상을 보인다. 

![image](https://user-images.githubusercontent.com/77658029/141420128-348ecc0e-7894-4700-8235-bc60a18cef07.png)

위 사진은 학습된 Position Embedding의 경우를 보여주는데, 이것은 model이 image안에서 비슷한 position을 학습한다고 생각할 수 있다. 더 가까운 patch는 더 유사한 position embedding을 갖는 경향이 있으며 하나의 Patch기준으로 행과 열에 비슷한 경향성을 확인 할 수 있다.

![image](https://user-images.githubusercontent.com/77658029/141420454-31fc4f72-99c9-4d38-9a47-951906390d98.png)

![image](https://user-images.githubusercontent.com/77658029/141420605-0f113041-4d79-4c7e-bcd3-6b6f87095e82.png)

CNN의 Receptive Field와 비슷하게 ViT에서도 Attention Distance를 계산할 수 있다. Self-attention의 weight를 기반으로 이미지 공간의 평균거리를 계산할 수 있다. 이 부분을 확인해보면 lowest layer에서도 globally한 정보를 가지고 있다는 것을 확인 할 수 있다.


![image](https://user-images.githubusercontent.com/77658029/141421151-f69bcb16-ecf0-43e8-a1d0-f7fbd7d44740.png)

모델이 의미상의 분류와 관련된 이미지 영역을 담당한다는 것을 알 수 있다.


### 4.6 SELF-SUPERVISION -> pseudo labeling?


![image](https://user-images.githubusercontent.com/77658029/141421260-a235dc87-2d61-49ab-98f6-06cfc209f85f.png)
![image](https://user-images.githubusercontent.com/77658029/141421278-c415faed-f35b-4c4d-842f-cac6da1e3e5a.png)

Transformer는 NLP task에서 인상적인 성능을 보여줬는데, 확장성과 함께 self-supervised pre-training에서 비롯된다. 또한 BERT에서 사용되는 MLM(Masked Language Modeling)을 모방하여 self-supervision을 위한 Masked Patch Prediction에 대한 예비탐색하여 이후 작업에 사용한다.?


## 5. CONCLUSION

![image](https://user-images.githubusercontent.com/77658029/141421835-45b5bd21-6418-48b6-b7fe-2d70fab9561b.png)

이번 논문은 Image Recognition에 Transformer를 적용하기 위한 여러 탐색들을 진행했다. 이전 CV연구들은 Image-specific Inductive Baises를 사용하였지만, 본 연구는 Self-attention을 적용하였다.(초기 patch 추출할때에만 사용됨)
Transformer의 간단하면서도 확장가능한 이점은 large-scale dataset에 대한 pre-train과 결합될 때 효과적있었고, 따라서 Vision Transformer는 많은 image classification dataset에서 SotA를 능가하고 pretrain의 학습 비용이 상대적으로 저렴하다. 
추가적으로 아직 dctection 및 segmentation Task에 적용과 pre-training method에 대해 연구가 필요하다. 또한 Scaling후에도 아직 Saturate 상태가 아니기 때문에 추가적인 성능 향상이 기대된다.

**📌reference**
- [ViT 논문](https://arxiv.org/pdf/2010.11929.pdf)
- [Deep Learner](https://jeonsworld.github.io/vision/vit/)
- [KM-hana](https://kmhana.tistory.com/27)

<br>
```
💡 수정 필요한 내용은 댓글이나 메일로 알려주시면 감사하겠습니다!💡 
```
