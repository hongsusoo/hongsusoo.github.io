---
defaults:
  - scope:
      path: ""
      type: posts
    values:
      layout: single
      author_profile: true
      comments: true
      share: true
      related: true

title: "[paper review] ViT(Vision Transformer)_2020"
excerpt: "about : pr"
toc: true
toc_sticky: true
toc_label: "Label"
categories:
  - AI Paper Review
tags:
  - 
date: 2021-11-12
last_modified_at: 2021-11-12
---
<br>

# Vision Transformer Review

![image](https://user-images.githubusercontent.com/77658029/141405793-2660d30b-1a61-45c9-8f22-d6c925f00652.png)

## ABSTRACT

![image](https://user-images.githubusercontent.com/77658029/141405980-39d89dd3-4457-482e-8921-699983135613.png)

TransformerëŠ” NLP Taskì—ì„œëŠ” í‘œì¤€ì²˜ëŸ¼ ì‚¬ìš©ë˜ê³  ìˆì§€ë§Œ, CVì—ì„œëŠ” ì•„ì§ ì œì•½ì´ ìˆë‹¤. ë‹¨ìˆœí•˜ê²Œ CNNì˜ í° í‹€ì€ ë‚¨ê²¨ë‘ê³  ì¤‘ê°„ì— ë¼ì›Œ ë„£ëŠ” ì”©ìœ¼ë¡œë§Œ Attentionì„ ì‚¬ìš©í•˜ê³  ìˆëŠ”ë°, ì´ë²ˆ ë…¼ë¬¸ì—ì„œëŠ” CNNì˜ ìš”ì†Œë¡œì¨ Attentionì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ, image patchë¥¼ ì´ìš©í•˜ì—¬ ìˆœìˆ˜í•œ Transformerë¥¼ ì ìš©í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒ ì‹œì¼°ë‹¤ê³  í•œë‹¤. ë§ì€ ì–‘ì˜ ë°ì´í„°ë¥¼ Pretrained ì‹œí‚¤ê³  ì—¬ëŸ¬ê°€ì§€ recognition benchmark(ImageNet, CIFAR-100, VTAB ë“±)ì— ëŒ€í•´ Transfer learningì„ ìˆ˜í–‰í•  ê²½ìš° Vivion Transformer(ViT)ëŠ” CNN SoTA modelê³¼ ë¹„êµí•´ë„ í›¨ì”¬ ì ì€ computational resourceì„ ê°€ì§€ë©° ë™ì‹œì— ìš°ìˆ˜í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤


## INTRODUCTION

![image](https://user-images.githubusercontent.com/77658029/141407106-ad223712-d669-4ef9-ae20-ac5ad840ecda.png)

TransformerëŠ” íš¨ìœ¨ì ì¸ ì—°ì‚°ê³¼ í™•ì¥ì„±ì˜ íŠ¹ì§•ì„ ê°€ì§€ê³  ìˆê³ , ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì´ ì»¤ì§€ë”ë¼ë„ ì„±ëŠ¥ì´ Saturation ë˜ì§€ ì•ŠëŠ” ì„±ì§ˆì„ ê°€ì§€ê³  ìˆë‹¤. ì´ëŸ° TransformerëŠ” NLPì—ì„œ í‘œì¤€ìœ¼ë¡œ ì‚¬ìš©ë˜ê³  ìˆë‹¤.

![image](https://user-images.githubusercontent.com/77658029/141407305-ca910c84-a997-4386-8d8b-3e9260a5a79f.png)

CVì—ì„œë„ ì´ëŸ° Transformerë¥¼ ì´ìš©í•œ ì‹œë„ë“¤ì´ ìˆì—ˆê³ , ì¼ë¶€ëŠ” CNNì„ ì™„ì „íˆ ëŒ€ì²´í•œ ê²ƒë„ ìˆì—ˆì§€ë§Œ ì´ë¡ ì ìœ¼ë¡œëŠ” íš¨ìœ¨ì ì´ì§€ë§Œ Specialized Attention Patternì„ ì‚¬ìš©í•˜ì—¬ Hardware ë¬¸ì œë¡œ ì•„ì§ ì‚¬ìš©ì´ ì–´ë ¤ìš´ ìƒí™©ì´ë‹¤. Image Sizeê°€ ì»¤ì§€ë©´ ê²°êµ­ CNN Architectureë¥¼ ê°€ì§„ ê¸°ì¡´ SoTAëª¨ë¸ë“¤ ì´ê¸¸ ìˆ˜ê°€ ì—†ì—ˆë‹¤

![image](https://user-images.githubusercontent.com/77658029/141407532-3d19e175-9b6f-4510-81cc-11d7c5e918fa.png)

ViT ë…¼ë¬¸ íŒ€ì—ì„œëŠ” ì´ëŸ° NLPì˜ Transformerì˜ ì„±ê³µì—ì„œ ì˜ê°ì„ ë°›ì•„ ê°€ëŠ¥í•œ ìˆœìˆ˜í•œ Transformerë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê³ ì í–ˆë‹¤. ê·¸ëŸ¬ê¸° ìœ„í•´ì„œ Imageë¥¼ Patchë¡œ ë¶„í• í•˜ê³  ì´ëŸ¬í•œ Patchë“¤ì„ Embedding Vectorë¡œ ë³€í™˜í•˜ì—¬ Transformerì˜ ì…ë ¥í•˜ëŠ” ì‹œë„ë¥¼ í•œë‹¤. ì´ ê²½ìš° Image Patchë¥¼ NLP Taskì˜ Wordì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ë³¸ë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤.

![image](https://user-images.githubusercontent.com/77658029/141407919-487fcca9-7421-45e6-b4b7-1124ee6d139e.png)
![image](https://user-images.githubusercontent.com/77658029/141407946-0c95f310-0295-4469-9138-668d0e77db68.png)

CNNì˜ ê²½ìš° Inductive bias(ê·€ë‚© í¸í–¥, ê²½í—˜ì— ì˜í•œ í¸í–¥)ìœ¼ë¡œ ì¸í•´ ì¼ë°˜í™”ë¥¼ ì˜ ì‹œí‚¤ì§€ë§Œ, Transformerì˜ ê²½ìš° ì´ëŸ° CNNì˜ Inductive bias ì„±ì§ˆì´ ì—†ì–´ dataê°€ ë§ì•„ì•¼ ì¼ë°˜í™”ê°€ ê°€ëŠ¥í•˜ë‹¤.

CNNì˜ ê²½ìš° ë¬¼ì²´ì˜ ìœ„ì¹˜ê°€ ë³€í•˜ë”ë¼ë„ ì¸ì§€í•  ìˆ˜ ìˆìŒ(locality)
-> Feature mapë„ ë¬¼ì²´ì˜ ì›€ì§ì„ì— ë”°ë¼ ë™ì¼í•˜ê²Œ ì´ë™í•˜ê²Œë¨(Translation equivariance)
-> ë§Œì•½ classification taskì˜ ê²½ìš° CNNì€ Translation invariance ì„±ì§ˆì„ ê°–ê²Œë¨

- ëª¨ë¸ì˜ ì¼ë°˜ì ì´ ë¬¸ì œ -> inductive biasê°€ ì´ëŸ° ë¶€ë¶„ë“¤ì„ ì™„í™”ì‹œì¼œì¤Œ..?
    1. Models are brittle : ëª¨ë¸ ìì²´ê°€ ë¶ˆì•ˆì •í•¨
        - ë°ì´í„°ê°€ ì¡°ê¸ˆë§Œ ë°”ë€Œì–´ë„ ê²°ê³¼ê°€ ì•ˆì¢‹ìŒ
    2. Models are spurious : ê²‰ìœ¼ë¡œë§Œ ê·¸ëŸ´ë“¯í•œ ëª¨ë¸
        - ê²°ê³¼ì™€ í¸í–¥ì„ í•™ìŠµ(ì´ë¯¸ì§€ì˜ ë³¸ë˜ì˜ ëª¨ìŠµì´ ì•„ë‹Œ)


![image](https://user-images.githubusercontent.com/77658029/141408679-d0ef04b3-8161-48b4-ac96-a9bf665cfed3.png)

í•˜ì§€ë§Œ large-scale dataset(14M-300M Images)ì—ì„œ ëª¨ë¸ì„ í•™ìŠµí•˜ë©´ inductive biasê°€ ì—†ëŠ” Transformerê°€ ë” íš¨ê³¼ì ì¸ í•™ìŠµì„ í•˜ëŠ” ê²ƒì„ ì•Œê²Œë˜ì—ˆë‹¤. inductive biasëŠ” ê³¼ì í•©ì‹œí‚¤ëŠ” ë¶€ë¶„ì´ ìˆê¸° ë•Œë¬¸ì— ì´ëŸ° ë¶€ë¶„ì´ ì ì€ TransformerëŠ” ë°ì´í„°ê°€ ë§ì•„ì§ˆ ìˆ˜ë¡ í•™ìŠµì— ìœ ë¦¬í•œ ë¶€ë¶„ì´ë‹¤.

## RELATED WORK

![image](https://user-images.githubusercontent.com/77658029/141408916-0021ad1a-011d-432f-b390-88e5cde89bcd.png)

- Transformers ì¢…ë¥˜
    - BERT : Self-Supervised Pre-Training Task
    - GPT : ì¼ë°©í–¥ ì–¸ì–´ ëª¨ë¸ë§
![image](https://user-images.githubusercontent.com/77658029/141409112-4af4f278-6075-464f-ac1c-065f590e9fb9.png)

![image](https://user-images.githubusercontent.com/77658029/141408932-f2c3be67-c177-452f-aefe-4df022a675c8.png)

- Naive Application (Self-Attentionì„ ì ìš©í•˜ë ¤ëŠ” ì´ˆê¸° ì‹œë„)
    - Parmar : ì „ì—­ì (Globally)ìœ¼ë¡œ ì ìš© X â†’ ì¿¼ë¦¬(Query) í”½ì…€ ì£¼ë³€(Local Neighborhood)ì—ë§Œ Self-Attention ì ìš©
    - Weissenborn : ë‹¤ì–‘í•œ í¬ê¸°ì˜ Blockì„ Scale Attention ì ìš©,  But, íš¨ìœ¨ì ì¸ GPU ì‚¬ìš©ì„ ìœ„í•œ ë³µì¡í•œ ì—”ì§€ë‹ˆì–´ë§ í•„ìš”


![image](https://user-images.githubusercontent.com/77658029/141409346-ae062ab5-16e7-4c6b-80a8-a1abc110f9c8.png)

ViT modelê³¼ ì—°ê´€ìˆëŠ” ëª¨ë¸ì€ Cordonnier modelë¡œ 2x2 Image Patchë¥¼ ì¶”ì¶œí•˜ì—¬, Full Self-attention ì‚¬ìš©í•œ ëª¨ë¸ì´ë‹¤. í•˜ì§€ë§Œ 2x2 Image Patchë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì‘ì€ í•´ìƒë„(Resolution)ì—ë§Œ ì ìš©ê°€ëŠ¥í•˜ë‹¨ ë‹¨ì ì´ ìˆë‹¤.


![image](https://user-images.githubusercontent.com/77658029/141409729-f8fd37a2-c5c4-4dd0-8a2d-1b223d50327a.png)

CNNê³¼ Self Attentionì„ ìœ„í•œ ì—¬ëŸ¬ ì‹œë„ë“¤ì´ ìˆì—ˆëŠ”ë° ê·¸ì¤‘ ìµœì‹ ì— ë‚˜ì˜¨ Image GPT(iGPT) modelì´ë‹¤. Generative Modelì²˜ëŸ¼ í•™ìŠµí•˜ê³ , í•´ìƒë„ì™€ Color Spaceë¥¼ ì¶•ì†Œí•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í–ˆê³ , Transformersë¥¼ Image Pixel ë‹¨ìœ„ë¡œ ì ìš©í–ˆë‹¤. 

![image](https://user-images.githubusercontent.com/77658029/141410398-a59a07ef-5f34-4e11-be74-c73fdd77c6ea.png)

ë…¼ë¬¸ì˜ ì €ìëŠ” Transformerì˜ ImageNet-21k, JFT-300Mì˜ Large Scaleì˜ ë°ì´í„°ì…‹ì˜ í•™ìŠµì— ëŒ€í•œ ê²½í—˜ì  íƒìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì— ì´ˆì ì„ ë‘ê³  ìˆë‹¤.


## METHOD

![image](https://user-images.githubusercontent.com/77658029/141410956-3107cb4a-2fc0-480c-9534-d3de0ba07ffe.png)

image patch (Query) â†’ encoder(head(key, value)) â†’ embedding vector?

### 3.1 ViT
![image](https://user-images.githubusercontent.com/77658029/141410669-1671d65d-3100-420f-8b2c-9e1351d7cc38.png)

TransformerëŠ” ì‰½ê²Œ Setupí•  ìˆ˜ ìˆê³ , í™•ì¥ê°€ëŠ¥ì„±ì— ëŒ€í•œ ì´ì ì´ ìˆê¸° ë•Œë¬¸ì— ê°€ëŠ¥í•œí•œ ìˆœìˆ˜í•œ Transformer í˜•ì‹ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ ë…¸ë ¥í–ˆë‹¤. 


![image](https://user-images.githubusercontent.com/77658029/141410841-b3c6db25-3131-4649-a25b-eafa70899a43.png)

ê¸°ì¡´ Transformerë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ Image Patchë¥¼ Flatten í•˜ì—¬ ì‚¬ìš©í•˜ì˜€ê³  ë˜í•œ Transformerì— ë™ì¼í•œ í¬ê¸°ë¡œ ì…ë ¥í•´ì£¼ê¸°ìœ„í•´ D Dimensionsë¡œ Linear Projection ì§„í–‰í•˜ì˜€ë‹¤.(ì•„ë˜ ìˆ˜ì‹)

![image](https://user-images.githubusercontent.com/77658029/141411419-2b14d6f7-b557-4a2d-934c-6985fb7e844f.png)

- reshape : HxWxC â†’ Nx(P^2xC)
- (P,P) : Image Patch split ìˆ˜
- N : Patchì˜ ìˆ˜?
- D : Linear Projection(E)ì„ ì‚¬ìš©í•˜ì—¬ Dì°¨ì›ìœ¼ë¡œ ë§¤í•‘

![image](https://user-images.githubusercontent.com/77658029/141412137-d169f86b-7aa4-4132-91cf-949322ae9371.png)

BERTì˜ CLASS Token ì²˜ëŸ¼ í•™ìŠµ ê°€ëŠ¥í•œ Embedding Patchë¥¼ ì¶”ê°€í•˜ê³ 
Pre-train ë° fine-tuningì¤‘ì— classification headëŠ” Z^0_Lì— ì¶”ê°€ëœë‹¤.

![image](https://user-images.githubusercontent.com/77658029/141412365-67c9bfd9-e1c3-41e8-b9d2-78e3172442a4.png)

![image](https://user-images.githubusercontent.com/77658029/141414722-ffc836d2-e71f-4b3b-9faf-4797fe102f06.png)

Patchì˜ Embedding Positionì„ ìœ ì§€í•˜ê¸° ìœ„í•´ì„œ Position Embeddingë¥¼ ì¶”ê°€í–ˆê³ , ë…¼ë¬¸ì—ì„œëŠ” 1D Position Embeddingì„ ì‚¬ìš©í•˜ì˜€ëŠ”ë°, 2D Position Embeddingì„ ì‚¬ìš©í•´ë„ ì„±ëŠ¥ì— í° ì°¨ì´ê°€ ì—†ì–´ 1Dë¡œ ì‚¬ìš©í•˜ì˜€ë‹¤.


### ìˆ˜ì‹

![image](https://user-images.githubusercontent.com/77658029/141415845-ff1fa485-5c79-4380-a2e7-33735244c829.png)

### FINE TUNING AND HIGHER RESOLUTION

![image](https://user-images.githubusercontent.com/77658029/141415936-39e5f1c0-ffe1-47bc-b2a9-b7ea414bd93b.png)

ì¼ë°˜ì ìœ¼ë¡œ large-scale datasetì— ëŒ€í•´ ViTë¥¼ pre-trainí•˜ê³  downstream taskì— ëŒ€í•´ fine-tuningì„ ìˆ˜í–‰í•œë‹¤. ì´ë¥¼ ìœ„í•´ pre-trained prediction headë¥¼ ì œê±°í•˜ê³  0ìœ¼ë¡œ ì´ˆê¸°í™”ëœ KxD feedforward layerë¥¼ ì¶”ê°€í•œë‹¤
Pre-train ë³´ë‹¤ ë†’ì€ resolutionìœ¼ë¡œ fine-tuningí•˜ëŠ”ê²ƒì€ ì¢…ì¢… ë„ì›€ì´ ëœë‹¤. ë” ë†’ì€ resolutionì˜ ì´ë¯¸ì§€ë¥¼ feedí•  ë•Œ patch í¬ê¸°ë¥¼ ë™ì¼í•˜ê²Œ ìœ ì§€í•˜ë¯€ë¡œ sequence lengthê°€ ë” ê¸¸ì–´ì§€ê²Œ ëœë‹¤. ì´ë•Œ PreTrained ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” Sequence Lengthë¥¼ ë§ì¶°ì£¼ê¸° ìœ„í•´ì„œ 2D Interpolationì„ ìˆ˜í–‰í•˜ê²Œ ë˜ê³ , ì´ë•Œ ViTì— Inductive Biasê°€ ì‹¬ì–´ì§€ëŠ” íš¨ê³¼ê°€ ìƒê¸´ë‹¤. 


## EXPERIMENTS

![image](https://user-images.githubusercontent.com/77658029/141416415-28b9fe76-66c7-4028-a5a1-ea1983ce17a5.png)

pre-training ê³„ì‚°ë¹„ìš©ì„ ê³ ë ¤í•  ë•Œ ViTëŠ” ë” ë‚®ì€ ë¹„ìš©ìœ¼ë¡œ ëŒ€ë¶€ë¶„ì˜ recognition benchmarkì—ì„œ SotAë¥¼ ë‹¬ì„±í•˜ì˜€ë‹¤.

### 4.1 SETUP

#### Datasets
- pretrained 
    1. ILSVRC-2012 ImageNet dataset(ImageNet) - 1k í´ë˜ìŠ¤ ë° 1.3M ì´ë¯¸ì§€
    2. ImageNet-21k - 21,000 í´ë˜ìŠ¤ ë° 14M ì´ë¯¸ì§€
    3. JFT - 18k í´ë˜ìŠ¤ ë° 303M ì´ë¯¸ì§€
- Transfer Learning dataset
    1. ImageNet ë° ReaL labels
    2. CIFAR 10/100
    3. Oxford-IIIT Pets
    4. Oxford Flowers-102
    5. 19-task VTAB classification suite

#### Model Variants

BERTì— ì‚¬ìš©ë˜ëŠ” êµ¬ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ê³ , ì ‘ë¯¸ì‚¬ëŠ” â€œBâ€(Base), â€œLâ€(Large), â€œHâ€(Huge) ë¥¼ ëœ»í•¨
ì˜ˆë¥¼ë“¤ì–´ ViT-L/16 ì˜ ê²½ìš°ëŠ” Large ì‚¬ì´ì¦ˆ ì´ë©° 16x16 ì˜ íŒ¨ì¹˜í¬ê¸°ë¥¼ ê°€ì§

#### Training & Fine-tuning

- Pre-train
    - Adam optimizer, b_1 = 0.9, b_2=0.999, batch size=4096
    - weight decay: 0.1

- Fine-tuning
    - SGD w/ momentum, batch size=512batchsize=512
    - using linear learning rate warmup and decay
    - Higher resolution: 512 for ViT-L/16 and 518 for ViT-H/14


### 4.2  COMPARISON TO STATE OF THE ART

![image](https://user-images.githubusercontent.com/77658029/141417543-5353b442-897f-4e69-b8b2-0afdf78e7c78.png)

![image](https://user-images.githubusercontent.com/77658029/141417498-1eaac578-3423-4140-b242-0f554dad7d53.png)

ëª¨ë“  ëª¨ë¸ì€ TPUv3ì—ì„œ í•™ìŠµë˜ì—ˆìœ¼ë©° pre-trainì— ì†Œìš”ëœ ì¼ìˆ˜ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

ViT-L/16 ëª¨ë¸ì€ ëª¨ë“  datasetì—ì„œ BiT-Lê³¼ ì¼ì¹˜í•˜ê±°ë‚˜ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œê³¼ ë™ì‹œì— í›¨ì”¬ ì ì€ computational resourceë¥¼ í•„ìš”ë¡œ í•œë‹¤. ë” í° ëª¨ë¸ì¸ Vit-H/14ëŠ” ImageNet ë° CIFAR-100ê³¼ VTABì—ì„œ ì„±ëŠ¥ì´ ë”ìš± í–¥ìƒë˜ì—ˆë‹¤.

![image](https://user-images.githubusercontent.com/77658029/141417753-c6cd8993-71f8-4f5f-99d6-c5347fa4fb63.png)

VTAB : ImageNetì˜ GLUE ê°™ì€ í‰ê°€ ì§€í‘œ

### 4.3  PRE-TRAINING DATA REQUIREMENT

![image](https://user-images.githubusercontent.com/77658029/141418309-152c5704-4f81-442d-be09-9c105a0da112.png)

Datasetì˜ í¬ê¸°ì— ë”°ë¼ ì„±ëŠ¥ì´ ì–¼ë§ˆë‚˜ ë°”ë€”ê¹Œ?

![image](https://user-images.githubusercontent.com/77658029/141418370-356783c6-e0a3-435f-a6e2-fc3f2459fc49.png)

![image](https://user-images.githubusercontent.com/77658029/141418404-ff9e78b0-68ac-4eb7-aa2e-e0d0d3a258b9.png)

![image](https://user-images.githubusercontent.com/77658029/141418601-5c4bc401-1ba1-4207-8ba7-400be9519b0a.png)

![image](https://user-images.githubusercontent.com/77658029/141418653-1c7fd563-3d34-4d63-9d5d-2eff27cdfd94.png)

ê°€ì¥ ì‘ì€ datasetì— ëŒ€í•´ pre-trainì„ ìˆ˜í–‰í•œ ê²½ìš° ViT-Large ëª¨ë¸ì€ ì •ê·œí™”ì—ë„ ë¶ˆêµ¬í•˜ê³  ViT-Baseë³´ë‹¤ ì„±ëŠ¥ì´ ë–¨ì–´ì§„ë‹¤. ê·¸ëŸ¬ë‚˜ ImageNet-21k datasetì„ ì‚¬ìš©í•˜ë©´ ì„±ëŠ¥ì´ ë¹„ìŠ·í•˜ë‹¤. JFT-300M datasetì—ì„œëŠ” ViTê°€ BiTë³´ë‹¤ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

ê²°ê³¼ì ìœ¼ë¡œ ë” ì‘ì€ datasetì˜ ê²½ìš° convolutional inductive biasê°€ ìœ ë¦¬í•˜ì§€ë§Œ í° datasetì˜ ê²½ìš° ê´€ë ¨ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì¶©ë¶„í•˜ê³  ì´ì ì´ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œìˆ˜ ìˆë‹¤. 


### 4.4 SCALING STUDY

![image](https://user-images.githubusercontent.com/77658029/141419168-cf8b780e-af8e-449c-baeb-2a23cc8eaa22.png)

JFT-300M datasetì—ì„œ transfer performanceì— ëŒ€í•´ ë‹¤ì–‘í•œ ëª¨ë¸ì˜ í™•ì¥ ì—°êµ¬ë¥¼ ìˆ˜í–‰í–ˆë‹¤. Dataset í¬ê¸°ëŠ” Transfer Performanceì— í° ì˜í–¥ì´ ì—†ì—ˆë‹¤.
ê·¸ë˜ì„œ, ì—¬ëŸ¬ ì¡°ê±´ì—ì„œ ì—¬ëŸ¬ ëª¨ë¸ì„ transfer performance vs pre-training computeì— ëŒ€í•œ ì‹¤í—˜ì„ ì§„í–‰í•˜ì˜€ë‹¤.

![image](https://user-images.githubusercontent.com/77658029/141418898-c32aba45-4f84-4ed6-8541-da2436205d80.png)

ì—°ì‚°ëŸ‰ ë˜í•œ ê°™ì€ ìˆ˜ì¤€ì˜ Accuracyì—ì„œ ë³´ë©´ BiT ëª¨ë¸ë³´ë‹¤ ViTê°€ ì ì€ ìˆ˜ì¤€ì¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆê³ , Hybridë„ ì •í™•ë„ê°€ ë†’ì•„ì§ˆ ìˆ˜ë¡ ViTê°€ ì¢‹ì€ ì„±ëŠ¥ì„ ê°€ì§€ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤.

### INSPECTING VISION TRANSFORMER

![image](https://user-images.githubusercontent.com/77658029/141419518-3c1378fd-c792-4352-a360-1db26aaca38d.png)

ViTê°€ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ ì´í•´í•˜ê¸°ìœ„í•´ internal representationsì„ ë¶„ì„í•´ë³´ì•˜ë‹¤. ViTì˜ Flatten ëœ Patchë¥¼ 2Dë¡œ Projectioní•œ Spaceë¥¼ í™•ì¸í•˜ì˜€ë‹¤.

![image](https://user-images.githubusercontent.com/77658029/141419923-078b4506-d139-43ec-ac1d-63d7d24357fb.png)

í•™ìŠµëœ learned Embedding filterëŠ” ë§ˆì¹˜ CNNì˜ Low dimensional feature mapê³¼ ë™ì¼í•œ ì–‘ìƒì„ ë³´ì¸ë‹¤. 

![image](https://user-images.githubusercontent.com/77658029/141420128-348ecc0e-7894-4700-8235-bc60a18cef07.png)

ìœ„ ì‚¬ì§„ì€ í•™ìŠµëœ Position Embeddingì˜ ê²½ìš°ë¥¼ ë³´ì—¬ì£¼ëŠ”ë°, ì´ê²ƒì€ modelì´ imageì•ˆì—ì„œ ë¹„ìŠ·í•œ positionì„ í•™ìŠµí•œë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤. ë” ê°€ê¹Œìš´ patchëŠ” ë” ìœ ì‚¬í•œ position embeddingì„ ê°–ëŠ” ê²½í–¥ì´ ìˆìœ¼ë©° í•˜ë‚˜ì˜ Patchê¸°ì¤€ìœ¼ë¡œ í–‰ê³¼ ì—´ì— ë¹„ìŠ·í•œ ê²½í–¥ì„±ì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤.

![image](https://user-images.githubusercontent.com/77658029/141420454-31fc4f72-99c9-4d38-9a47-951906390d98.png)

![image](https://user-images.githubusercontent.com/77658029/141420605-0f113041-4d79-4c7e-bcd3-6b6f87095e82.png)

CNNì˜ Receptive Fieldì™€ ë¹„ìŠ·í•˜ê²Œ ViTì—ì„œë„ Attention Distanceë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. Self-attentionì˜ weightë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ ê³µê°„ì˜ í‰ê· ê±°ë¦¬ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. ì´ ë¶€ë¶„ì„ í™•ì¸í•´ë³´ë©´ lowest layerì—ì„œë„ globallyí•œ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆë‹¤ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤.


![image](https://user-images.githubusercontent.com/77658029/141421151-f69bcb16-ecf0-43e8-a1d0-f7fbd7d44740.png)

ëª¨ë¸ì´ ì˜ë¯¸ìƒì˜ ë¶„ë¥˜ì™€ ê´€ë ¨ëœ ì´ë¯¸ì§€ ì˜ì—­ì„ ë‹´ë‹¹í•œë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.


### 4.6 SELF-SUPERVISION -> pseudo labeling?


![image](https://user-images.githubusercontent.com/77658029/141421260-a235dc87-2d61-49ab-98f6-06cfc209f85f.png)
![image](https://user-images.githubusercontent.com/77658029/141421278-c415faed-f35b-4c4d-842f-cac6da1e3e5a.png)

TransformerëŠ” NLP taskì—ì„œ ì¸ìƒì ì¸ ì„±ëŠ¥ì„ ë³´ì—¬ì¤¬ëŠ”ë°, í™•ì¥ì„±ê³¼ í•¨ê»˜ self-supervised pre-trainingì—ì„œ ë¹„ë¡¯ëœë‹¤. ë˜í•œ BERTì—ì„œ ì‚¬ìš©ë˜ëŠ” MLM(Masked Language Modeling)ì„ ëª¨ë°©í•˜ì—¬ self-supervisionì„ ìœ„í•œ Masked Patch Predictionì— ëŒ€í•œ ì˜ˆë¹„íƒìƒ‰í•˜ì—¬ ì´í›„ ì‘ì—…ì— ì‚¬ìš©í•œë‹¤.?


## 5. CONCLUSION

![image](https://user-images.githubusercontent.com/77658029/141421835-45b5bd21-6418-48b6-b7fe-2d70fab9561b.png)

ì´ë²ˆ ë…¼ë¬¸ì€ Image Recognitionì— Transformerë¥¼ ì ìš©í•˜ê¸° ìœ„í•œ ì—¬ëŸ¬ íƒìƒ‰ë“¤ì„ ì§„í–‰í–ˆë‹¤. ì´ì „ CVì—°êµ¬ë“¤ì€ Image-specific Inductive Baisesë¥¼ ì‚¬ìš©í•˜ì˜€ì§€ë§Œ, ë³¸ ì—°êµ¬ëŠ” Self-attentionì„ ì ìš©í•˜ì˜€ë‹¤.(ì´ˆê¸° patch ì¶”ì¶œí• ë•Œì—ë§Œ ì‚¬ìš©ë¨)
Transformerì˜ ê°„ë‹¨í•˜ë©´ì„œë„ í™•ì¥ê°€ëŠ¥í•œ ì´ì ì€ large-scale datasetì— ëŒ€í•œ pre-trainê³¼ ê²°í•©ë  ë•Œ íš¨ê³¼ì ìˆì—ˆê³ , ë”°ë¼ì„œ Vision TransformerëŠ” ë§ì€ image classification datasetì—ì„œ SotAë¥¼ ëŠ¥ê°€í•˜ê³  pretrainì˜ í•™ìŠµ ë¹„ìš©ì´ ìƒëŒ€ì ìœ¼ë¡œ ì €ë ´í•˜ë‹¤. 
ì¶”ê°€ì ìœ¼ë¡œ ì•„ì§ dctection ë° segmentation Taskì— ì ìš©ê³¼ pre-training methodì— ëŒ€í•´ ì—°êµ¬ê°€ í•„ìš”í•˜ë‹¤. ë˜í•œ Scalingí›„ì—ë„ ì•„ì§ Saturate ìƒíƒœê°€ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ì¶”ê°€ì ì¸ ì„±ëŠ¥ í–¥ìƒì´ ê¸°ëŒ€ëœë‹¤.

**ğŸ“Œreference**
- [ViT ë…¼ë¬¸](https://arxiv.org/pdf/2010.11929.pdf)
- [Deep Learner](https://jeonsworld.github.io/vision/vit/)
- [KM-hana](https://kmhana.tistory.com/27)

<br>
```
ğŸ’¡ ìˆ˜ì • í•„ìš”í•œ ë‚´ìš©ì€ ëŒ“ê¸€ì´ë‚˜ ë©”ì¼ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤!ğŸ’¡ 
```
