---
defaults:
  - scope:
      path: ""
      type: posts
    values:
      layout: single
      author_profile: true
      comments: true
      share: true
      related: true

title: "FCN model - transpose convolution"
excerpt: "about : object detection"
toc: true
toc_sticky: true
toc_label: "Label"
categories:
  - DL
tags:
  - [Segmentation]
date: 2021-10-18
last_modified_at: 2021-10-18
---

<br>

# FCN Fully Convolutaional Network

- VGG 네트워크 백본을 사용(Feature Extracting Network)
    - Feature Extracting Network(backbone) : VGG, AlexNet, ResNet, EfficienctNet
- VGG 네트워크의 FC Layer(nn.Linear)를 Convolution으로 대체
- Transposed Convolution을 이용하여 Pixel Wise Prediction을 수행함

    
![image](https://user-images.githubusercontent.com/77658029/137853384-f22e6bf6-0acb-4390-97b7-afae0f27d567.png)

![image](https://user-images.githubusercontent.com/77658029/137852646-bde45a44-4087-4e4a-8c34-e539d5c777f3.png)

- 백본으로 VGG를 사용하는 이유!
    - pretrained 된 모델을 그대로 사용할 수 있음 -> 원래는 가중치를 새로 학습시켜야함
    - 여러 parameter tuning에 들어가는 자원을 아낄 수 있음

## FC Layer vs FCN

1. 위치정보 보존
    - Fully Connected Layer의 경우 위치 정보가 지워지나
    - FCN의 경우 위치정보가 그대로 남아있음

![image](https://user-images.githubusercontent.com/77658029/137853729-7c46fdf4-69d5-4d3c-88ce-c09fef4a9064.png)

![image](https://user-images.githubusercontent.com/77658029/137853833-65abd9eb-2e13-448c-a567-c05daeb4f1cc.png)

2. 입력 Size
    - FC Layer의 경우 정해진 Input Size가 정해져 있음
    - FCN의 경우 Input Size와 상관없이 학습 및 inference가 가능함


## Transposed Convolution

- Maxpooling으로 인해서 Feature map의 w,h의 사이즈가 작아지게 되는데, 우리가 원하는 output은 image의 원본 사이즈이기 때문에 upsampling하는 과정이 필요한데, 이 연산을 도와주는게 Transposed Convolution
- Transposed Convolution의 연산도 Convolution과 동일하게 Kernel을 학습하여 사용하게 됨(Backprobagation과정으로 update됨)


**❓ 왜 Transposed Convolution이라고 부를까?**

- 사이즈를 키운다는 의미로 upsampling, deconvolution, transposed convolution등 여러가지 용어로 설명하게 되는데, deconvolution이란 단어는 맞지 않다고 하는 사람들도 있음
- Transposed Convolution 계산 방법

![image](https://user-images.githubusercontent.com/77658029/137856851-13318e66-2f0f-4c62-afc2-37d2db047022.png)

- Transpose Convolution이라고 불리는 이유는 Convolution 연산을 Transposed한 형태와 동일한 연산 결과를 얻을 수 있음

![image](https://user-images.githubusercontent.com/77658029/137856702-c580f9aa-e3fb-46fa-9a0c-6614025e429b.png)

- Transposed Convolution의 경우 완벽한 Convolution를 되돌리는 연산이 아니기 때문에Deconvolution란 용어는 잘못된 표현이지만, 여러 논문이나 설명하는 글에서 혼용하여 사용하고 있음

### Transposed Convolution의 Stride와 Padding 방법

- Stride 

![image](https://user-images.githubusercontent.com/77658029/139700939-d496e046-a133-4b2b-b6ab-5de7cd75a1f4.png)

- Padding : 연산을 진행하여 나온 결과에 Padding을 진행함

![image](https://user-images.githubusercontent.com/77658029/139701230-68335aa1-1f06-429f-8a08-ea3be7ea0e5e.png)

<br>

**📰code**
```python
import torch
import torch.nn as nn

x = torch.randn([1, 3, 3, 3])
print(nn.ConvTranspose2d(3, 3, kernel_size=2, stride=1, padding=0)(x).size())
print(nn.ConvTranspose2d(3, 3, kernel_size=2, stride=1, padding=1)(x).size())
print(nn.ConvTranspose2d(3, 3, kernel_size=3, stride=1, padding=0)(x).size())
print(nn.ConvTranspose2d(3, 3, kernel_size=3, stride=1, padding=1)(x).size())
print(nn.ConvTranspose2d(3, 3, kernel_size=2, stride=2, padding=0)(x).size())
```

**🔍result**
```
torch.Size([1, 3, 4, 4])
torch.Size([1, 3, 2, 2])
torch.Size([1, 3, 5, 5])
torch.Size([1, 3, 3, 3])
torch.Size([1, 3, 6, 6])
```

<br>

## 모델 구조 

![image](https://user-images.githubusercontent.com/77658029/139704895-2721d55e-ce46-4fa4-9129-4059c00d1605.png)

```
FCN(backbone:VGG)의 구조
conv1 : [conv→relu→conv→relu→maxpool] 이미지 크기 input의 1/2로 감소
conv2 : [conv→relu→conv→relu→maxpool] 이미지 크기 input의 1/4로 감소
conv3 : [conv→relu→conv→relu→conv→relu→maxpool] 이미지 크기 input의 1/8로 감소
conv4 : [conv→relu→conv→relu→conv→relu→maxpool] 이미지 크기 input의 1/16로 감소
conv5 : [conv→relu→conv→relu→conv→relu→maxpool] 이미지 크기 input의 1/32로 감소
FC6 : [conv(1×1)→relu→dropout]
FC7 : [conv(1×1)→relu→dropout]
score : [conv(input_channel, num_classes,1,1,0)]
upscore : [convTransposed2d(num_classes, num_classes, karnel_size=64, stride=32, padding=16)] 
        1/32가 된 이미지를 32배 키워 원본 사이즈로 복원
```
- 결국 Upsampling으로 인한 문제로 해상도가 낮음, 이를 해결하기 위해 low level의 feature map을 가져와 합치는 작업을 진행하여 성능을 향상 시킴

![image](https://user-images.githubusercontent.com/77658029/139705945-58cb7cdf-a4ef-4e01-85dd-8ee5d4b22d18.png)
<br><br>


**📌reference**
- boostcourse AI tech

<br>

```
💡 수정 필요한 내용은 댓글이나 메일로 알려주시면 감사하겠습니다!💡 
```
