---
defaults:
  - scope:
      path: ""
      type: posts
    values:
      layout: single
      author_profile: true
      comments: true
      share: true
      related: true

title: "HyperParameter Tuning - Ray"
excerpt: "about : "
toc: true
toc_sticky: true
toc_label: "Label"
categories:
  - Visualization
tags:
  - [PyTorch, hyperparameter, Multi GPU]
date: 2021-08-20
last_modified_at: 2021-08-20
---

<br>  

# Multi GPU

- ì˜ˆì „ì—ëŠ” ì–´ë–»ê²Œ í•˜ë©´ GPUë¥¼ ì ê²Œ ì“¸ê¹Œ?
- ì§€ê¸ˆì€ GPUë¥¼ ë§ì´ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ì¢‹ì€ ëª¨ë¸ì„ ë§Œë“¤ì!
- ì˜¤ëŠ˜ë‚ ì˜ ë”¥ëŸ¬ë‹ì€ ì—„ì²­ë‚œ ë°ì´í„°ì™€ì˜ ì‹¸ì›€(GPT-3 $10^11$ ë°ì´í„°ë¥¼ ì‚¬ìš©)
- ì—°êµ¬ëŠ” ì¥ë¹„ë¹¨...

<br>

## ê¸°ë³¸ ê°œë… ì •ë¦¬ 

- single GPU : GPU 1ê°œ
- Multi GPU : GPU 2ê°œ ì´ìƒ
- NodeëŠ” 1ëŒ€ì˜ ì»´í“¨í„°ë¥¼ ì˜ë¯¸í•¨
- Single Node Single GPU - 1ëŒ€ì˜ ì»´í“¨í„°ì— GPU 1ê°œ
- Single Node Multi GPU(4~8ê°œ?) - 1ëŒ€ì˜ ì»´í“¨í„°ì— 2ê°œ ì´ìƒì˜ GPU
- Multi Node Multi GPU(server) - ì—¬ëŸ¬ëŒ€ì˜ ì»´í“¨í„°ì— ì—¬ëŸ¬ê°œì˜ GPU

<br>

## Multi GPU ì‚¬ìš© ë°©ë²•

<br>

### 1. Model Parallel

- ëª¨ë¸ì„ GPUì— ë”°ë¡œ ë‚˜ëˆ ì„œ ëŒë¦¬ëŠ” ê²ƒ
- ì˜ˆì „ë¶€í„° ë§ì´ ì‚¬ìš©í–ˆë˜ ë°©ì‹ (Alexnetì— ì‚¬ìš©)
- ì‚¬ì‹¤ ëª¨ë¸ì˜ ë³‘ëª©í˜„ìƒê³¼ íŒŒì´í”„ë¼ì¸ì˜ ì–´ë ¤ì›€ìœ¼ë¡œ, ëª¨ë¸ ë³‘ë ¬í™”ëŠ” ê³ ë‚œì´ë„ ê³¼ì œ
- í”„ë¡œì„¸ìŠ¤
    1. Model1,2ë¥¼ ê°ê° GPU1,2ì— ë„£ì–´ì¤Œ
    2. network í•™ìŠµ -> ì¤‘ê°„ì— ë°ì´í„°ë¥¼ ë‹¤ë¥¸ GPUë¡œ ë„˜ê²¨ì£¼ë©° ë³‘ëª©í˜„ìƒ ë°œìƒ
    3. ë§ˆì§€ë§‰ ê²°ê³¼ë¥¼ GPUí•˜ë‚˜ë¡œ ëª¨ì•„ì„œ Loss or ê²°ê³¼ ì¶œë ¥

![image](https://user-images.githubusercontent.com/77658029/130342072-623f3f65-e429-4b22-80fb-4ca2adfc03ad.png)

- ë™ì¼í•œ ë°ì´í„°ë¥¼ ëª¨ë¸ì„ ë‘˜ë¡œ ë‚˜ëˆ  Multi GPUë¥¼ ì‚¬ìš©í•¨

**ğŸ“°code**
```python
class ModelParallelResNet50(ResNet):
    def __init__(self, *args, **kwargs):
        super(ModelParallelResNet50, self).__init__(
        Bottleneck, [3, 4, 6, 3], num_classes=num_classes, *args, **kwargs)
        self.seq1 = nn.Sequential(self.conv1, self.bn1, self.relu, self.maxpool, self.layer1, self.layer2).to('cuda:0')# GPU1
        self.seq2 = nn.Sequential(self.layer3, self.layer4, self.avgpool).to('cuda:1') #GPU2
        self.fc.to('cuda:1') #GPU2
        
    def forward(self, x):
        x = self.seq2(self.seq1(x).to('cuda:1')) #GPUë¡œ í•©ì³ì„œ ë§ˆì§€ë§‰ ì‘ì—… ì§„í–‰
        return self.fc(x.view(x.size(0), -1))
```

<br>

### 2. Data Parallel

- ë°ì´í„°ë¥¼ Batchë¡œ ë‚˜ëˆ ì„œ L# Hyperparameter Tuning

- ëª¨ë¸ í•™ìŠµì„ í•  ë•Œ ì‚¬ëŒì´ ì •í•´ì¤˜ì•¼í•˜ëŠ” ëª‡ê¹Œì§€ parameterë“¤ì„ Hyperparameterë¼ê³  ë¶€ë¦„
- í•™ìŠµë¥ (Learning_Rate), Activation Function ë“±
- ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ì„œ í¬ê²Œ 3ê°€ì§€ ë°©ë²•ì„ ì‚¬ìš©í•¨(ì œì¼ íš¨ê³¼ê°€ í° ê±´ Data)

    1. ëª¨ë¸ì„ ë°”ê¿”ë³¸ë‹¤ -> ê±°ì˜ ê³ ì •ë˜ì–´ ìˆìŒ(CV : Resnet, CNN ê³„ì—´, ViT /NLP : transformer)
    2. ë°ì´í„° ì¶”ê°€ or ì´ìƒ ë°ì´í„° -> ì œì¼ ì¤‘ìš”í•¨, ì œì¼ ë§ì´ ë°”ê¿ˆ
    3. Hyperparameter Tuning -> ë§ˆë¥¸ ìˆ˜ê±´ë„ ì§œë³´ëŠ” ëŠë‚Œ

-> ëª¨ë¸ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ì§€ ì•ŠëŠ” ê°’ì€ ì‚¬ëŒì´ ì§€ì •(learning rate(NAS), ëª¨ë¸ì˜ í¬ê¸°, Optimizer ë“±ë“±)
- ì˜ˆì „ì—ëŠ” Hyperparameterì— ë”°ë¼ì„œ ê²°ê³¼ê°€ í¬ê²Œ ì¢Œìš°ë¨(ìš”ì¦˜ì€ ì°¨ì´ê°€ í¬ì§€ ì•ŠìŒ, why? ë°ì´í„°ë¥¼ í›¨ì”¬ ë§ì´ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸)
- ë§ˆì§€ë§‰ ì„±ëŠ¥ì„ 0.01, 0.001 ì¥ì–´ì§œì•¼í• ë•Œ ì‹œë„..

<br>

## Hyperparameterë¥¼ ì°¾ëŠ” ë°©ë²•

<br>

### Grid Search

- ëª‡ê°œì˜ Hyperparameterë¥¼ íŠ¹ì • ë²”ìœ„ì—ì„œ ë³€ë™ ì‹œí‚¤ë©° í™•ì¸í•˜ëŠ” ë°©ë²•
- ì¼ì •í•œ ê°„ê²©(log scale)ìœ¼ë¡œ Gridë¥¼ ë§Œë“¤ì–´ ìµœì ì˜ hyperparameterë¥¼ ì°¾ìŒ

![image](https://user-images.githubusercontent.com/77658029/130343528-a65de0dc-d8fa-4b6e-b665-034e9182325a.png)

<br>

### random Search

- Hyperparameterë¥¼ ëœë¤ìœ¼ë¡œ ì„ íƒí•˜ì—¬ ìµœì ì˜ hyperparameterë¥¼ ì°¾ìŒ

![image](https://user-images.githubusercontent.com/77658029/130343555-82ca4ab9-8278-467e-b679-152666b5340a.png)

<br>

### BOHB(2018)

- ë² ì´ì§€ì•ˆ ê¸°ë°˜ ê¸°ë²•ìœ¼ë¡œ ìš”ì¦˜ ìœ í–‰í•¨
- [ì°¸ê³ ìë£Œ](http://proceedings.mlr.press/v80/falkner18a/falkner18a.pdf)

<br>

## Ray

- Multi-node Multi processing ì§€ì› ëª¨ë“ˆ
- ML/DLì˜ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ê°œë°œëœ ëª¨ë“ˆ
- ìš”ì¦˜ì˜ Python ë³‘ë ¬ì²˜ë¦¬ëŠ” Rayê°€ ê±°ì˜ í‘œì¤€
- Hyperparameter Searchë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ ëª¨ë“ˆ ì œê³µ
- SPARKë¥¼ ë§Œë“  ì—°êµ¬ì‹¤, ìš”ì¦˜ì€ Rayì— ì§‘ì¤‘í•˜ê³  ìˆë‹¤ê³  í•¨

ğŸ’¡ SPARK(ë°ì´í„°ë¥¼ ë³‘ë ¬ì²˜ë¦¬í•˜ëŠ” ëª¨ë“ˆ, í•˜ë‘¡ì˜ ê¸°ëŠ¥ì„ ë›°ì–´ë„˜ìŒ) 

- rayë¥¼ í™œìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 4ê°€ì§€ë¥¼ ë§Œë“¤ì–´ì¤˜ì•¼í•¨

    1. íƒìƒ‰í•  Hyperparameterë³„ ë²”ìœ„ ì„¤ì •(search space ì§€ì •) - dictionaryí˜•íƒœë¡œ
    2. ì–´ë–»ê²Œ í•™ìŠµí• ì§€ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ì„¤ì •
    3. ê²°ê³¼ë¥¼ ì¶œë ¥ ì–‘ì‹ ì„¤ì •
    4. ìœ„ 3ê°€ì§€ë¥¼ ëª¨ì•„ì„œ resultë¡œ í•˜ë‚˜ë¡œ ë¬¶ì–´ì£¼ëŠ” ì‘ì—…


**ğŸ“°code**
```python
data_dir = os.path.abspath("./data")
load_data(data_dir)

config = {
    "l1": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),
    "l2": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),
    "lr": tune.loguniform(1e-4, 1e-1),
    "batch_size": tune.choice([2, 4, 8, 16])
}

scheduler = ASHAScheduler(metric="loss", mode="min", max_t=max_num_epochs, grace_period=1,reduction_factor=2)

reporter = CLIReporter(metric_columns=["loss", "accuracy", "training_iteration"])

result = tune.run(
    partial(train_cifar, data_dir=data_dir),
    resources_per_trial={"cpu": 2, "gpu": gpus_per_trial},
    config=config, num_samples=num_samples,
    scheduler=scheduler,
    progress_reporter=reporter
)
```

<br>

**ğŸ“Œreference**
- boostcourse AI tech
- [Random Search for Hyper-Parameter Optimization](https://dl.acm.org/doi/pdf/10.5555/2188385.2188395)

<br>

```
ğŸ’¡ ìˆ˜ì • í•„ìš”í•œ ë‚´ìš©ì€ ëŒ“ê¸€ì´ë‚˜ ë©”ì¼ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤!ğŸ’¡ 
```
