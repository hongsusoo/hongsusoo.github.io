---
defaults:
  - scope:
      path: ""
      type: posts
    values:
      layout: single
      author_profile: true
      comments: true
      share: true
      related: true

title: "Multi GPU"
excerpt: "about : python"
toc: true
toc_sticky: true
toc_label: "Label"
categories:
  - ai_dlbasic
tags:
  - [PyTorch, Multi GPU]
date: 2021-08-20
last_modified_at: 2021-08-20
---

<br>  

# Multi GPU

- ì˜ˆì „ì—ëŠ” ì–´ë–»ê²Œ í•˜ë©´ GPUë¥¼ ì ê²Œ ì“¸ê¹Œ?
- ì§€ê¸ˆì€ GPUë¥¼ ë§ì´ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ì¢‹ì€ ëª¨ë¸ì„ ë§Œë“¤ì!
- ì˜¤ëŠ˜ë‚ ì˜ ë”¥ëŸ¬ë‹ì€ ì—„ì²­ë‚œ ë°ì´í„°ì™€ì˜ ì‹¸ì›€(GPT-3 $10^11$ ë°ì´í„°ë¥¼ ì‚¬ìš©)
- ì—°êµ¬ëŠ” ì¥ë¹„ë¹¨...

<br>

## ê¸°ë³¸ ê°œë… ì •ë¦¬ 

- single GPU : GPU 1ê°œ
- Multi GPU : GPU 2ê°œ ì´ìƒ
- NodeëŠ” 1ëŒ€ì˜ ì»´í“¨í„°ë¥¼ ì˜ë¯¸í•¨
- Single Node Single GPU - 1ëŒ€ì˜ ì»´í“¨í„°ì— GPU 1ê°œ
- Single Node Multi GPU(4~8ê°œ?) - 1ëŒ€ì˜ ì»´í“¨í„°ì— 2ê°œ ì´ìƒì˜ GPU
- Multi Node Multi GPU(server) - ì—¬ëŸ¬ëŒ€ì˜ ì»´í“¨í„°ì— ì—¬ëŸ¬ê°œì˜ GPU

<br>

## Multi GPU ì‚¬ìš© ë°©ë²•

<br>

### 1. Model Parallel

- ëª¨ë¸ì„ GPUì— ë”°ë¡œ ë‚˜ëˆ ì„œ ëŒë¦¬ëŠ” ê²ƒ
- ì˜ˆì „ë¶€í„° ë§ì´ ì‚¬ìš©í–ˆë˜ ë°©ì‹ (Alexnetì— ì‚¬ìš©)
- ì‚¬ì‹¤ ëª¨ë¸ì˜ ë³‘ëª©í˜„ìƒê³¼ íŒŒì´í”„ë¼ì¸ì˜ ì–´ë ¤ì›€ìœ¼ë¡œ, ëª¨ë¸ ë³‘ë ¬í™”ëŠ” ê³ ë‚œì´ë„ ê³¼ì œ
- í”„ë¡œì„¸ìŠ¤
    1. Model1,2ë¥¼ ê°ê° GPU1,2ì— ë„£ì–´ì¤Œ
    2. network í•™ìŠµ -> ì¤‘ê°„ì— ë°ì´í„°ë¥¼ ë‹¤ë¥¸ GPUë¡œ ë„˜ê²¨ì£¼ë©° ë³‘ëª©í˜„ìƒ ë°œìƒ
    3. ë§ˆì§€ë§‰ ê²°ê³¼ë¥¼ GPUí•˜ë‚˜ë¡œ ëª¨ì•„ì„œ Loss or ê²°ê³¼ ì¶œë ¥

![image](https://user-images.githubusercontent.com/77658029/130342072-623f3f65-e429-4b22-80fb-4ca2adfc03ad.png)

- ë™ì¼í•œ ë°ì´í„°ë¥¼ ëª¨ë¸ì„ ë‘˜ë¡œ ë‚˜ëˆ  Multi GPUë¥¼ ì‚¬ìš©í•¨

**ğŸ“°code**
```python
class ModelParallelResNet50(ResNet):
    def __init__(self, *args, **kwargs):
        super(ModelParallelResNet50, self).__init__(
        Bottleneck, [3, 4, 6, 3], num_classes=num_classes, *args, **kwargs)
        self.seq1 = nn.Sequential(self.conv1, self.bn1, self.relu, self.maxpool, self.layer1, self.layer2).to('cuda:0')# GPU1
        self.seq2 = nn.Sequential(self.layer3, self.layer4, self.avgpool).to('cuda:1') #GPU2
        self.fc.to('cuda:1') #GPU2
        
    def forward(self, x):
        x = self.seq2(self.seq1(x).to('cuda:1')) #GPUë¡œ í•©ì³ì„œ ë§ˆì§€ë§‰ ì‘ì—… ì§„í–‰
        return self.fc(x.view(x.size(0), -1))
```

<br>

### 2. Data Parallel

- ë°ì´í„°ë¥¼ Batchë¡œ ë‚˜ëˆ ì„œ Lossì˜ Gradientë¥¼ êµ¬í•˜ê³  ê²°ê³¼ê°’ì˜ í‰ê· ìœ¼ë¡œ ë‹µì„ ì–»ìŒ
- Minibatchì™€ ë¹„ìŠ·í•œ ë°©ì‹
- í”„ë¡œì„¸ìŠ¤
    1. GPUì— ë™ì¼í•œ ëª¨ë¸ì„ ë³µì‚¬í•´ì„œ ë„£ì–´ì¤Œ
    2. Dataë¥¼ ë‚˜ëˆ ì„œ ê°ê° GPUì— Input
    3. Layerì—°ì‚°
    4. ì—°ì‚°í•œ ê°’ì„ GPUí•˜ë‚˜ë¡œ ëª¨ì•„ì„œ í‰ê· ì„ ëƒ„ -> í•œ GPUì— ì—°ì‚°ì´ ëª°ë ¤ ë³‘ëª©í˜„ìƒ ë°œìƒ

![image](https://user-images.githubusercontent.com/77658029/130342446-bcf2c123-44c3-445d-9c9f-4ccdc94f9962.png)

- `torch.nn.DataParallel(model)`ë§Œ ì¶”ê°€í•˜ë©´ Data parallel ì‚¬ìš©ê°€ëŠ¥í•¨

**ğŸ“°code**
```python
parallel_model = torch.nn.DataParallel(model) # Encapsulate the model 

predictions = parallel_model(inputs) # Forward pass on multi-GPUs
loss = loss_function(predictions, labels) # Compute loss function
loss.mean().backward() # Average GPU-losses + backward pass
optimizer.step() # Optimizer step
predictions = parallel_model(inputs) # Forward pass with new parameters
```

<br>

### 3. DistributedDataParallel

- ê° CPU ë§ˆë‹¤ Processë¥¼ ìƒì„±í•˜ì—¬ ê°œë³„ GPUì— í• ë‹¹ -> ê¸°ë³¸ì ìœ¼ë¡œ Data parallelë¡œ í•˜ë‚˜ ê°œë³„ì ìœ¼ë¡œ ì—°ì‚°ì˜ í‰ê· ì„ ëƒ„
- CPUê°œìˆ˜ë‘ GPUê°œìˆ˜ë¥¼ ë™ì¼í•˜ê²Œ ë§Œë“¤ì–´ì„œ ì§„í–‰í•¨
- Dataparallelì€ Layerë§ˆë‹¤ ê°’ì„ ëª¨ì•„ì£¼ê³  í¼ì³ì£¼ëŠ” ì‘ì—…ì´ ìˆì–´ì§€ë§Œ,
- DistributedDataParallelëŠ” ì—°ì‚°ì´ ëë‚˜ê³  ê²°ê³¼ê°’ë§Œ í•©ì³ì£¼ê¸° ë•Œë¬¸ì— ë³‘ëª©í˜„ìƒì—ì„œ ì¢€ ë” ììœ ë¡œì˜´
- DistributedDataParallelì˜ ê²½ìš°ëŠ” Multiprocessing ê¸°ë²•ì´ í•„ìš”í•¨, web ì²˜ëŸ¼ í†µì‹ ê·œì•½ í†µí•´ì„œ ì§„í–‰ë˜ê²Œë¨

**ğŸ“°code**
```python
train_sampler = torch.utils.data.distributed.DistributedSampler(train_data)
shuffle = False
pin_memory = True
trainloader = torch.utils.data.DataLoader(train_data, batch_size=20, shuffle=shuffle, pin_memory=pin_memory, num_workers=3,
shuffle=shuffle, sampler=train_sampler)


def main():
    n_gpus = torch.cuda.device_count()
    torch.multiprocessing.spawn(main_worker, nprocs=n_gpus, args=(n_gpus, ))
    
def main_worker(gpu, n_gpus):
    image_size = 224
    batch_size = 512
    num_worker = 8
    epochs = 100
    batch_size = int(batch_size / n_gpus)
    num_worker = int(num_worker / n_gpus)
    torch.distributed.init_process_group(
    backend='ncclâ€™, init_method='tcp://127.0.0.1:2568â€™, world_size=n_gpus, rank=gpu) #Multiprocessing í†µì‹  ê·œì•½
    model = MODEL
    torch.cuda.set_device(gpu)
    model = model.cuda(gpu)
    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[gpu]) # DistributedDataParallel ì •ì˜
```

<br>

**ğŸ“Œreference**
- boostcourse AI tech
- [SIA](https://blog.si-analytics.ai/12)

<br>

```
ğŸ’¡ ìˆ˜ì • í•„ìš”í•œ ë‚´ìš©ì€ ëŒ“ê¸€ì´ë‚˜ ë©”ì¼ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤!ğŸ’¡ 
```
