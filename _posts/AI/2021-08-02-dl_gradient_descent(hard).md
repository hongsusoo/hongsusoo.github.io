---
defaults:
  - scope:
      path: ""
      type: posts
    values:
      layout: single
      author_profile: true
      comments: true
      share: true
      related: true

title: "ê²½ì‚¬í•˜ê°•ë²• ë§¤ìš´ë§›(gradient descent)"
excerpt: "about : python"
toc: true
toc_sticky: true
toc_label: "Label"
categories:
  - DL
tags:
  - [gradient descent]
date: 2021-08-02
last_modified_at: 2021-08-02
---
<br>

# ê²½ì‚¬í•˜ê°•ë²•ì„ í†µí•œ ì„ í˜• íšŒê·€

- ì—­í–‰ë ¬ê³¼ ë¬´ì–´íœë¡œì¦ˆ(Moore-penrose)ì—­í–‰ë ¬ì„ í†µí•˜ì—¬ ì„ í˜•íšŒê·€ê°€ ê°€ëŠ¥í•˜ì§€ë§Œ, ì„ í˜• ëª¨ë¸ì— ëŒ€í•´ì„œë§Œ ì ìš©ì´ ê°€ëŠ¥í•¨

**ğŸ”‘ ê²½ì‚¬í•˜ê°•ë²•ì„ í†µí•œ ì„ í˜•íšŒê·€**

- ì„ í˜•ëª¨ë¸ì„ í¬í•¨í•˜ì—¬ ë‹¤ë¥¸ ëª¨ë¸ë„ ëŒ€ì‘ì´ ê°€ëŠ¥í•¨
- ì—­í–‰ë ¬ì„ í†µí•œ íšŒê·€ë²•ë³´ë‹¤ ë” powerfulí•œ ë°©ë²•

## ì„ í˜•íšŒê·€ì— ëŒ€í•œ ì´í•´ 

- ë§ì€ ìˆ˜ì˜ dataê°€ ìˆì„ ê²½ìš°($ë²¡í„°ì˜ ìˆ˜â‰¥ë²¡í„°ì˜ ê¸¸ì´$), ê·¸ ëª¨ë“  ë²¡í„°ë¥¼ ë§Œì¡±í•˜ëŠ” í•´ë‹µì„ êµ¬í•  ìˆ˜ ì—†ìŒ(ì¼ì§ì„ ì— ìˆëŠ” ê²½ìš°ëŠ” ì œì™¸)
- ë”°ë¼ì„œ ì„ í˜•íšŒê·€ ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ë°ì´í„°ë“¤ì„ ì•„ìš°ë¥¼ ìˆ˜ ìˆëŠ” ìµœì ì˜ ì‹ì„ ì°¾ì•„ì•¼ í•¨

ğŸ’¡ data -> í–‰ë ¬ì‹ìœ¼ë¡œ í‘œí˜„

![image](https://user-images.githubusercontent.com/77658029/127202018-d3dff48d-4964-43e8-8b8a-dea17975253f.png)

- ì°¾ê³ ì í•˜ëŠ” ìµœì ì˜ ì‹ì„ $X\beta = \hat Y â‰ˆ Y$ë¡œ ì •ì˜í•˜ì—¬ $Y-\hat Y$ë¥¼ ìµœì†Œë¡œ ë§Œë“œëŠ” $\beta$ì„ êµ¬í•´ì•¼ í•œë‹¤.
- ë¬´ì–´íœë¡œì¦ˆ ì—­í–‰ë ¬ì„ ì‚¬ìš©í•˜ë©´, ë°”ë¡œ ìµœì†Œê°€ ë˜ëŠ” $\beta$ì„ êµ¬í•  ìˆ˜ ìˆì§€ë§Œ, ì‚¬ìš©ë²”ìœ„ê°€ ë” ë„“ì€ ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ íŠ¸ë¼ì´ í•´ë³´ì
- **ì„ í˜•íšŒê·€ ëª©ì ì‹** $||Y-\hat Y||_2 = ||Y-X\beta||_2$ê°€ ìµœì†Œê°€ ë˜ëŠ” $\beta$ êµ¬í•˜ê¸° ìœ„í•´ gradient vectorë¥¼ êµ¬í•´ë³´ì

ğŸ”‘ gradient ìˆ˜ì‹ í’€ì´

![image](https://user-images.githubusercontent.com/77658029/127207989-1e0c5c33-21ad-42f8-9d4d-0089bc31175e.png)

- ì í™”ì‹ í‘œí˜„ $\beta^{(t+1)} = \beta^{(t)} - \lambda\nabla_\beta||Y-X\beta^{(t)}|| = \beta^{(t)} + \frac{2\lambda}{n}X^T(y-X\beta^{(t)}$


## ì„ í˜•íšŒê·€ algorithm êµ¬í˜„(ê²½ì‚¬í•˜ê°•ë²•)

**ğŸ“°code**
```python
import numpy as np

#í•œí‰ë©´ ìƒ -> [1,2,3]ì´ ì •ë‹µ
X= np.array([[1,1],[1,2],[2,2],[2,3]]) 
y= np.dot(x,np.array([1,2]))+3

# 0,0 ë§Œ ë‹¤ë¥¸ í‰ë©´ì— ìˆì„ ê²½ìš° -> [0.7333, 1.8666, 0.7333]ì´ ì •ë‹µ
# X= np.array([[1,1],[1,2],[2,2],[2,3],[0,0]]) 
# y= np.array([3,5,6,8,1])
beta_gd=[10.1,15.1,-6.5] 
X_=np.array([np.append(x,[1]) for x in X])

for t in range(10000): 
    error = y - X_@beta_gd
#     error = error/np.linalg.norm(error) #ì´ë¶€ë¶„ì„ ì•ˆí•´ì£¼ë©´ learning_rate:0.01ì— ë°˜ì˜ë¨
    grad = - np.transpose(X_) @ error
    beta_gd = beta_gd - 0.01*grad
    
print(beta_gd)
```
**ğŸ”result**
```
[1. 2. 3.]
```

## ê²½ì‚¬í•˜ê°•ë²•ì€ ë§ŒëŠ¥ì¼ê¹Œ?

- ê²½ì‚¬í•˜ê°•ë²•ì€ ê²°êµ­ ë¯¸ë¶„ê°€ëŠ¥ì´ ê°€ëŠ¥í•˜ê³ , ìµœì†Œì ì„ êµ¬í•  ìˆ˜ ìˆëŠ” ë³¼ë¡(convex)í•œ í•¨ìˆ˜ì— ëŒ€í•´ì„œëŠ” ìˆ˜ë ´ì´ ê°€ëŠ¥í•œë‹¤
- ì ì ˆí•œ í•™ìŠµë¥ ê³¼ í•™ìŠµíšŸìˆ˜ë¥¼ ì„ íƒí–ˆì„ ë•Œ ìˆ˜ë ´ì´ ë³´ì¥ë¨
- ì„ í˜•íšŒê·€ì˜ ê²½ìš°ëŠ” ëª©ì ì‹($||Y-X\beta||_2$)ì´ íšŒê·€ê³„ìˆ˜$\beta$ì— ëŒ€í•´ ë³¼ë¡í•¨ìˆ˜ì´ê¸° ë•Œë¬¸ì— ìˆ˜ë ´ê°€ëŠ¥ì„±ì´ ë³´ì¥ëœë‹¤
- í•˜ì§€ë§Œ ê·¹ì†Œê°’ì´ ì—¬ëŸ¬ê°œê±°ë‚˜, ë¯¸ë¶„ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°ëŠ” ì‚¬ìš©í•˜ì§€ ëª»í•œë‹¤(ë¬´ì–´íœë¡œì¦ˆ ì—­í–‰ë ¬ë³´ë‹¤ëŠ” ë” ë§ì€ ëª¨ë¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤)
- ë¹„ì„ í˜•íšŒê·€ ë¬¸ì œì˜ ê²½ìš° ëª©ì ì‹ì´ ë³¼ë¡í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìˆ˜ë ´ì„±ì´ í•­ìƒ ë³´ì¥ë˜ì§€ ì•ŠìŒ

![image](https://user-images.githubusercontent.com/77658029/127268266-add97292-b46f-40a7-a05d-0f94e4d1a011.png)

### ê²½ì‚¬í•˜ê°•ë²• gradient vector ê³„ì‚°

<br>

<a href="https://hongsusoo.github.io/categories/GradientDescent"><img src="https://img.shields.io/badge/gradient vector-ìˆ˜ì‹ ìœ ë„-green"/></a>


<br>

## í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•(Stochastic Gradient Descent)

- ê²½ì‚¬í•˜ê°•ë²•ì€ non-convexí•¨ìˆ˜ì— ëŒ€í•´ì„œ ìµœì†Œê°’ì´ ë³´ì¥ë˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ë¬¸ì œ ìˆìŒ
- ê·¸ê±¸ ë³´ì™„í•˜ê¸° ìœ„í•œ ë°©ë²•ì„
- í•œ ê°œ í˜¹ì€ ì¼ë¶€ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ì—…ë°ì´íŠ¸(ê¸°ì¡´ ê²½ì‚¬í•˜ê°•ë²•ì€ ëª¨ë“  ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ì—…ë°ì´íŠ¸)
- ì¼ë¶€ ë°ì´í„°ë¥¼ í™œìš©í•˜ê¸° ë•Œë¬¸ì— ì—°ì‚°ìì›ì„ íš¨ìœ¨ì ìœ¼ë¡œ í™œìš© ê°€ëŠ¥í•¨($O(d^2n) -> O(d^2b)$ : ì—°ì‚°ëŸ‰ b/në§Œí¼ ê°ì†Œí•¨)
![image](https://user-images.githubusercontent.com/77658029/127457548-7697a65a-4fbc-4485-8230-cd355c57f433.png)

- ë³¼ë¡ì´ ì•„ë‹Œ(non-convex) ëª©ì ì‹ì€ SGDë¥¼ í†µí•´ ìµœì í™” ê°€ëŠ¥í•¨
- mini-batchëŠ” 10~1000ê°œì˜ ë°ì´í„°ë¥¼ ê°€ì§€ê³  í•¨
- ë”¥ëŸ¬ë‹ì—ì„œëŠ” SGDê°€ ì‹¤ì¦ì ìœ¼ë¡œ ë” ë‚«ë‹¤ëŠ” í‰ê°€ë¥¼ ë°›ìŒ

![image](https://user-images.githubusercontent.com/77658029/127513594-67c3d399-52a8-41cc-97c6-a4cb95a2ca8a.png)

ğŸ’¡ mini-batch SGD 

![image](https://user-images.githubusercontent.com/77658029/127514342-237efeb8-6344-4358-a818-e9535045f5b4.png)
![image](https://user-images.githubusercontent.com/77658029/127514249-95da0f34-1a68-4189-9822-6a5c85596d29.png)


## ê²½ì‚¬í•˜ê°•ë²• vs ë¯¸ë‹ˆë°°ì¹˜ SGD

![image](https://user-images.githubusercontent.com/77658029/127515340-fca8d0ed-c7c3-4330-9c2f-f9ef4563bd25.png)


## ê²½ì‚¬í•˜ê°•ë²• vs ë¯¸ë‹ˆë°°ì¹˜ SGD code ë¹„êµ

**ğŸ“°code**
```python
import numpy as np
import sympy as sym
from sympy.abc import x,w,b #ì¶”ê°€
from sympy.plotting import plot

train_x = (np.random.rand(1000) - 0.5) * 10
train_y = np.zeros_like(train_x)


def func(val):
    fun = sym.poly(7*x + 2)
    return fun.subs(x, val)

def l2_norm(x): #ì¶”ê°€
    x_norm = x*x
    x_norm = np.sum(x_norm)
    x_norm = np.sqrt(x_norm)
    return x_norm    

for i in range(1000):
    train_y[i] = func(train_x[i])

# initialize
lr_rate = 1e-2
w, b = 0.0, 0.0
n_data = 1000
errors_gd = []

# ê²½ì‚¬í•˜ê°•ë²•
expand_x = np.array([np.append(x, [1]) for x in train_x])

for i in range(100):
    ## Todo
    # ì˜ˆì¸¡ê°’ y
    _y = w*train_x+b
    
    # gradient
    gradient_w = - np.transpose(expand_x)[0]@(train_y-_y)/n_data #l2_norm(train_y-_y) #ê°’ì´ ì‘ì–´ì§€ë©° learning rateë¥¼ í¬ê²Œ ë“¦
    gradient_b = - np.transpose(expand_x)[1]@(train_y-_y)/n_data #l2_norm(train_y-_y) 

    # w, b update with gradient and learning rate
    w -= lr_rate * gradient_w
    b -= lr_rate * gradient_b
    
    # L2 normê³¼ np_sum í•¨ìˆ˜ í™œìš©í•´ì„œ error ì •ì˜
    error_gd = l2_norm(train_y-_y)
    # Error graph ì¶œë ¥í•˜ê¸° ìœ„í•œ ë¶€ë¶„
    errors_gd.append(error_gd)

print("ê²½ì‚¬í•˜ê°•ë²• w : {} / b : {} / error : {}".format(w, b, error_gd))

# initialize
w, b = 0.0, 0.0
n_data = 10
minibatch_size = 10
errors_sgd = []

# í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•
for i in range(100):
    ## Todo
    #minibatch 10
    minibatch_idx = np.random.choice(train_y.shape[0], minibatch_size, replace = True)
    minibatch_x = train_x[minibatch_idx]
    minibatch_y = train_y[minibatch_idx]
    
    # ì˜ˆì¸¡ê°’ y
    _y = w*minibatch_x +b
    
    # gradient
    gradient_w = -np.transpose(minibatch_x)@(minibatch_y-_y)/n_data    #l2_norm(minibatch_y-_y) #ê°’ì´ ì‘ì–´ì§€ë©° learning rateë¥¼ í¬ê²Œ ë“¦
    gradient_b = -np.ones(10)@(minibatch_y-_y)/n_data                  #l2_norm(minibatch_y-_y)

    # w, b update with gradient and learning rate
    w = w - lr_rate*gradient_w
    b = b - lr_rate*gradient_b
    
    # L2 normê³¼ np_sum í•¨ìˆ˜ í™œìš©í•´ì„œ error ì •ì˜
    error_sgd = l2_norm(minibatch_y-_y)
    # Error graph ì¶œë ¥í•˜ê¸° ìœ„í•œ ë¶€ë¶„
    errors_sgd.append(error_sgd)

print("í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²• w : {} / b : {} / error : {}".format(w, b, error_sgd))
```
**ğŸ”result** 
ì •ë‹µì€ w : 7, b : 2
```
ê²½ì‚¬í•˜ê°•ë²• w : 7.000429738930361 / b : 1.2742577531192194 / error : 23.181636293256783
í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²• w : 7.039753269083189 / b : 1.2571391750926415 / error : 2.2038064311604137
```

### GD vs SGD ê·¸ë˜í”„

**ğŸ“°code**
```python
from IPython.display import clear_output
import matplotlib.pyplot as plt
%matplotlib inline

def plot(errors_gd,errors_sgd):
    clear_output(True)
    plt.figure(figsize=(20,5))
    plt.subplot(211)
    plt.ylabel('errors_gd')
    plt.xlabel('time step')
    plt.plot(errors_gd)
    plt.show()
    
    plt.figure(figsize=(20,5))
    plt.subplot(212)
    plt.ylabel('errors_sgd')
    plt.xlabel('time step')
    plt.plot(errors_sgd)
    plt.show()
plot(errors_gd,errors_sgd)
```
**ğŸ”result** 
![image](https://user-images.githubusercontent.com/77658029/128598620-edeb82e4-64d0-4b49-829a-3e24af5ef3f7.png)


**ğŸ“Œreference**
- boostcourse AI tech
- https://www.kakaobrain.com/blog/113

<br>

```
ğŸ’¡ ìˆ˜ì • í•„ìš”í•œ ë‚´ìš©ì€ ëŒ“ê¸€ì´ë‚˜ ë©”ì¼ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤!ğŸ’¡ 
```
