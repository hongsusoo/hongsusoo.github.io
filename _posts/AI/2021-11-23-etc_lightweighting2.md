---
defaults:
  - scope:
      path: ""
      type: posts
    values:
      layout: single
      author_profile: true
      comments: true
      share: true
      related: true

title: "ê²½ëŸ‰í™” ëª¨ë¸ ì°¾ê¸° : AutoML"
excerpt: "about : ê²½ëŸ‰í™”"
toc: true
toc_sticky: true
toc_label: "Label"
categories:
  - AI ETC
tags:
  - [ê²½ëŸ‰í™”, AutoML]
date: 2021-11-23
last_modified_at: 2021-11-23
---

<br>

## ê¸°ì¡´ Deep Learning ê³¼ì •

- ìµœì ì˜ ëª¨ë¸ê³¼ Hyperparameterë¥¼ êµ¬í•˜ê¸° ìœ„í•œ ëŠì„ì—†ëŠ” ë°˜ë³µì˜ ê³¼ì •

![image](https://user-images.githubusercontent.com/77658029/142879225-5874b114-46a3-449d-825e-4fa3b11b9bea.png)

**â—â“ ì´ëŸ° ë‹¨ìˆœ ë°˜ë³µì ì¸ ì‘ì—…ì„ ìë™ì ìœ¼ë¡œ í•  ìˆ˜ ì—†ì„ê¹Œ?**

## ê²½ëŸ‰í™” ëª¨ë¸ ì°¾ê¸° : AutoML

- ëª¨ë¸ì„ ê²½ëŸ‰í•˜ê¸° ìœ„í•´ 2ê°€ì§€ ì ‘ê·¼ ë°©ë²•ì´ ìˆìŒ
    1. ì£¼ì–´ì§„ ëª¨ë¸ì„ ê²½ëŸ‰í™” : Pruning, Tensor Decomposition
    2. ìƒˆë¡œìš´ ê²½ëŸ‰í™” ëª¨ë¸ ì°¾ê¸° : NAS, AutoML
    
- AutoMLì€ Searchë¥¼ í†µí•œ ê²½ëŸ‰ ëª¨ë¸ì„ ì°¾ëŠ” ê¸°ë²•

![image](https://user-images.githubusercontent.com/77658029/142881549-550ea946-2eb3-4014-be01-f8fade181dc5.png)

### AutoML ë¬¸ì œ ì •ì˜(HPO)

- HPO : Hyperparameter Optimization
    - Lossë¥¼ ìµœì†Œí™”í•˜ëŠ” Configuration $\lambda^{*}$ë¥¼ íƒìƒ‰í•˜ëŠ” ë¬¸ì œ
![image](https://user-images.githubusercontent.com/77658029/142880658-27fd886f-4a03-4c16-b6f3-4e3e8bd8a8e0.png)
- Search Space
    - Categorical : Optimizer(adam, SGD), Module(Conv, BottleNeck, InvertedResidual)
    - Continuous : Learning Rate, Regularize Parameter
    - Integer : Batch size, Epochs

### Bayesian Optimization(BO)

![image](https://user-images.githubusercontent.com/77658029/142881690-a7a3cf40-1ac9-4ca4-aaf5-b3da55a626cb.png)

- Surrogate Function : $f(\lambda)$ì˜ Regression model
- Acquisition Function : ë‹¤ìŒ í™•ì¸í•  Point ì„ ì •

### Bayesian Optimization(BO) ê³¼ì • ì„¤ëª… - GPR

![image](https://user-images.githubusercontent.com/77658029/142886007-4fb9167c-9f0c-4bd6-aa86-bbec4de00ec0.png)
- GPR : Gaussian Process Regression
- Regressionì˜ ê²½ìš° ì£¼ì–´ì§„ Dataë¡œ ì—ì¸¡í•˜ê³  ì‹¶ì€ fì— ê°€ì¥ ê°€ê¹ê²Œ ê·¼ì‚¬ì‹œí‚¤ëŠ” ê²ƒ
- ê¸°ì¡´ì— ì£¼ì–´ì§„ $(X,Y)$ì™€ ìƒˆë¡­ê²Œ ì£¼ì–´ì§„ $X_{\ast}$ë¥¼ í™œìš©í•˜ì—¬ $Y_{\ast}$ ì¶”ì •
- ë‹¨ì ì€ High-dim(O(N^3)), Continuous, Discreteê°€ í˜¼ì¬ëœ ê²½ìš° ì ìš© ì–´ë ¤ì›€

#### ì¶”ì • ê³¼ì •

1. í•¨ìˆ˜ë“¤ì˜ ë¶„í¬ë¥¼ ê°€ì •í•¨(ì—¬ê¸°ì„œëŠ” Multivariate Gaussian Distribution)
    - $Y_{*} = f(x)$ë¥¼ Random Variableë¡œ ë³´ê³  ë‹¤ë¥¸ Random Variableë“¤ë„ ëª¨ë‘ Multivariate Gaussian Distributionê´€ê³„ì— ìˆë‹¤ê³  ê°€ì •
    - $f(x)$ëŠ” Gaussian Processë¥¼ ë”°ë¦„

â­$Kernel$ì€ ë‘ ê°€ì§€ ë²¡í„°ì˜ ìœ ì‚¬ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í•¨ìˆ˜ë¡œ ë‚´ì ê³¼ ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ê°€ì§


### Bayesian Optimization(BO) ê³¼ì • ì„¤ëª… - TPE

- TPE : Tree-structured Parzen Estimator
- ì—°ì‚° : GPRëŠ” GPR($p(f\vert\lambda)$)ì—°ì‚°ì„ í•˜ì§€ë§Œ, TPEëŠ” $p(f\vert\lambda)$ì™€ $p(\lambda)$ë¥¼ ê³„ì‚°í•¨
- í˜„ì¬ê¹Œì§€ì˜ Observationë“¤ì„ íŠ¹ì • Quantile(inverse CDF)ë¡œ êµ¬ë¶„
- KDE(Kernel Density Estimation)ìœ¼ë¡œ Good Observation ë¶„í¬(p(g)), bad Observationì˜ ë¶„í¬ (p(b))ë¥¼ ê°ê° ì¶”ì •
- p(g)/p(b)ëŠ” EI(Expected Improvement, Acquisition Functionì¤‘ í•˜ë‚˜)ì— ë¹„ë¡€í•˜ì—¬ ë†’ì€ ê°’ì„ ê°€ì§€ëŠ” $\lambda$ë¥¼ ë‹¤ìŒ Stepìœ¼ë¡œ ì„¤ì •

![image](https://user-images.githubusercontent.com/77658029/142892429-f074b290-f8ad-4be4-8106-a55129e10da0.png)


## í•œê³„ì 

- ì†Œìš” ì‹œê°„ì´ í¬ê³  Scalability ë“±ì˜ ë¬¸ì œê°€ ìˆìŒ

![image](https://user-images.githubusercontent.com/77658029/142894044-6c96312b-ee90-4d55-a6cb-7e324b7d6bbf.png)

## ê·¹ë³µí•˜ê¸° ìœ„í•œ ë…¸ë ¥

- Hyperparameter Gradient Descent(íƒìƒ‰ê³¼ í•™ìŠµì„ ë™ì‹œì— ì§„í–‰)
- Meta-Learning(Auto "AutoML")
- Multi-Fidelity Optimization
    - Dataì˜  Subsetë§Œì„ í™œìš©
    - ì ì€ Epoch
    - RLì„ í™œìš©í•œ ì ì€ Trial
    - Image Downsampling
- RandomAugmentationì„ í†µí•œ ì‹œê°„ ì ˆì•½

â­ì–´ëŠì •ë„ Prior ê°œì…, ì ì€ Search space, ëŒ€í‘œì„±ì„ ê°€ì§€ëŠ” Subset í™œìš©, early terminate(ASHA Scheduler, BOHB(Bayesian Optimiation & Hyperband) ë“±ë“± ê¸°ë²•ì„ í™œìš©í•˜ì—¬ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ

![image](https://user-images.githubusercontent.com/77658029/142959858-12d6ad77-b3ed-43c1-a876-6b0288dc7dcd.png)

- Search space : module block 7ê°œ, Hyperparameterê³ ì •, batch 128, epoch 200, SGD, Cosine annealing, Randaug ì ìš©


**ğŸ“Œreference**
- boostcourse AI tech
- [Kernel](https://sonsnotation.blogspot.com/2020/11/11-1-kernel.html)
- [GPR](https://sonsnotation.blogspot.com/2020/11/11-2-gaussian-progress-regression.html)
- [ì‹œê°í™” Exploration Gaussian Process](https://distill.pub/2019/visual-exploration-gaussian-processes/)
- [[Gaussian process ì°¸ê³ ìë£Œ] Bayesian Deep Learning; ìµœì„±ì¤€ êµìˆ˜ë‹˜](https://www.edwith.org/bayesiandeeplearning/lecture/24811?isDesc=false)
- [A Visual Exploration of Gaussian Processes](https://distill.pub/2019/visual-exploration-gaussian-processes)
- [Towards automating machine learning - Dr Thorben Jensen](https://www.youtube.com/watch?v=7lvwCZsrTn4)
- [Algorithms for Hyper-Parameter Optimization](https://papers.nips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf)


<br>

```
ğŸ’¡ ìˆ˜ì • í•„ìš”í•œ ë‚´ìš©ì€ ëŒ“ê¸€ì´ë‚˜ ë©”ì¼ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤!ğŸ’¡ 
```
