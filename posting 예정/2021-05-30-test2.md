---
defaults:
  - scope:
      path: ""
      type: posts
    values:
      layout: single
      author_profile: true
      comments: true
      share: true
      related: true

title: "ADsP 자료 정리"
excerpt: "about : ADsP"
toc: true
toc_sticky: true
toc_label: "Label"
categories:
  - data_science
tags:
  - [data_science,ADsP]
date: 2021-05-22
last_modified_at: 2021-05-22
---

# ADsP 내용 정리


## Part 1 데이터 이해  
---
### 1장 데이터의 이해

- #### 1절 데이터와 정보
    - 1.  데이터의 정의
        - 데이터1646년 영국 문헌에 처음 등장 라틴어 Dare(주다)의 과거형인 주어진것
        - 1940년대 이후 컴퓨터 시대 시작과 함께 자연과학분만 아니라 경영학, 통계학 등 다양한 사회과학이 진이보 하며, 데이터의 의미는 과거의 관념적이고 추상적인 개념에서 기술적이고 사실적인 의미로 변화
        - 데이터는 추론과 추정의 근거를 이루는 사실
        - 단순 객체로서의 가치에 추가적으로 다른 객체와의 상호관계 속에서 가치를 갖는것
    - 2. 데이터의 특성
        - 존재적 특성 : 객관적 사실
        - 당위적 특성 : 추론, 예측, 전망, 추정을 위한 근거
    - 3. 데이터의 유형
        - 정성적 데이터
            - 형태 : 언어, 문자
            - 예시 : 회사 매출이 증가
            - 특징 : 저장, 검색, 분석에 많은 비용 소모
        - 정량적 데이터
            - 형태 : 수치, 도형, 기호
            - 예시 : 나이, 몸무게, 주가
            - 특징 : 정형화가 된 데이터로 비용 소모가 적음
    - 4. 지식경영의 핵심 이슈
        - 암묵지와 형식지의 상호작용이 중요
        - 암묵지 : 개인에게 축정된 내면화된 지식 → 조직의 지식으로 공통화
        - 형식지 : 언어, 기호, 숫자로 표출화된 지식 → 개인의 지식으로 연결화
    - 5. 데이터와 정보의 관계
        - DIKW
            - Data : 개별 데이터 자체로는 의미가 중요하지 않은 객관적인 사실
            - Information : 데이터의 가공, 처리와 데이터간 연관관계 속에서 의미가 도출된것
            - Knowledge : 고유의 지식으로 내재화 된것
            - Wisdom : 지식의 축적과 아이디어가 결합된 창의적인 산물
        - DIWK 피라미드

            ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled.png)

- #### 2절 데이터 베이스 정의와 특징
    - 1. 용어의 연혁
        - 1950년 : 데이터 베이스 이름 탄생
        - 1963년 : 데이터베이스 초기개념
        - 1963년 : GE의 C.바크만은 데이터베이스 관리 시스템 IDS개발
        - 1975년 : 미국의 CAC가 KORSTIC을 통해 서비스되어 우리나라에서 데이터베이스 이용이 이뤄짐
    - 2. 데이터베이스의 정의
        - 1차개념확대 : 정형데이터 관리
        - 2차개념확대 : 빅데이터 출현, 비정형 데이터 포함
    - 3. 데이터베이스의 특징
        - 1. 일반적인 특징
            - 통합된 데이터 : 동일한 데이터가 중복되지 않음을 의미
            - 저장된 데이터 : 컴퓨터가 접근할 수 있는 저장 매체에 저장되는것
            - 공용 데이터 : 여러 사용자가 다른 목적으로 데이터 이용_대용량화, 복잡한 구조
            - 변화되는 데이터 : 현시점의 상태를 나타냄( 삽입, 삭제, 갱신으로 변화하더라도 정확한 데이터 유지)
        - 2. 다양한 측면에서의 특징
            - 정보의 축적 및 전달 : 기계가독성, 검색가독성, 원격조작성
            - 정보 이용 측면 : 정보를 신속하게 획득, 정확하고 경제적으로 데이터 찾음
            - 정보 관리 측면 : 일정한 질서로 정리, 저장, 검색, 관리하여 방대한 양의 정보를 체계적으로 관리
            - 정보기술 발전 측면 : 정보처리, 검색,관리 소프트웨어, 관련 하드웨어, 정보전송을 위한 네트워크 기술의 발전을 견인
            - 경제 산업측면 : 경제 산업 사회 활동에 효율성 제고, 국민의 편의 증진
- #### 3절 데이터 베이스의 활용
    - 1. 기업내부 데이터 베이스
        - 1980년대
            - OLTP(On-Line Transaction Processing) : 호스트 컴퓨터가 데이터 베이스를 액세스하고 바로 처리 결과를 돌려보내는 형태(주문입력시스템, 재고관리시스템) _ 데이터 갱신 위주
            - OLAP(On-Line Analytical Processing) : 정보위주의 분석처리, 다양한 비즈니스 관점에서 쉽고 빠르게 다차원적인 데이터에 접근하여, 의사결정에 활용할 수 있는 정보를 얻게 해주는 기술_데이터 조회 위주
        - 2000년대
            - CRM(Customer Relationship Management) : 고객 관계 관리, 기업이 고객과 관련된 내,외부 자료를 분석, 통합해 고객 중심 자원 극대화
            - SCM(Supply Chain Management) : 공급망 관리, 기업에서 원재료 생산, 유통등 모든 공급망 단계를 최적화해 수요자가 원하는 제품을 원하는 시간, 장소에 제공
            - 
        - 각 분야별 데이터베이스 개념
            - 제조부문 : 클라이언트/서버 → 웹기반 데이터베이스, ERP → CRM(Customer Relationship Management)로 발전(대기업), RTE을 통한 IT화 확대(Real Time Enterprise)
            - 금융부문 : EAI(Enterprise Application Integration) - 정보를 중앙집중적으로 통합, 관리, 사용할 수 있는 환경 구축, EDW(Enterprise Data Warehouse) - 기업 리소스의 유기적 통합, 다원화된 관리체계 장비, 데이터의 중복 방지등을 위해 시스템을 재설계
            - 유통부문 : KMS(Knowledge Management System) - 지식관리시스템
        - 사회기반구조로서의 데이터베이스
            - EDI(Electronic Data Interchange) : 주문서, 납품서, 청구서 등 무역에 필요한 각종 서류를 표준화된 양식을 통해 거래처에 전송하는 시스템
            - VAN(Value Added Network) : 독자적인 네트워크로 각종 정보를 부호, 영상, 음성등으로 교환하거나 정보를 축적, 단순한 통신이 아닌 부가가치가 높은 소비스
            - CALS(Commerce At Light Speed) : 제품의 설계, 개발, 생산에서, 유통 폐기에 이르는 제품의 라이프사이클 전반에 관련된 데이터를 공유, 교환할 수있는 경영통합정보시스템
            - NEIS(National Education Information System) : 교육 행정 정보

### 2장 데이터의 가치와 미래

- #### 1절 빅데이터의 이해
    - 1. 빅데이터의 이해
        - 관점에 따른 정의

            첫째, 좁은 범위의 정의 : 3V로 요약되는 데이터 자체의 특성 변화에 초점

            둘째, 중간 범위의 정의 : 데이터 자체 + **처리, 분석 기술 변화(데이터 처리, 저장, 분석, 기술 아키텍처, 클라우드 컴퓨터)**

            셋째, 넓은 관점의 정의 : 데이터 자체 + 처리, 분석 기술 변화 + **인재, 조직 변화(Data Scientist같은 새로운 인재, 데이터 중심 조직)**

        - 3V(가트너 그룹의 더그 래니의 3V) + (4V(Value가치, Visualization시각, Veracity정확))
            - Volume(양) - 데이터 규모의 측면(센싱데이터, 비정형데이터)
            - Variety(다양성) - 데이터의 유형과 소스 측면(정형, 비정형데이터(영상, 사진))
            - Velocity(속도) - 데이터의 수집과 처리 측면(원하는 데이터 추출 및 분석속도)
    - 2. 출현 배경과 변화
        - 산업계 : 고객데이터 축적 - 데이터에 숨어있는 가치를 발굴, 성장원동력으로 사용
        - 학계 : 거대 데이터 활용, 과학 확산 - 거대 데이터를 다루는 학문 분야 많아짐, 아키텍쳐, 통계도구들이 발전
        - 기술 발전 : 관련 기술의 발달 - 디지털화, 저장 기술의 발달, 인터넷 보급, 모바일 혁명, 클라우드 컴퓨팅
        - 데이터 규모 : EB(EXA Byte, 90년대) → ZB진입(Zetta Byte, 2011년=1.8ZB) → ZB 본격화 시대(20년, 11년대비 50배 증가)
        - 데이터 유형 : 정형데이터                 → 비정형 데이터                                → 사물정보, 인지 정보
        - 데이터 특성 : 구조화                         → 다양성, 복잡성, 소셜                      → 현실성, 실시간성
    - 3. 빅데이터의 기능
        - 비유적 표현
            - 산업혁명의 석탄, 철 : 제조업 포함 서비스분야에서 생산성을 획기적으로 끌어올림
            - 21세기의 원유 : 사업전반 생산성 한단계 더 향상, 기존에 없던 새로운 범주의 산업 생산
            - 렌즈 : 렌즈를 통해 현미경이 생물학 발전에 미쳤던 영향 만큼 데이터가 산업 발전에 영향을 미칠것, 구글 Ngram Viewer
            - 플랫폼 : 공동활용의 목적으로 구축된 유무형의 구조물로 다양한 서드파티 비지니스에 활용되면서 플랫폼 역할을 할 것으로 전망
    - 4. 빅데이터의 본질적인 변화
        - 사전처리 → 사후처리 : 필요한 정보만 사용 → 가능한 많은 데이터 확보, 다양한 방식으로 조합하여 숨은 정보 발굴
        - 표본조사 → 전수조사 : 클라우드 컴퓨팅 기술발전으로 데이터 처리비용 감소, 표본조사 → 전수조사하여 샘플링 못하는 패턴이나 정보 발굴
        - 질 → 양 : 데이터 지속 추가시, 양질의 데이터가 오류 정보보다 많아 전체적으로 좋은 결과 산출
        - 인과관계 → 상관관계 : 상관관계를 통한 인사이트
- #### 2절 빅데이터의 가치와 영향
    - 1. 빅데이터의 가치 : 여러가지 변수로 인해 가치 산정이 어려움
        - 데이터 활용 방식 : 데이터의 재사용, 재조합, 다목적용 데이터 개발이 일반화되며, 특정 데이터를 언제 어디서 누가 활용하는지 알기 어려움
        - 새로운 가치 창출 : 기존에 없던 가치를 창출하여 가치 측정이 어려움
        - 분석 기술 발전 : 현재는 가치가 없는 데이터지만, 추후 새로운 분석 기법으로 거대한 가치로 탈바꿈 가능
    - 2. 빅데이터의 영향
        - 기업 : 혁신, 경쟁력제고, 생산성향상 → 소비자의 행동 분석, 시장변동 예측, 비즈니스 모델 혁신 및 신사업 발굴
        - 정부 : 환경 탐색, 상황분석, 미래대응 → 기상, 인구이동, 각종통계, 번제 데이터등을 수집해 사회 변화를 추정, 관련정보를 추출
        - 개인 : 목적에 따른 활용 → 데이터 분석 비용 하락, 개인 인지도 향상에 빅데이터 활용
- #### 3절 비즈니스 모델
    - 1. 빅데이터 활용 사례
        - 기업 : 구글, 사용자 로그데이터 활용 → 검색 서비스 개선
        - 정부 : 국가 안전 확보 활동을 위한 실시간 모니터링(기후정보, 지질활동, 소방서비스), 의료와 교육개선에 빅데이터로 해결책 모색
        - 개인 : 사회관계망 분석으로 유세지역 선정, 효과적인 선거활동, 가수, 음악청취기록 분석, 노래 순서 짜는데 활용
    - 2. 빅데이터 활용 기본 테크닉
        - 연관규칙학습 : 상관관계 분석_연관분석
        - 유형분석 : 문서 분류, 조직 그룹화 _ Classifier
        - 유전자알고리즘 : 최적화 필요문제, 점진적 진화
        - 기계학습 : 훈련데이터로 알려진 특성 활용 예측
        - 회귀분석 : 독립변수를 조작, 종속변수가 어떻게 변하는지 보면서 두 변인의 관계 파악
        - 감정분석 : 주제나 감정 분석
        - 소셜네트워크분석(사회관계망분석) : 영향력 있는 사람 찾기

- #### 4절 위기 요인과 통제방안
    - 1. 빅데이터 시대의 위기 요인
        - 사생활 침해 : 개인정보를 목적외에 활용할 경우 사생활 침해를 넘어 사회 경제적 위협으로 변형 → 익명화(anonymization) 기술 발전이 필요
        - 책임 원칙 훼손 : 분석이 되는 사람들은 예측 알고리즘의 희생양이 될 가능성 증가(마이너리티 리포트, 범행을 저지르기전에 체포, 신용도와 무관하게 대출 거부)
        - 데이터 오용 : 데이터에 대한 의존으로 항상 맞을 수 없음
    - 2. 위기 요인에 따른 통제 방안
        - 동의에서 책임으로 : 사용주체의 적극적인 보호장치를 강구
        - 결과 기반 책임 원칙 고수 : 책임원칙 훼손 위기 요인에 대한 통제 방안으로 기존의 원칙을 좀 더 보강하고 강화
        - 알고리즘 접근 허용 : 알고리즘에 대한 접근권을 제공하여 예측 알고리즘의 부당함을 반증할 수 있는 방법을 명시에 공개 주문
- #### 5절 미래의 빅데이터(활용에 필요한 3요소)
    - 데이터 : 모든 것의 데이터화
        - 목적없이 축적된 데이터를 통한 창의적인 분석, 새로운 가치로 부상
    - 기술 : 진화하는 알고리즘, 인공지능
        - 대용량 데이터를 빠르게 처리하기 위한 알고리즘의 진화와 함께 스스로 학습하고 데이터를 처리할 수 있는 인공지능 기술 출현
    - 인력 : 데이터 사이언티스트 알고리즈미스트
        - 빅데이터의 다각적 분석을 통한 인사이트 도출이 중요

### 3장 가치 창조를 위한 데이터 사이언스와 전략 인사이트

- #### 1절 빅데이터 분석과 전략 인사이트
    - 1. 빅데이터 열풍과 회의론
        - 빅데이터 분석에서 찾을 수 있는 수많은 가치들을 발굴하기도 전에 차단 당할 수 있다.
    - 2. 빅데이터 회의론의 원인 및 진단 : 빅데이터에 포커스가 아닌 분석을 통한 가치를 만드는 것에 집중
        - 투자효과를 거두지 못했던 부정적 학습효과 → 과거의 고객관계관리(CRM)
            - 공포 마케팅이 잘 통하는 영역 : 도입만 하면 모든 문제를 한번에해소 할 것 처럼 강조
            - 투자 후 어떻게 활용, 가치를 뽑아내야 할지 난감
        - 빅데이터 성공사례가 기존 분석 프로젝트를 포함해 놓은 것이 많다
            - 굳이 빅데이터가 필요없는 경우
            - 국내 빅데이터 업체들이 CRM 분석 성과를 빅데이터 분석으로 과대포장

    - 3. 빅데이터 분석 'big'이 핵심이 아니다.
        - 빅데이터에 대한 관심 증대 : 데이터 기반의 통찰의 중요성에 대한 공감대
        - 빅데이터 프로젝트에 거는 기대 : 거시적이고 전략적인 가치를 이끌어낼 수 있을 것으로 기대
        - 빅데이터 분석의 가치 : 데이터는 크기의 이슈가 아니라 어떤 통찰을 얻을 수 있느냐가 문제, 객관적이고 종합적인 통찰을 줄 수 있는 데이터를 찾는 것이 그 무엇보다 중요함
    - 4. 전략적 통찰이 없는 분석의 함정
        - 분석이 경쟁의 본질을 제대로 바라보지 못할 때, 쓸모없는 분석결과들만 잔뜩 쏟아냄, 전략적인 통찰력
    - 5. 전략도출을 위한 가치기반 분석
        - 해당 사업에 중요한 기회를 발굴하고, 주요 경영진의 지원을 얻어낼 수 있음
        - 활용 범위를 더 넓고 전략적으로 변화
        - 전략적 인사이트를 주는 가치 기반 분석 단계로 나아감
    - 6. 일차원적인 분석 vs 전략도출 위한 가치기반 분석

        ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%201.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%201.png)

- #### 2절 전략 인사이트 도출을 위한 필요 역량
    - 1. 데이터 사이언스의 의미와 역할
        - 의미 : 전문 지식을 종합한 학문, 다양한 유형의 데이터를 대상으로 분석 + 효과적으로 구현 + 전달하는 과정까지 포함한 포괄적 개념
        - 데이터 사이언티스트의 역할 : 성과를 좌우하는 핵심이슈에 답하고, 사업 성과를 견인할 수 있어야함, 소통력 필요
    - 2. 데이터 사이언스의 구성요소
        - Analytics, 비즈니스 분석, IT 전문성
        - 데이터 사이언티스트의 역할 : 데이터 찾고, 구조화, 연결, 강력한 호기심, 질문을 찾고, 가설을 세움, 스토리텔링, 커뮤티케이션, 창의력, 열정, 직관력, 비판적 시각, 글쓰기

    - 3. 데이터 사이언티스트의 요구 역량
        - Hard skill-정량적
            - 이론적 지식(기업에 대한 이해, 방법론 습득)
            - 분석 기술에 대한 숙련(노하우 축적)
        - Soft skill-정성적
            - 통찰력 있는 분석
            - 설득력 있는 전달
            - 다분야간 협력
    - 4. 전략적 통찰력과 인문학의 부활
        - 통찰력 있는 분석 : 1. 전체 업계의 방향 이해, 2. 고객 이해, 3. 넓은 시각
        - 외부 환경의 변화
            - 컨버전스 → 디컨버전스 : 단순세계화 → 복잡한 세계화(규모의 경제, 세계화, 표준화, 이성화 → 복잡한 세계, 다양성, 관계, 연결성, 창조성)
            - 생산 → 서비스 : 제품 생산 → 서비스로 비즈니스 중심 이동(고장나지 않는 제품의 생산 → 뛰어난 서비스로 응대)
            - 생산 → 시장창조 : 기술경쟁 → 무형자산의 경쟁(생산에 관련된 기술 중심 → 현재 패러다임에 근거한 시장 창조 현지 사회와 문화에 관한 지식)
            - 
- #### 3절 빅데이터 그리고 데이터 사이언스의 미래
    - 1. 빅데이터의 시대
    - 2. 빅데이터 회의론을 넘어 가치 패러다임의 변화
        - 과거 : 아날로그 세상을 어떻게 디지털화하는가
        - 현재 : 디지털화된 정보와 대상들을 연결, 연결을 효과적, 효율적으로 제공하는가
        - 미래 : 복잡한 연결을 얼마나 효과적이고 믿을 만하게 관리하는가
    - 3. 데이터 사이언스의 한계
        - 데이터 사이언스의 한계
            - 분석과정에서 인간의 해석이 개입되는 단계를 거침
            - 분석결과가 의미하는 바는 사람에 따라서 해설과 결론이 달라짐
            - 정량적인 분석이라도 모든 분석은 가정에 근거함
- #### 기타 최신 빅데이터 상식
    - 1.  DBMS와 SQL
        - DBMS(Data Base Management System)
            - 데이터 베이스를 공유하며 사용할 수 있는 환경을 제공하는 S/W
            - 데이터 베이스를 구축하는 틀 제공
            - 효율적인 데이터 검색, 저장 기능 제공
            - 오라클, 인포믹스, 액세스
        - DBMS의 종류
            - 관계형 DBMS
                - 하나 이상의 테이블로 이뤄짐
                - 고유키로 각 Row를 식별
                - 엔티티 타입을 대표함
            - 객체지향 DBMS
                - 객체 형태로 표현하는 데이터 베이스 모델
            - 네트워크 DBMS
                - 레코드들이 노드로, 레코드들의 사이의 관계가 간선으로 표현되는 그래프기반으로 하는 데이터 베이스 모델
            - 계층형 DBMS
                - 트리 구조를 기반으로 하는 데이터베이스 모델
        - SQL(Structured Query Language)
            - 데이터 베이스에 접근할 수 있는 하부언어
            - 단순한 질의 기능 뿐 아니라 완전한 데이터의 정의와 조작 기능을 갖춤
            - 테이블 단위 연산
            - 영어 문자과 비슷한 구문으로 초보자도 쉽게 사용
            - 
    - 2. Data에 관련한 기술
        - 개인정보 비식별 기술 : 개인을 식별할 수 있는 정보를 지우거나 대체해 개인을 알아볼 수 없도록 하는 기술
            - 데이터 마스킹 : 데이터의 길이, 유형, 형식과 같은 속성을 유지한채, 새롭고 읽기 쉬운 데이터를 익명으로 생성하는 기술
            - 가명처리 : 개인정보 주체의 이름을 다른 이름으로 변경하는 기술, 규칙이 노출되면 안됨
            - 총계처리 : 데이터의 총합 값을 보임,( 특정 속성을 지닌 개인으로 구성된 단체의 속성정보는 주의해야함)
            - 데이터값 삭제 : 필요없는 값 또는 개인 식별에 중요한 값 삭제, 개인과 관련된 날짜정보는 연단위로
            - 데이터 범주화 : 데이터 값을 범주의 값으로 변한
        - 무결성과 레이크
            - 데이터 무결성 : 일관성, 유효성, 신뢰성을 보장하기 위해 데이터 변경 /수정시 여러가지 제한을 두어 정확성을 보증

                ㄴ 개체 무결성, 참조 무결성, 범위 무결성

            - 데이터 레이크(data lake) : 방식에 상관없이 데이터를 저장하는 시스템, 대용량의 정형/비정형 데이터를 저장 및 접근성을 높인 대규모의 저장소를의미
    - 3. 빅데이터 분석 기술
        - 하둡 : 여러 개의 컴퓨터를 하나인 것처럼 묶어 대용량 데이터를 처리, 분산파일 시스템(HDFS)을 통해 수 천대의 장비에 대용량 파일을 저장할 수 있는 기능
        - Apache spark : 실시간 분산형 컴퓨터 플랫폼, In-memory 방식으로 하둡보다 빠름(스칼라, 자바, R, 파이썬, API를지원)
        - smart factory : 공장 내 설비와 기계에 IOT를 설치, 공정데이터 실시간, 데이터 기반의 의사결정이 이뤄짐, 생산성 극대화
        - machine learning&deep learning : 인간의 학습 능력과 같은 기능을 컴퓨터에 실현,

## Part 2 데이터 분석 기획

### 1장 데이터 분석 기획의 이해

- #### 1절 분석기획 방향성 도출
    - 1. 분석기획의 특징
        - 분석기획이란 : 과제를 정의 → 결과를 도출 → 관리 사전에 계획하는 것(어떤 목표(what), 어떠한 데이터, 근거(why)로 어떻게 수행할지(How)결정하는 작업)
        - 데이터 사이언스티스트의 역량 : 수학적 지식, 도메인 지식, 정보기술력,
    - 2. 분석 대상과 방법
        - 대상(what), 방법(how)에 따라 4가지로 나누어짐
            - Optimization(최적화) : 대상 - O, 방법 - O
            - Solution(해결책) : 대상 - O, 방법 - X
            - Insight(통찰) : 대상 - X, 방법 - O
            - Discovery(발견) : 대상 - X, 방법 - X
    - 3. 목표 시점 별 분석 기획 방안
        - 과제 중심적인 접근 방식 : 당면한 과제를 빠르게 해결(Problem Solving)
            - 1차 목표 : Speed&Test
            - 과제 유형 : Quick & Win
            - Problem Solving
        - 장기적인 마스터 플랜 방식 : 지속적인 분석 내재화(Problem Definition)
            - 1차 목표 : Accuracy&Deploy(효율)
            - 과제 유형 : Long Term View
            - Problem Definition
    - 4. 분석 기획시 고려사항
        1. 가용데이터 : 데이터 유형에 따른 적용 가능 솔루션 및 분석 방법이 다르기 때문에 **데이터 유형**에 대한 **분석이 선행**적으로 필요함
        2. 적절한 활용방안과 유즈케이스 : 기존에 구현되어 있는 유사 분석 시나리오 및 솔루션을 최대한 활용(바퀴를 재 발명하지 마라)
        3. 장애요소들에 대한 사전 계획 수립 : 조직에 내재화 하기 위한 지속적인 교육과 활용방안등의 변화 관리
    - 데이터 종류

        ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%202.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%202.png)

- #### 2절 분석 방법론
    - 1. 분석 방법론 개요
        - 개요
            - 체계화된 절차와 방법이 정리된 데이터 분석 방법론 수립 필요
            - 프로젝트는 우연한 성공에 기인해선 안됨
            - 일정한 수준의 품질을 갖춘 산출물과 프로젝트의 성공 가능성을 확보/제시
            - 방법론은 상세한 절차(procesdures), 방법(methods), 도구와 기법(tool&techniques), 템플릿과 산출물(Templates&Outputs)로 구성됨
        - 데이터 기반 의사 결정의 필요성
            - 경험과 감에 따른 의사결정 → 데이터 기반의 의사결정
            - 기업의 합리적 의사결정을 가로막는 장애요소 : 고정관념, 편향된 생각, 프레이밍 효과
        - 방법론의 생성과정

            ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%203.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%203.png)

        - 방법론의 적용 업무의 특성에 따른 모델
            - 폭포수 모델(Waterfall Model)
                - 단계를 순차적으로 진행하는 방법
                - 이전 단계가 완료되어야 다음 단계로 나아감 문제 발견시 피드백 과정 수반
            - 프로토타입 모델(Prototype Model)
                - 폭포수 단점 보완
                - 점진적 개발 방식
                - 고객의 요구 완벽하게 이해 못함
                - 일부분 우선 개발하여 사용자에게 제공(prototype) → 그 결과를 통한 개선 작업
            - 나선형 모델
                - 반복을 통한 점증적 개발
                - 처음 시도하는 프로젝트에 적용 용이
                - 관리 체계 미흡할 시 복잡도 상승
        - 방법론의 구성
            - 단계 : 최상위 계층, 프로세스 그룹을 통하여 완성된 단계별 산출물이 생성, 각 단계는 기준선으로 설정되어 관리, 버전관리등을 통해 통제(단계별 완료보고서)
            - 태스크 : 단계를 구성하는 활동단위, 물리적 또는 논리적 단위로 품질 검토의 항목이 된다(보고서)
            - 스탭 : WBS(work breakdown structur)의 워크패키지에 해당, 입력자료, 처리및도구, 출력자료로 구성된 단위 프로세스(보고서 구성요소)
    - 2. KDD 분석 방법론
        - 개요
            - KDD(Knowledge Discovery in Database)
            - 1996년 Fayyad가 프로파일링 기술을 기반 데이터 통계적 패턴이나 지식을 찾기위해 체계적으로 정리한 데이터 마이닝 프로세스
        - KDD 분석 절차
            - 1. 데이터셋 선택
                - 데이터베이스, 원시데이터에서 분석에 필요한 데이터를 선택하는 단계
                - 목표 데이터를 구성하여 분석에 활용
                - 비즈니스 도메인 이해, 프로젝트 목표 설정 필수
            - 2. 데이터 전처리
                - 추출된 분석 대상용 데이터 셋에 포함되어 있는 잡음(noise), 이상치(outlier), 결측치(missing value)를 식별, 제거, 변환
                - 데이터셋 정제
                - 추가로 요구되는 데이터 셋이 필요할 경우 데이터 선택 프로세스를 재실행
            - 3. 데이터 변환
                - 데이터 분석 목적에 맞게 변수 생성, 선택
                - 데이터의 차원 축소
                - 효율적 데이터 마이닝을 위한 데이터 변경하는 단계
                - 학습용(training data), 검증용(test data)로 분리
            - 4. 데이터 마이닝
                - 분석 목적에 맞게 데이터 마이닝 기법 선택
                - 적절한 알고리즘 사용
                - 필요에 따라 데이터 전처리, 데이터 변환 프로세스를 추가로 실행
            - 5. 데이터 마이닝 결과 평가
                - 결과에 대한 해석과 평가
                - 분석 목적과의 일치성 확인
                - 데이터 마이닝을 통해 발견한 지식을 업무에 활용
                - 필요에 따라 데이터셋 선택, 데이터 전처리, 데이터 변환, 데이터 마이닝 과정을 반복 수행
    - 3. CRISP-DM 분석 방법론
        - 개요
            - CRISP-DM(Cross Industry Standard Process For Data Mining)
            - 1996년 시작
            - 주요 5개의 업체들이 주도하여 사용
            - 계측적 프로세스 모델로 4개 레벨로 구성
        - 1. CRISP-DM의 4레벨 구조
            - 여러개의 단계(Phases)로 구성_최상위 레벨
            - 각 단계는 일반화 태스크(generic tasks)를 포함 → 일반화 태스크는 데이터 마이닝의 단일 프로세스를 완전하게 수행하는 단위
            - 세분화 태스크(Specialized Tasks) _ 구체적인 수행 레벨
            - 프로세스 실행(process instances) - 데이터 마이닝을 위한 구체적인 실행을 포함
        - 2. CRISP-DM의 프로세스
            - CRISP-DM 6단계 프로세스
                - 업무이해
                    - 프로젝트의 목적과 요구사항을 이해하는 단계
                    - 도메인 지식을 데이터 분석을 위한 문제정의로 변경
                    - 초기 프로젝트를 계획

                    → 업무 목적파악, 상황파악, 데이터 마이닝 목표설정, 프로젝트 계획 수립

                - 데이터 이해
                    - 데이터를 수집
                    - 데이터 속성 이해
                    - 데티어 품질에 대한 문제점 식별 및 인사이트 발견 단계

                    → 초기 데이터 수집, 데이터 기술 분석, 데이터 탐색, 데이터 품질 확인

                - 데이터 준비
                    - 수집된 데이터에서 분석기법에 적합한 데이터를 편성하는 단계
                    - 시간 오래걸림

                    → 분석용 데이터 셋 선택, 데이터 정제, 데이터셋 편성, 데이터 통합, 데이터 포맷팅

                - 모델링
                    - 다양한 모델링 기법과 알고리즘을 선택
                    - 파라미터 최적화하는 단계
                    - 데이터 셋 추가 필요하면, 데이터 준비 단계 반복 수행
                    - 테스트용 데이터셋으로 평가하여 과적합 문제 확인

                    → 모델링 기법 선택, 모델 테스트 계획 설계, 모델 작성, 모델 평가

                - 평가
                    - 프로젝트 목정에 부합하는지 평가
                    - 데이터 마이닝 결과를 최종적으로 수용할 것인지 판단

                    → 분석결과 평가, 모델링 과정 평가, 모델 적용성 평가

                - 전개
                    - 모델링과 평가 단계를 통해 완성된 모델을 실업무에 적용
                    - 모니터링과 모델 유지보수계획 마련
                    - 도메인 특성, 입력 데이터의 품질편차, 운영모델 평가 기준에 따라 생명주기(life cycle)가 다양함으로 상세한 전개 계획 필요
                    - CRISP-DM의 마지막 단계로 프로젝트 종료 관련 프로세스 수행

                    → 전개 계획 수립, 모니터링과 유지보수 계획 수립, 프로젝트 종료보고서 작성, 프로젝트 리뷰

    - 4. KDD와 CRISP-DM 비교

        ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%204.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%204.png)

    - 5. 빅데이터 분석 방법론
        - 빅데이터 분석의 계측적 프로세스
            - 단계(Phase)
                - 프로세스 그룹을 통하여 완성된 단계별 산출물 생성
                - 각 단계는 기준선으로 관리
                - 버전관리(Configuration Management)를 통하여 통제됨
            - 테스크(Task) :
                - 각 단계는 여러 테스크로 구성
                - 테스크는 각 단계를 구성하는 활동 단위
                - 물리적, 논리적 단위로 품질 검토의 항목이 될 수 있다
            - 스텝(Step)
                - WBS(Work Breakdown structure)의 워크 패키지에 해당
                - **입력, 자료, 처리 및 도구, 출력 자료**로 구성된 프로세스
        - 빅데이터 분석 방법론 - 5단계
            - 분석기획 : 비즈니스 도메인과 문제점 인식, 프로젝트 수행계획 수립
                - 비즈니스 이해
                    - 내부 업무 매뉴얼 숙지, 외부 관련 자료 조사, 향후 프로젝트 방향 설정
                    - 입력자료 : 업무 매뉴얼, 전문가 지식, 빅데이터 분석 대상, 도메인의 관련자료
                    - 프로세스 및 도구 : 자료 수집 및 비즈니스 이해
                    - 출력자료 : 비즈니스 이해 및 도메인 문제점
                - 프로젝트 범위설정
                    - 프로젝트 목적에 부합하는 범위 설정, 프로젝트 범위정의서 작성(SOW, Statement of Work)
                    - 입력자료 : 중장기 계획서, 빅데이터 분석 프로젝트 지시서, 비즈니스 이해 및 도메인 문제점
                    - 프로세스 및 도구 : 자료 수집 및 비즈니스 이해, 프로젝트 범위 정의서 작성 절차
                    - 출력자료 : 프로젝트 범위 정의서(SOW)
                - 데이터 분석 프로젝트 정의
                    - 프로젝트 목표 및 KPI, 목표수준 구체화, 상세 프로젝트 정의서 작성, 모델 운영 이미지 및 평가기준 설정
                    - 입력자료 : 프로젝트 범위 정의서(SOW), 빅데이터 프로젝트 지시서
                    - 프로세스 및 도구 : 프로젝트 목표 구체화, 모델 운영 이미지 설계
                    - 출력자료 : 프로젝트 정의서, 모델 운영 이미지 설계서, 모델 평가 기준
                - 프로젝트 수행 계획 수립
                    - 프로젝트 목적 및 배경, 기대효과, 수행방법, 일정, 추진조직, 프로젝트 관리방안 작성, WBS는 산출물 위주로 작성, 프로젝트 범위 명확화
                    - 입력자료 : 프로젝트 범위 정의서(SOW), 모델 운영 이미지 설계서, 모델 평가 기준
                    - 프로세스 및 도구 : 프로젝트 범위 정의서(SOW), WBS작성
                    - 출력자료 : 프로젝트 수행계획서, WBS
                - 데이터 분석 위험 식별
                    - 프로젝트 산출물 참조, 전문가의 판단을 활용하여 발생 가능한 **위험 식별**, **영향도**와 **빈도**, **가능성**에 따라 **우선순위 설정**
                    - 입력자료 : SOW, 프로젝트 수행 계획서, 선행 프로젝트 산출물 및 정리자료
                    - 프로세스 및 도구 : 위험 식별 절차, 위험 영향도 및 발생 가능성 분석, 위험 우선순위 판단
                    - 출력자료 : 식별된 위험 목록
                - 위험 대응 계획 수립
                    - 식별된 위험은 정량적, 정성적 분석 및 대응방안 수립, 회피(Avoid), 전이(Transfer), 완화(Mitigate), 수용(Accept)으로 구분하여 위험관리 계획서 작성
                    - 입력자료 : 식별된 위험 목록, SOW, 프로젝트 수행 계획서
                    - 프로세스 및 도구 : 위험 정량적 분석, 위험 정성적 분석
                    - 출력자료 : 위험 관리 계획서
            - 데이터 준비 : 비즈니스 요구사항과 데이터 분석에 필요한 원천 데이터 정의 및 준비
                - 데이터 정의
                    - 시스템, 데이터베이스 파일, 문서 등의 다양한 내,외부의 데이터 소스로 부터 분석에 필요한 데이터를 정의
                    - 입력자료 : 프로젝트 수행 계획서, 시스템 설계서, ERD(Entity Relationship Diagram), 메타데이터 정의서, 문서자료
                    - 프로세스 및 도구 : 내,외부 데이터 정의 , 정형, 비정형, 반정형 데이터 정의
                    - 출력자료 : 데이터 정의서
                - 데이터 획득방안 수립
                    - 다양한 데이터 소스로 부터 데이터 수집 방안 수립, 내부 데이터(부서간 업무협조, 개인정보보호 및 정보보안 문제점 사전점검), 외부데이터(다양한 인터페이스 및 법적 문제점 고려하여 상세한 데이터 획득 계획 수립)
                    - 입력자료 : 데이터 정의서, 시스템 설계서, ERD, 메타데이터 정의서, 문서자료, 데이터 구입
                    - 프로세스 및 도구 : 데이터 획득 방안 수립
                    - 출력자료 : 데이터 획득 계획서
                - 정형데이터 스토어 설계
                    - 일반적으로 관계형 데이터 베이스(RDBMS)를 사용, 저장과 활용을 위해 데이터 스토어의 논리적, 물리적 설계를 구분하여 설계
                    - 입력자료 : 데이터 정의서, 데이터 획득 계획서
                    - 프로세스 및 도구 : 데이터 베이스 논리, 물리 설계, 데이터 맵핑
                    - 출력자료 : 정형데이터 스토어 설계서, 데이터 맴핑 정의서
                - 비정형데이터 스토어 설계
                    - 하둡, NoSQL등 이용하여 비정형, 반정형 데이터를 저장하기 위한 논리적, 물리적 데이터 스토어 설계
                    - 입력자료 : 데이터 정의서, 데이터 획득 계획서
                    - 프로세스 및 도구 : 비정형, 반정형, 데이터 논리, 물리설계
                    - 출력자료 : 비정형 데이터 스토어 설계서, 데이터 맵핑 정의서
                - 데이터 수집 및 저장
                    - 크롤링, ETL도구 등을 통한 데이터 수집, 데이터 스토어 저장
                    - 입력자료 : 데이터 정의서, 데이터 획득 게획서, 데이터 스토어 설계서
                    - 프로세스 및 도구 : 데이터 크롤링 도구, ETL 도구(Extraction, Transformation, Loading), 데이터 수집 스크립트
                    - 출력자료 : 수집된 분석용 데이터
                - 데이터 정합성 점검
                    - 데이터 스토어의 품질 점검을 통하여, 데이터 정합성 확보, 품질 개선 보완작업
                    - 입력자료 : 수집된 분석용 데이터
                    - 프로세스 및 도구 : 데이터 품질 확인, 정합성 점검 리스트
                    - 출력자료 : 정합성 점검 보고서
            - 데이터 분석 : 원천 데이터 → 데이터셋으로 편성, 다양한 분석 기법과 알고리즘을 이용하여 데이터 분석, 필요에 따라 **데이터 준비단계**로 **피드백**하여 반복
                - 비즈니스 룰 확인
                    - 비즈니스 이해, 도메인 문제점 인식, 프로젝트 목표 정확하게 인식, 분석에 필요한 데이터 범위파악
                    - 입력자료 : 프로젝트 정의서, 프로젝트 수행 계획서, 데이터 정의서, 데이터 스토어
                    - 프로세스 및 도구 : 프로젝트 목표 확인, 비즈니스 룰 확인
                    - 출력자료 : 비즈니스 룰, 분석에 필요한 데이터 범위
                - 분석용 데이터셋 준비
                    - 데이터 스토어에서 정형, 비정형 데이터 추출, 필요시 적절한 가공, 추출된 데이터는 데이터 베이스나 구조화된 형태로 구성
                    - 입력자료 : 데이터 정의서, 데이터 스토어
                    - 프로세스 및 도구 : 데이터 선정, 데이터 변환, ETL도구
                    - 출력자료 : 분석용 데이터 셋
                - 텍스트 데이터 확인 및 추출
                    - 텍스트 스토어 에서 필요한 텍스트 데이터 추출
                    - 입력자료 : 비정형 데이터 스토어
                    - 프로세스 및 도구 : 분석용 테스트 데이터 확인, 텍스트 데이터 추출
                    - 출력자료 : 분석용 텍스트 데이터
                - 텍스트 데이터 분석
                    - 추출된 텍스트 데이터를 분석도구로 적재 다양한 기법으로 분석하여 모델구축, 용어사전 사전확보, 도메인에 맞도록 작성, 구축된 모델은 시각화를 통해 모델의 의미전달을 명확하게
                    - 입력자료 : 분석용 텍스트 데이터, 용어사전(유의어사전, 불용어 사전)
                    - 프로세스 및 도구 : 분류체계 설계, 형태소 분석, 키워드 도출, 토픽분석, 감성분석, 의견분석, 네트워크 분석
                    - 출력자료 : 텍스트 분석 보고서
                - 탐색적 데이터 분석
                    - 다양한 관점 별로 기초통계량(평균,분산, 표준편차, 최대, 최소값)을 산출하여 데이터의 분포와 변수간의 관계등을 이해하고 모델링을 위한 기초 자료로 활용
                    - 입력자료 : 분석용 데이터셋
                    - 프로세스 및 도구 : EDA도구(Exploratory Data Analysis), 통계분석, 연관성 분석, 데이터 분포 확인
                    - 출력자료 : 데이터 탐색 보고서
                - 데이터 시각화
                    - 모델의 시스템화를 위한 시각화 목저으로 활용시 시각화 기획, 시각화 설계, 시각화 구현등의 별도의 프로세스 필요, 향후 시스템 구현시 프로토타입으로 활용가능함
                    - 입력자료 : 분석용 데이터 셋
                    - 프로세스 및 도구 : 시각화도구, 시각화 패키지, 인포그래픽, 시각화 방법론
                    - 출력자료 : 데이터 시각화 보고서
                - 데이터 분할
                    - 훈련용, 검증용 데이터 셋 구분, 모델 적용 기법에 따라 검증 횟수, 생성 모델 개수 설정
                    - 입력자료 : 분석용 데이터 셋
                    - 프로세스 및 도구 : 데이터 분할 패키지
                    - 출력자료 : 훈련용 데이터, 테스트용 데이터
                - 데이터 모델링
                    - 기계학습등을 이용한 데이터 분류, 예측, 군집등의 모델링을 만들어 기운영 시스템에 적용
                    - 입력자료 : 분석용 데이터 셋
                    - 프로세스 및 도구 : 통계 모델링 기법, 기계학습, 모델 테스트
                    - 출력자료 : 모델링 결과 보고서
                - 모델 적용 및 운영 방안
                    - 모델에 대한 상세 알고리즘 설명서 작성, 의사코드 수준의 상세한 작성이 필요, 모니터링 방안 수립
                    - 입력자료 : 모델링 결과 보고서
                    - 프로세스 및 도구 : 모니터링 방안 수립, 알고리즘 설명서 작성
                    - 출력자료 : 알고리즘 설명서, 모니터링 방안
                - 모델 평가
                    - 프로젝트 정의서의 모델 평가 기준에 따라 모델을 객관적으로 평가, 결과를 보고 알고리즘을 파악하고, 테스트용 데이터나 필요시 모델 검증을 위한 별도의 데이터 활용
                    - 입력자료 : 모델링 결과 보고서, 평가용 데이터
                    - 프로세스 및 도구 : 모델평가, 모델품질관리, 모델 개선작업
                    - 출력자료 : 모델 평가 보고서
                - 모델 검증
                    - 모델의 실적용성 검증, 검증용데이터를 이용해 검증 작업을 실시, 모델링 검증 보고서 작성, 테스트용 데이터가 아닌, 실 운영용 데이터를 확보해 모델의 품질을 최종 검증
                    - 입력자료 : 모델링 결과 보고서, 모델 평가 보고서, 검증용 데이터
                    - 프로세스 및 도구 : 모델검증
                    - 출력자료 : 모델 검증 보고서
            - 시스템 구현 : 분석 기획에 맞는 모델을 도출, 운영중인 시스템에 적용 및 구현
                - 시스템 분석 및 설계
                    - 가동중인 시스템 분석, 알고리즘 설명서에 근거하여 응용시스템 구축 설계 프로세스를 진행
                    - 입력자료 : 알고리즘 설명서, 운영중인 시스템 설계서
                    - 프로세스 및 도구 : 정보 시스템 개발 방법론
                    - 출력자료 : 시스템 분석 및 설계서
                - 시스템 구현
                    - 시스템 분석 및 설계서에 따라 BI 패키지를 활용하거나 새롭게 시스템을 구축하거나 가동중인 운영 시스템의 커스터마이징 등을 통해 설계된 모델 구현
                    - 입력자료 : 시스템 분석 및 설계서, 알고리즘 설명서
                    - 프로세스 및 도구 : 시스템 통합 개발 도구(IDE), 프로그램언어, 패키지
                    - 출력자료 : 구현 시스템
                - 시스템 테스트
                    - 시스템 검증을 위해 단위 테스트, 통합테스트, 시스템 테스트 실시, 적용된 시스템의 객관성과 완전성을  확보
                    - 입력자료 : 구현 시스템, 시스템 테스트 계획서
                    - 프로세스 및 도구 : 품질 관리 활동
                    - 출력자료 : 시스템 테스트 결과보고서
                - 시스템 운영 계획
                    - 사용자 대상 교육 실시, 시스템 운영 계획 수립
                    - 입력자료 : 시스템 분석 및 설계서, 구현 시스템
                    - 프로세스 및 도구 : 운영계획 수립, 운영자 및 사용자교육
                    - 출력자료 : 운영자 매뉴얼, 사용자 매뉴얼, 시스템 운영 계획서
            - 평가 및 전개 : 프로젝트 성과 평가, 정리, 모델의 발전 계획을 수립, 차기 분석 기획으로 전달하고 프로젝트를 종료
                - 모델 발전 계획
                    - 지속적인 운영과 기능향상을 위해 발전계획을 상세하게 수립
                    - 입력자료 : 구현 시스템, 프로젝트 산출물
                    - 프로세스 및 도구 : 모델 발전 계획 수립
                    - 출력자료 : 모델 발전 계획서
                - 프로젝트 성과 평가
                    - 정량적, 정성적 성과를 나누어 성과 평가서를 작성
                    - 입력자료 : 프로젝트 산출물, 품질관리 산출물, 프로젝트 정의서, 프로젝트 수행 계획서
                    - 프로세스 및 도구 : 프로젝트 평가기준, 프로젝트 정량적 평가, 프로젝트 정성적 평가
                    - 출력자료 : 프로젝트 성과 평가서
                - 프로젝트 종료
                    - 모든 산출물을 지식 자산화 하고, 최종 보고서를 작성하여 의사소통 절차를 따라 보고하고 종료
                    - 입력자료 : 프로젝트 산출물, 품질관리 산출물, 프로젝트 정의서, 프로젝트 수행 계획서, 프로젝트 성과 평가서
                    - 프로젝트 및 도구 : 프로젝트 지식 자산화 작업, 프로젝트 종료
                    - 출력자료 : 프로젝트 최종 보고서
- #### 3절 분석 과제 발굴
    - 1. 분석과제 발굴 방법론
        - 개요
            - 분석과제는 다양한 문제를 데이터 분석 문제로 변환하고 관계자들과 이해하고 프로젝트 수행할 수 있는 **과제 정의서** 도출 필요
        - 하향식 접근 방법 (Top Down Approach)
            - 1. 문제 탐색(Problem Discovery) 단계
                - 개요
                    - 전체적인 관점의 기준 모델을 활용하여 빠짐없이 문제 도출, 식별
                    - 기업 내, 외부 환경을 포괄하는 비즈니스 모델과 외부 참조 모델 존재
                    - 과제 발굴단계에서 세부적인 구현 및 솔루션에 초점이 아니라 **문제 해결함**으로 발생하는 **가치 중점**
                - 비즈니스 모델 기반 문제 탐색 5단계
                    - 비즈니스 모델 캔버스 5가지 영역
                        - 업무(operation)
                            - 제품,서비스 생산을 위해 운영하는 내부 프로세스 및 주요자원 관련 주제 도출
                            - 생산 공정 최적화, 재고량 최소화
                        - 제품(product)
                            - 제품, 서비스를 개선 하기 위한 주제 도출
                            - 제품의 주요기능 개선, 서비스 모니터링 지표 도출
                        - 고객(customer)
                            - 사용자 및 고객, 이를 제공하는 채널의 관점에서 주제 도출
                            - 고객 Call 대기 시간 최소화, 영업점 위치 최적화
                        - 규제와 감사(regulation & audit)
                            - 규제 및 보안의 관점에서 주제 도출
                            - 제공 서비스 품질의 이상 징후 관리, 새로운 환경 규제 시 예상 되는 제품 추천
                        - 지원 인프라(IT & Human & Resource)
                            - 분석을 수행하는 시스템 영역 및 이를 운영, 관리하는 인력의 관점에서 주제 도출
                            - EDW 최적화(Enterprise Data  warehouse), 적정 운영 인력 도출
                - 분석 기회 발굴의 범위 확장 4단계
                    - 거시적 관점의 메가 트랜드(STEEP)
                        - 사회(Social)
                            - 사회적, 문화적, 구조적, 트렌드 변화에 기반한 분석 기회를 도출
                            - 노령화, 미레니엄 세대 등장, 저출산에 따른 사업모델 변화
                        - 기술(Technological)
                            - 과학, 기술, 의학 등 최신 기술의 등장 및 변화에 따른 역량 내 재화와 제품 서비스 개발 기회 도출
                            - 나노기술, IT융합 기술, 로봇 기술의 고도화로인한 제품의 Smart화
                        - 경제(Economic)
                            - 산업과 금융 전반의 변동성 및 경제 구조 변화 동향에 따른 흐름 파악 및 기회 도출
                            - 원자재 가격, 환율, 금리변동,에 따른 구매 전략의 변화
                        - 환경(Environmental)
                            - 환경 관련 정부, 사회단체, 시민사회의 관심과 규제 동향 파악, 이에 대한 분석 기회를 도출
                            - 탄소 배출 규제 및 거래 시장등장에 따른 원가 절감 및 정보 가시화
                        - 정치(Political)
                            - 주요 정책 방향, 정세, 지정학적 동향 등의 거시적 흐름을 토대로 한 분석 기회
                            - 대북관계 동향에 따른 원자재 구매 거래선의 다변화
                    - 경쟁자 확대 관점
                        - 대체제
                            - 융합적인 경쟁 환경에서 현재 생산하는 제품, 서비스를 온라인으로 제공하는 것에 대한 탐색 및 잠재적 위험을 파악
                            - 오프라인 제공 서비스 → 온라인제공에 대한 탐색 및 잠재적 위협 파악
                        - 경쟁자
                            - 현재 생산중인 제품,서비스의 주요 경쟁자에 대한 동향 파악, 이를 고려하여 분석 기회 도출
                            - 식별된 주요 경쟁사의 제품, 서비스 카탈로그 및 전략 분석을 통한 잠재적 위협 파악
                        - 신규 진입자
                            - 향후 시장에서 파괴적인 역할을 수행할 수 있는 신규 진입자에 대한 동향 파악, 이를 고려한 분석
                            - 새 제품에 대한 클라우드 소싱 서비스인 킥 스타터의 유사 제품을 분석하고 잠재적 위협 파악
                    - 시장 니즈 탐색 관점
                        - 고객
                            - 고객 구매 동향 및 고객 컨텍스트를 더욱 깊게 이해하여 제품, 서비스의 개선에 필요한 분석 기회 도출
                            - 철강 기업의 경우 조선 산업과 자동차 산업의 동향 및 주요 거래선의 경영 현황등을 파악하고 분석 기회 도출
                        - 채널
                            - 영업사원, 직판대리점, 홈페이지 등의 자체적으로 운영하는 채널에 최종고객에게 상품, 서비스를 전달하는 것이 가능한 경로 파악
                            - 인터넷 전문 은행등 온라인 채널의 등장에 따른 변화에 대한 전량 북성 기회 도출
                        - 영향자
                            - 기업 의사 결정에 영향을 미치는 주주, 투자자, 협회 및 기타 이해 관계자의 주요 관심사항에 대해서 파악하고 분석 기회를 탐색
                            - M&A 시장 확대에 따른 유사 업종의 신규기업 인수 기회 탐색
                        - 역량의 재해석
                            - 내부역량
                                - 지적 재산권,기술력 등 기본적인 것과 지식, 기술, 스킬등의 노하우와 인프라적인 유형자산에 대해서 재해석 하고 분석 기회 탐색
                                - 자사 소유 부동산을 활용한 부가가치 창출 기회 발굴
                            - 파트너와 네트워크
                                - 밀접한 관계를 유지하고 있는 관계사와 공급사 등의 역량을 활용해 수행할 수 있는 기능을 파악하고 분석기회 추가적으로 도출
                                - 수출입, 통관, 노하우를 활용한 추가 사업기회 탐색

                - 외부참조 모델기반 문제탐색
                    - 유사 동종 사례 벤치 마킹을 통한 분석기회 발굴은, 제공되는 산업별, 업무 서비스별 테맘 후보 그룹 Pool을 통해 Quick & easy 방식으로 필요한 분석기회가 무엇인지에 아이디어를 얻고 기업에 적용할 분위기 분석 테마 후보 목록은 워크숍 형태의 브레인 스토밍을 통한 빠르게 도출하는 방법이야
                - 분석 유즈 케이스
                    - 빠짐없이 도출한 기회를 구체적인 과제로 만들어 전에 분석 유즈 케이스로 표기하는 것이 필요, 풀어야할 문제에 대한 상세한 설명 및 해당 문제를 해결했을때 발생하는 효과 명시

                        ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%205.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%205.png)

            - 2. 문제 정의(Problem Definition) 단계
                - 문제 탐색 단게에서 식별한 비즈니스 문제에 대해 데이터의 문제로 변환하여 정의
                - 1단계에서 무엇(what)을 어떤 목적(why) 관점 → 2단계 필요한 데이터 및 어떻게(how), 기법을 정읭하기 위한 변환
                - 데이터 분석 문제의 정의 및 요구사항 : 분석을 수행하는 당사자 뿐만 아니라 최종사용자 관점에서 이뤄져야 함
                - 정확하게 분석의 관점으로 문제를 재정의할 필요가 있음
            - 3. 해결방안 탐색(Solution Search) 단계
                - 데이터 분석 문제를 해결하기 위한 다양한 방안
                    - 기존 정보 시스템의 단순한 보완으로 분석이 가능한지 고려
                    - 엑셀 등의 간단한 도구로 분석이 가능한지 고려
                    - 하둡 등 분산 병렬처리를 활용한 빅데이터 분석 도구를 통해 보다 체계적이고 심도 있는 방안 고려
                - 분석역량을 기존에 가지고 있는지 파악, 없을 경우 교육이나 전문인력 채용을 통하여 역량 확보 및 분석 업체 소싱을 통한 과제 해결 방안 사전 검토

                    ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%206.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%206.png)

            - 4. 타당성 검토(Feasibility Study) 단계
                - 경제적 타당성
                    - 비용대비 편익 분석 관점의 접근 필요
                    - 비용항목 - 데이터, 시스템, 인력, 유지보수 → 분석비용
                    - 편익 - 실적적 비용 절감, 추가적 매출과 수익 → 경제적 가치
                - 데이터 및 기술적 타당성
                    - 데이터 분석은 데이터 존재여부, 분석 시스템 환경, 분석 역량이 필요
                    - 분석역량의 경우 실제 프로젝트시 걸림돌이 되는 경우가 많음
                    - 기술적 타당성 분석시 역량 확보 방안을 사전에 수립
                    - 효과적으로 평가하기 위해서 비즈니스 지식과 기술적 지식이 요구됨
                    - 타당성 검토를 통해 도출된 대안을 통해
                        1. 평가 과정을 거쳐 가장 우월한 대안을 선택
                        2. 도출한 데이터 분석 문제 및 선정된 솔루션 방안을 포함
                        3. 분석과제 정의서의 형태로 명시하는 후속작업을 시행
                        4. 프로젝트 계획의 입력물로 활용
        - 상향식 접근 방법 (Bottom Up Approach)
            - 정의
                - 다양한 원천 데이터를 대상으로 분석을 수행하여 가치있는 모든 문제를 도출하는 일련의 과정
                - 기존 통계적 분석에서는 인과관계분석을 위해 가설을 설정하고 검정하여 가설 검정을 실시하였다
                - 하지만 빅데이터 환경에서는 논리적 **인과관계**와 더불어 **상관관계, 연관분석**을 통하여 문제해결에 도움을 받음
                - big data → Analysis → problem
            - 기존 하향식 접근법의 한계를 극복하기 위한 분석 방법론
                - 하향식 접근법의 한계
                    - 문제의 구조가 논리적으로 명확하다 가정
                    - 데이터 분석가, 의사결정자에게 솔루션 도출은 유효, 새로운 문제 탐색에 한계가 있음
                    - 최근 복잡하고 다양한 환경에서 발생하는 문제에는 적합하지 않을 수 있음

                → 하향식 접근법의 한계를 극복하기 위해 **디자인 사고(Design Thinking)** 접근법 사용

                - 통상적으로 사물을 인식하려는 Why를 강조하지만, 결국 우리가 알고 있다고 가정하는것, 문제와 맞지 않는 솔루션일 경우 오류가 발생할 수 있음
                - 미리 답을 내는게 아니라, 사물을 있는 그대로 인식하는 What 관점에서 보아야함
                - 객관적으로 존재하는 데이터 그 자체를 관찰
                - 디자인 사고 프로세스 (감정이입(Empathize) → 정의 → 상상 → 프로토타입 → Test

                    ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%207.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%207.png)

            - 비지도 학습과 지도 학습
                - 비지도 학습
                    - 상향식 접근방식의 데이터 분석은 비지도 학습(Unsupervised Learning) 방법에 의해 수행
                    - 데이터 분석의 목적이 명확히 정의된 형태의 특정 필드 값을 구하는것이 아님
                    - 데이터 자체 결함, 연관성, 유사성 등 데이터의 상태를 표현
                    - 장바구니 분석, 군집분석, 기술통계 및 프로파일링
                - 지도 학습
                    - 명확한 목적 하에 데이터분석을 실시하는 것
                    - 분류, 추측, 예측, 최적화를 통해 분석을 실시하고 지식을 도출
                - 예시

                    ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%208.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%208.png)

            - 시행착오를 통한 문제 해결_프로토타이핑
                - 정의
                    - 프로토타이핑 접근법은 요구사항이나 데이터 정확기 규정짓기 어렵고, 데이터 소스도 파악이 어려운 상황에서 일단 분석을 시도해 보고 그 결과를 확인해 가면서 반복적으로 개선해 나가는 방법
                    - 프로토타이핑 방법론은 비록 완전하지는 못하더라도, 신속하게 해결책이나 모형을 제시
                    - 문제를 좀 더 명확하게 인식 가능
                    - 필요한 데이터를 식별, 구체화 할 수 있도록 도와주는 상향식 접근 방식
                    - 프로토타이핑 프로세스 : 가설의 생성 → 디자인에 대한 실험 → 실제 환경에서의 테스트 → 테스트 결과에서의 통찰 도출 및 가설 확인
                - 필요성
                    - 문제에 대한 인식 수준 : 문제 정의가 불명확할 경우, 프로토타입을 이용하여 문제 이해 및 구체화
                    - 필요 데이터 존재 여부의 불확실성 : 필요한 데이터 집합이 존재 X 경우, 사용자와 분석가간의 반복적이고 순환적인 협의 과정이 필요, 대체 불가능한 데이터가 존재하는지 확인하여 리스트 사전 방지
                    - 데이터 사용 목적의 가변성 : 기존 데이터의 정의를 재검토하여 데이터의 사용 목적과 범위를 확대 할 수 있음
        - 분석과제 정의
            - 분석과제 정의서 발행
            - 필요한 소스데이터, 분석방법, 데이터 입수 및 분석 난이도, 분석 수행주기, 분석결과에 대한 검증 오너쉽, 상세 분석 과정등을 정의
            - 데이터 소스는 내,외부의 비구조적인 데이터와 소셜 미디어 및 오픈 데이터까지 범위를 확장하여 고려
            - 분석과제 정의서 예시

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%209.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%209.png)

        - 디자인 사고(Design thinking) : 상향식 접근 방식의 발산 단계와 하향식 접근 방식인 수렴단계를 반복적으로 수행하여 분석 가치를 높일 수 있는 최적의 의사결정 방식

            ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2010.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2010.png)

- #### 4절 분석 프로젝트 관리 방안
    - 1. 분석과제 관리를 위한 5가지 주요 영역
        - Data Size
            - 분석하고자 하는데이터 사이즈를 고려한 관리방안 도출
            - 하둡환경과 기존 정형 데이터 베이스에 있는 시간당 생성되는 데이터를 분석할때 관리방식은 차이가 있음
        - Data Complexity
            - BI 프로젝트처럼 정형데이터가 분석 마트로 구성되어 있는 상태가 아닌 텍스트, 오디오, 비디오등 비정형 데이터 및 다양한 시스템에 산재되어 있는 데이터를 통합해서 분석 프로젝트 진행할 때는 잘 적용될 수 있는 분석 모델 선정등 사전 조사가 필요함
        - Speed
            - 분석보델의 성능 및 속도를 고려한 개발 및 테스트가 이뤄져야함
        - Analytic & Complexity
            - 정확도와 분석도는 Trade off 관계가 존재
            - 복잡할 수록 정확도 올라가지만 해석이 어려워짐
            - 이에 대한 기준점을 사전에 정의해야함
            - 해석이 가능하면서 정확도를 올릴수 있는 최적의 모델을 찾는 방안을 사전에 모색
        - Accuracy & precision
            - Accuracy : 모델과 실제 값 사이 차이가 적다는 정확도_ 활용도 측면
            - Precision : 모델을 지속적으로 반복했을때의 편차 수준으로썽 일관적 정확도 _ 안정성 측면
            - Accuray와 precision 은 Trade off 관계일때가 많다
    - 2. 분석 프로젝트의 특성
        - 분석가의 목표 : 분석의 정확도 높이기, 도출된 분석 과제 구현, 전반적인 프로젝트관리
        - 분석가 입장 : 데이터영역과 비즈니스 영역의 중간에서 분석 모델을 통한 조율을 수행하는 조정자 역할
        - 분석 프로젝트는 도출된 결과의 재해석을 통해 지속적인 반복 및 정교화 작업이 필요
        - 프로토타이핑 방식의 어자일(Agile) 프로젝트 방식 고려 필요
        - 데이터 영역과 비즈니스 영역에 대한 이해 뿐만 아니라 지속적인 반복이 요구되는 분석 프로세스 특성을 이해한 관리방안 필요
        - 지속적인 개선 및 변경을 염두에 두고 기간 내 가능한 최선의 결과를 도출 할 수 있도록 해야함

    - 3. 분석 프로젝트 관리 방안
        - 범위
            - 데이터의 형태와 양
            - 적용되는 모델의 알고리즘에 따라 변경됨
            - 최종결과물이 보고서인지 시스템인지에 따라 차이가 큼, 사전에 충분한 검토 필요
        - 시간
            - 지속적으로 분석 프로세스가 반복되어 많은 시간이 소요될 가능성이 있음
            - 분석결과에 대한 품질이 보증된다는 전제로 Time Boxing 기법으로 일정 관리 필요
        - 원가
            - 외부데이터 사용으로 고가 비용이 지출될 수 있음, 사전에 확인 필요
            - 오픈 소스 외에 사용버전의 도구가 필요할 수 있음
        - 품질
            - 품질목표를 사전에 수립, 확정해야함
            - 품질 통제와 품질 보등으로 나눠서 수행되어야함
        - 통합
            - 프로젝트 관리 프로세스들이 통합적으로 운영될 수 있도록 관리
        - 조달
            - 프로젝트 목적성에 맞는 외부 소싱을 적절하게 운영할 필요가 있음
            - PoC(Proof of Concept)형태의 프로젝트는 인프라 구매가 아닌 클라우드 등의 당야한 방안을 검토할 필요가 있음
        - 자원
            - 고급 분석 및 빅데이터 아키텍쳐링을 수행할 수 잇는 인력 공급이 부족하므로, 프로젝트 수행 전 전무가 확보에 대한 검토
        - 리스크
            - 분석에 대한 데이터 미확보로 분석 프로젝트 진행이 어려울 수 있으므로 관련 위험을 식별, 대응방안 사전 수립
            - 데이터 및 분석 알고리즘의 한계로 품질 목표를 달성하기 어려울 수 있어, 그에 따른 방안 수립
        - 의사소통
            - 전문성이 요구되는 데이터 분석의 결과를 모든 프로젝트 이해 관계자가 공유할 수 있도록 해야함
            - 프로젝트의 원활한 진행을 위한 다양한 의사소통체계 마련 필요
        - 이해관계자
            - 데이터 분석 프로젝트는 데이터 전문가, 비즈니스 전문가, 분석 전문가, 시스템 전문가 등 다양한 전문가가 참여하므로 이해관게자 식별 및 관리 필요

### 2장 분석 마스터 플랜

- #### 1절 마스터 플랜 수립 프레임 워크
    - 1.  분석 마스터 플랜 수립 프레임 워크
        - 1. 마스터 플랜 수립개요
            - 전략적 중요도, 비즈니스 성과 및 ROI, 분석 과제의 실행 용이성 등 다양한 기준을 고려해 적용 우선순위를 설정
            - 업무 내재화 적용 수준, 분석데이터 적용 수준, 기술적용 수준 등 분석 적용 범위 및 방식에 대해서 종합적으로 고려 로드맵 수립
            - 우선순위 고려 요소 : ① 전략적 중요도, ② 비즈니스 성과 및 ROI, ③ 실행 용이성
            - 적용범위 /방식 고려요소 : ① 업무 내재화 적용 수준, ② 분석데이터 적용 수준, ③ 기술 적용 수준

            ※기업 및 공공기관에서 정보 전략 계획 ISP 수행

            → ISP (Information Strategy Planning) : 정보기술 또는 정보시스템을 전략적으로 활용하기 위해 조직 내,외부 환경을 분석하여 문제점 도출, 사용자 요구사항 분석, 시스템 구축 우선순위를 결정

        - 2. 수행 과제 도출 및 우선순위 평가
            - 1. 우선순위 평가 방법 및 절차
                - 정의된 데이터 과제에 대한 실행 순서
                - 우선순위 평가 기준에 따라 평가한 뒤 선,후행 관계를 고려 적용순위 조정
                - 분석과제 도출 → 우선순위 평가 → 우선순위 정련
            - 2. 일반적인 IT 프로젝트의 우선순위 평가 예시(ISP)
                - 전략적 중요도
                    - 전략적 필요성 : 목표 및 본원적 업무에 연관도가 높음, 이슈 미해결시 위험 및 손실에 대한 정도
                    - 시급성 : 사용자 요구사항, 업무능률 향상을 위해 시급히 수행되어야하는 정도, 향후 경쟁 우위 확보를 위한 중요성 정도
                - 실행 용이성
                    - 투자 용이성 :  기간 및 인력 투입 용이성 정도, 비용 및 투자 예산 확보 가능정도
                    - 기술 용이성 : 적용 기술이 안정성 검증 검토, 응용 시스템, H/W 유지보스 용이성 정도, 개발 스킬 성숙도 및 신기술 적용성 정도
            - 3. ROI 관점에서 빅데이터의 핵심 특징
                - 투자 비용요소
                    - 크기(Volume) : 데이터 규모 및 양을 의미, 대용량 데이터 저장 처리를 위해서는 새로운 투자 필요
                    - 다양성(Variety) : 다양한 종류와 형태를 가진 데이터 입수시 투자 필요
                    - 속도(Velocity) : 데이터 생성 속도 및 처리 속도를 빠르게 가공, 분석하는 기술 요구
                - 비즈니스 효과
                    - 가치(Value) : 분석결과를 활용, 실질적인 실행을 통행 얻게되는 비즈니스 효과 측면 요소로, 추구하거나 달성하고자하는 목표 가치를 의미
            - 4. 데이터 분석과제 추진시 고려해야 하는 우선순위 평가 기준
                - 시급성
                    - 전략적 중요도와 목표가치가 부합하는지에 따른 시급성이 중요한 기준
                    - 전략적 중요도가 핵심
                    - 현재 관점에서 전략적 가치를 둘것인지 미래의 중장기적 관점에 전략적 가치를 둘것인지 고려
                    - KPI 함께 고려
                - 난이도
                    - 현 시점에서 과제를 추진하는것이 적용 비용측면, 범위측면에서 쉬운것인지 어려운 것인지 판단하는 기준
                    - 데이터 분석의 적합성 여부를 확인
            - 5. 포트폴리오 사분면 분석을 통한 과제 우선순위 선정
                - 개요도

                    ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2011.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2011.png)

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2012.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2012.png)

                - 3사분면이 가장 우선적인 분석 과제
                - 경영에 미치는 미래 영향도가 크고 적용이 어려운 2사분면이 마지막
                - 시급성 우선으로 가면 3 → 4 → 2 사분면 순
                - 난이도 우선으로 가면 3 → 1 → 2 사분면 순
        - 3. 이행계획 수립
            - 1. 로드맵 수립
                - 포트폴리오 사분면 분석을 통해 1차적 우선순위 결정
                - 분석과제별 적용범위 및 방식을 고려하여 최종적인 실행 우선순위를 결정한 후 단계적 구현 로드맵을 수립
                - 단계별 목표 정의
                - 추진 과제별 선,후행 관계를 고려하여 단계별 추진 내용을 정렬
                - **데이터 분석 체계 도입**
                    - 비즈니스 Pain Point 식별, 해결 관점해서 분석 기회 발굴, 과제선정 및 마스터 플랜
                    - 분석기회 발굴 및 분석과제 정의, 마스터 플랜 수립
                - **데이터 분석 유효성 검증**
                    - 분석과제에 대한 Pilot수행,비즈니스적인 유효성과 타당성 검증, 기술적인 실현 가능성 검증
                    - 분석알고리즘 및 아키텍쳐 설계, 분석과제 Pilot 수행
                - **데이터 분석 확산 및 고도화**
                    - Pilot을 통해 검증된 분석과제 업무 프로세스에 내재화, 검증결과 확사하는 관점 활용시스템 구축 및 유관 시스템 고도화
                    - 업무 프로세스 내재화를 위한 Process Innovation, 변화관리, 빅데이터 분석-활용시스템 구축, 유관시스템 고도화

                ※ 폭포수 모델(water fall) : 소프트웨어 요구사항 기술, 소프트웨어 설계, 소프트웨어 구현, 통합시험과 디버깅 설치 소프트웨어 유지보수 순서로 이뤄짐

            - 2. 세부 이행계획 수립
                - 데이터 분석 체계에 폭포수 방식도 있으나 반복적인 정련 과정을 통하여 프로젝트 완성도를 높이는 방식이 주로 사용됨
                - 모든 단계를 반복하진 않음
                - 데이터 수집 및 확보와 분석 데이터 준비 하는 단계를 순차적으로 진행 모델링 하는 단계는 반복적으로 수행하는 혼합형을 적용
                - 세부적인 일정게획도 수립해야한다
                - 정제된 분석 모델 모식도

                    ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2013.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2013.png)

- #### 2절 분석 거버넌스 체계 수립
    - 1. 거버넌스 체계 : 데이터 분석을 기업의 문화로 정착, 데이터 분석 업무를 지속적으로 고도화
        - 5가지 구성요소
            - 분석 기획 및 관리를 수행하는 조직(Organization)
            - 과제 기획 및 운영 프로세스(process)
            - 분석 관련 시스템(System)
            - 데이터(data)
            - 분석 관련 교육 및 마인드 육성 체계(Human Resource)
    - 2. 데이터 분석 수준진단
        - 데이터 분석 도입을 위한 명확한 분석 수준 점검 필요
        - 수준 진단 목표 2가지
            - 정의 : 수준 진단을 통한 목표 수준 정의, 어떤 관점에서 보완이 필요한지 개선 방안 도출
            - **분석 준비도**
                - 업무 파악 : 발생한 사실 분석 업, 예측 분석업무, 시뮬레이션 분석 업무, 최적화 분석업무, 분석업무 정기적 개선
                - 인력 및 조직 : 분석 전문가 직무 존재, 분석 전문가 교육 훈련 프로그램, 관리자들의 기본적인 분석 능력, 전사 분석 업무 총괄조직 존재, 경영진의 분석업무 이해 능력
                - 분석기법 : 업무별 적합한 분석 기법 사용, 도입 방법론, 분석기법 라이브러리, 효과성 평가, 정기적 개선
                - 데이터 : 데이터 충분성, 신뢰성, 정시성, 비구조적 데이터 관리, 외부데이터 활용 체계, 기준데이터 관리(MDM)
                - 문화 : 사실에 의거한 의사결정, 관리자 데이터 중시정도, 회의에서 데이터 활용 상황, 경영진 직관vs 데이터 기반의 의사결정, 데이터 공유 협업문화
                - 인프라 : 운영 시스템 데이터 통합, EAI, ETL 등 데이터 유통체계, 서버 및 스토리지, 빅데이터 분석 환경, 통계 분석 환경, 비쥬얼 분석 환경
            - **분석 성숙도**
                - 조직의 성숙도 평가 도구(CMMI 모델, Capability Maturity Model Integration)
                - 성숙도 진단 분류 : 비즈니스 부분, 조직 역량 부분, IT 부분
                - 성숙도 수준 구분 : 도입 → 활용 → 확산 → 최적화
                    - 도입단계
                        - 설명 : 분석을 시작하여 환경과 시스템 구축
                        - 비즈니스 : 실적 분석 및 통계, 정기 보고 수행, 운영 데이터 기반
                        - 조직역량구분 : 일부부서수행, 담당자 역량에 의존
                        - IT부분 : 데이터 웨어하우스, 데이터 마트, ETL, EAI, OLAP
                    - 활용단계
                        - 설명 : 실제 업무에 적용
                        - 비즈니스 : 미래 결과 예측, 시뮬레이션, 운영 데이터 기반
                        - 조직역량부분 : 전문 담당 부서에서 수행, 분석기법 도입, 관리자가 분석 수행
                        - IT 부분 : 실시간 대시보드, 통계 분석 환경
                    - 확산단계
                        - 설명 : 전사 차원에서 분석을 관리하고 공유
                        - 비즈니스 : 전사 성과 실시간 분석, 프로세스 혁신 3.0, 분석 규칙 관리, 이벤트 관리
                        - 조직역량부분 : 전사 모든 부서 수행, 분석 COE 조직 운영, 데이터 사이어티스트 확보
                        - IT 부분 : 빅데이터 관리 환경, 시뮬레이션 최적화, 비주얼 분석, 분석 전용 서버
                    - 최적화단계
                        - 설명 : 분석을 진화시켜서 혁신 및 성과 향상에 기여
                        - 비즈니스 : 외부 환경 분석 활용, 실시간 분석, 비즈니스 모델 진화
                        - 조직역량부분 : 데이터 사이언스 그룹, 경영진 분석 활용, 전략연계
                        - IT 부분 : 분석 협업환경, 분석 Sandbox, 프로세스 내재화, 빅데이터 분석
            - 분석 수준 진단 결과
                - 기업의 현재 분석 수준 객관적 파악
                - 경쟁사의 분석 수준과 비교하여 분석 경쟁력 확보 및 강화
                - 사분면 분석(정착형, 확산형, 준비형, 도입형)

                    ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2014.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2014.png)

    - 3. 분석 지원 인프라 방안 수립
        - 플랫폼 : 장기적이고 안정적이고 확장성이 있음
            - 분석 서비스를 위한 응용프로그램이 실행될 수 잇는 기초를 이루는 컴퓨터 시스템
            - 프로그래밍 환경과 실행 및 서비스 환경을 제공하는 역할
            - 시스템이 아닌 서비스를 추가적으로 제공하는 방식으로 확장성 높일 수 있음
            - 플랫폼 구조

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2015.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2015.png)

    - 4. 데이터 거버넌스 체계 수립
        - 개요
            - 전사 차원 모든 데이터에 대하여 표준화된 관리체계 수립
            - 운영을 위한 프레임워크 및 저장소 구축
            - 마스터 데이터, 메타데이터, 데이터 사전은 데이터 거버넌스의 중요 관리 대상
        - 구성요소
            - 원칙 : 데이터를 유지 관리하기 위한 지침과 가이드, 보안, 품질기준, 변경관리
            - 조직 : 조직의 역할과 책임, 데이터 관리자, 데이터베이스 관리자, 데이터 아키텍트
            - 프로세스 : 데이터 관리를 위한 활동과 체계, 작업절차, 모니터링 활동, 측정활동
        - 체계
            - 데이터 표준화
                - 데이터 표준용어설정, 명명규칙, 메타데이터 구축, 데이터 사전
                - 사전간 상호 검증이 가능하도록 점검 프로세스 포함
                - 명명 규칙은 반드시 언어별로 작성되어 매핑 상태를 유지
            - 데이터 관리체계
                - 데이터 정합성 및 활용의 효율성을 위한 메타 데이터와 데이터 사전의 관리 원칙 수립
                - 상세한 프로세스를 만들고 관리 운영, 상세하게 준비
                - 관리방안 준비안하면 데이터 가용성 및 관리비용 증대 문제 직면
            - 데이터 저장소 관리
                - 전사차원 저장소 구성
                - 워크플로우 및 관리용 응용 소프트웨어 지원
                - 데이터 구조변경에 따른 사전 영향평가도 수행
            - 표준화 활동
                - 주기적으로 모니터링
                - 변화관리 및 주기적인 교육
                - 데이터 표준화 개선활동을 통해 실용성 향상
    - 5. 데이터 조직 및 인력 방안 수립
        - 분석조직
            - 목표 : 비즈니스 질문과 이에 부합하는 가치를 찾고, 최적화
            - 역할 : 분석업무 발굴, Insight 전파, Action화
            - 구성 : 기초 통계학 및 분석방법에 대한 지식과 분석 경험을 가지고 있는 인력으로 전사 또는 부서 내 조직으로 구성하여 운영
        - 분석 조직 및 인력 구성시 고려사항
            - 조직 구조
                - 비즈니스 질문을 선제적으로 찾아 낼 수 있는 구조인가
                - 유기적인 협조와 지원이 원활한가
                - 전사 및 단위부서가 필요시 접촉하며 지원할 수 있는 구조인가
            - 인력 구성
                - 비즈니스와 IT 전문가 조합으로 구성되었는가
                - 어떤 경험과 어떤 스킬을 갖춘 사람으로 구성해야하는가
                - 통계적 기법 및 분석 모델링 전문 인력을 별도로 구성해야하는가
                - 규모는 어느정도인가
            - 분석을 위한 3가지 조직구조(집중구조, 기능 구조, 분산 구조)

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2016.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2016.png)

            - 분석 조직의 인력 구성

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2017.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2017.png)

    - 6. 분석 과제 관리 프로세스 수립
        - 과제 관리 프로세스
            - 과제 발굴 : 도출한 분석 아이디어를 발굴하고, 과제화 하여 분석 과제 Pool로 관리 분석 프로젝트를 선정하는 작업 수행
            - 과제 수행 : 분석을 수행할 팀을 구성하고, 지속적인 모니터링과 과제결과를 공유하고 개선하는 절차를 수행
            - 분석 과제 관리 프로세스 모식도

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2018.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2018.png)

    - 7. 분석 교육 및 변화 관리
        - 데이터 분석 방법 및 분석적 사고 교육

            ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2019.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2019.png)

        - 분석 교육의 목표

            ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2020.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2020.png)

## Part 3 데이터 분석

### 1장 데이터 분석 개요

- #### 1절 데이터 분석 기법의 이해
    - 1. **데이터 처리**
        - 데이터 분석 통계지식과 복잡한 가정이 상대적으로 적은 실용적인 분야
        - 대기업 : 데이터 웨어하우스(DW), 데이터 마트(DM) 데이터 사용
        - But. 신규 시스템이나 DW에 미포함된 자료의 경우 기존 운영 시스템(Legacy), 스테이징 영역(Staging area), ODS(Operational Data Store)의 Data와 DW data를 결합하여 사용이 가능함
        - 운영시스템에 직접 접근하는 방식은 위험, Staging area는 임시데이터이기 때문에, 클린징 영역인 ODS에서 데이터 전처리하여 DW, DM와 결합이 이상적

        **최종 데이터 구조로 가공**

        1. 데이터 마이닝 분류 : 분류값과 입력변수들을 연관시켜 인구통계, 요약변수, 파생변수 등을 산출
        2. 정형화된 패턴 처리 : 비정형/소셜 data의 경우 정형화된 패턴으로 처리

            ▷ 비정형 데이터 : DBMS에 저장됐다가 텍스트 마이닝하여 데이터 마트로 통합됨

            ▷ 관계형 데이터 : DBMS에 저장되어 사회 신경만 분석을 거쳐 데이터 마트와 통합

    - 2. **시각화(시각화 그래프)**
        - 낮은 수준의 분석이지만 적절하게 사용할 경우 복잡한 분석보다 효율적
        - 빅데이터 분석, 탐색적 분석에서 시각화 필수(SNA 분석(사회연결망 분석)에서 자주 활용)
    - 3. **공간 분석(GIS_Geographic Information System)**
        - 공간분석(Spatial Analysis) : 공간적 차원과 관련된 속성들을 시각화하는 분석→ 지도 위에 속성을 생성하여 크기,모양,선굵기등으로 구분하여 Insight를 얻음
    - 4. **탐색적 자료 분석(EDA_Exploratory Data Analysis)**
        - 탐색적 분석 : 다양한 차원의 값을 여러 방식으로 조합 → 특이점, 의미있는 사실 도출, 분석의 최종 목적을 달성해 가는 과정
        - 
        - 데이터의 특징과 내재하는 구조적 관계를 알아내기 위한 기법의 통칭
        - 프린스톤 대학 튜키교수 1977년 저서 발표로 EDA 등장
        - 탐색적 분석의 효율 예(모형 개발 프로세스(KDD, CRISP-DM)
            - 데이터이해 단계 : 변수의 분포와 특성 파악
            - 변수생성 단계 : 분석 목정에 맞는 주요한 요약 및 파생변수 생성
            - 변수선택 단계 : 목적변수에 의미있는 후보 변수 선택

        EDA의 4가지 주제 

        - 1)저항성의 강조, 2)잔차 계산, 3)자료변수의 재표현, 4)그래프를 통한 현시성
    - 5. **통계 분석**
        - 일정한 체계에 따라 숫자, 표, 그림 형태로 한눈에 알아보기 쉽게 나타내어 분석
        - 기술 통계(descriptive statistics) : 추출된 표본이 가진 정보를 쉽게 파악하기 위한 형태로 표현
        - 추측(추론)통계(inferential statistics) : 표본의 표본통계량으로부터 모집단 모수를 통계적으로 추론
        - 활용 : 정책수립, 농업, 의학, 경영, 스포츠
    - 6. **데이터 마이닝**
        - 고급 데이터 분석법
        - 평가 기준 : 정확도(Accuracy), 리프트(lift), 디텍트 레이트(Detect Rate), 정밀도
        - 대용량의 자료로 요약, 미래 예측을 목표로 자료 내 관계, 패턴, 규칙 등을 탐색/모형화를 통한 Insight

        방법론 :

        - 데이터 베이스에서의 지식 탐색 : 데이터 하우스에서 데이터마트를 생성, 각 데이터들의 속성을 사전 분석하여 지식을 얻음
        - 기계 학습(machine learning) : 인공지능의 한 분야, 컴퓨터 학습(인공신경망, 의사결정나무, 클러스터링, 베이지안 분류 SVM등 존재)
        - 패턴 인식(Pattern recognition) : 원자료를 를 이용하여, 사전지식과 패턴에서 추출된 통계 정보를 기반으로 자료, 패턴을 분류(장바구니분석, 연관규칙 등)

        활용 분야

        - 데이터베이스 마케팅(고객 행동정보 Data → 목표 마케팅, 고객 세분화, 장바구니 분석, 추천 시스템)
        - 신용평가 및 조기경보시스템(금융권 → 신용카드 발급, 보험, 대출 발생시 업무 적용)
        - 생물정보학(세포 유전자 분석 → 질병진단, 치료법, 신약 개발)
        - 텍스트마이닝(전자우편, SNS등 디지털 텍스트 data →고객성향분석, 감성분석, 사회관계망분석 등)

        모델링

        - 통계적 가설이나 유의성에 집착 X (통계적 모델링이 아님)
        - 분석데이터는 학습, 테스트 데이터로 상황에 맞게 비율을 정하여 실시
        - 성능에 집착X (주목적인 실무 적용에 반하여 시간 낭비가 될 수 있음)_예상 성능 만족시 중단

### 3장 데이터 마트

- #### 1절 데이터 변경 및 요약
    - 1. R reshape를 이용한 데이터 마트 개발
        - **데이터 마트(DM)** : 데이터 웨어하우스(DW)와 사용자 사이 중간층에 위치, 하나의 주제나 부서 중심의 DW
            - 대부분 DW에서 복제되지만, 자체적으로도 수집가능
            - 관계형 DB, 다차원 DB를 이용하여 구축
                - 데이터 마트 구성 자료

                    ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2021.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2021.png)

            - 고객 데이터 마트 구축 - CRM(Customer Relationship Management)관련 업무 중 핵심
            - 데이터 마트의 구축 방식에 따라 분석효과 차이 큼(최신 분석기술 사용 시, 동일한 데이터 셋에서는 큰 차이가 없음)
        - **요약변수** : 수집된 정보를 분석에 맞게 종합한 변수
            - DM 가장 기본 변수
            - 총 구매 금액, 금액, 횟수, 구매여부 등
            - 재활용성 높음
            - 합계, 횟수와 같이 간단한 구조로 자동화 프로그램 구축 가능
            - 기준값 의미 해석이 애매할 수 있어, 연속형 변수를 그룹핑해 사용
        - **파생변수** : 특정 조건이나 함수에 의해 만들어져 의미가 부여된 변수
            - 논리적으로 타당성 필요(매우 주관적이 될 수 있음, 특정 상황에서만 유의미하지 않게 대표성이 필요)
            - 세분화, 고객행동예측, 캠페인 반응 예측
        - Reshape의 활용
            - melt() : 원데이터 형태로 만드는 함수, cast() : 요약 형태로 만드는 함수
    - 2 @. sqldf를 이용한 데이터 분석
        - sqldf : R에서 sql 명령어를 가능하게 해주는 패키지
        - SAS에서의 proc sql와 같은 역할을 하는 패키지
    - 3. plyr을 이용한 데이터 분석
        - plyr : apply 함수에 기반해 데이터와 출력변수를 동시에 배열로 치환하여 처리하는 패키지
        - split-apply-combine : 데이터를 분리하고 처리 후 다시 결합하는 등 필수적인 데이터 처리 기능을 제공
    - 4. 데이터 테이블
        - data.table 패키지 : R에서 가장 많이 사용하는 데이터 핸들링 패키지
        - 큰 데이터를 탐색, 연산, 병합에 유용함
        - data.frame보다 월등히 빠른 속도(그루핑, ordering, 짧은문장 지원)

- #### 2절 데이터 가공
    - 1. Data Exploration : 데이터 변수들의 상태를 파악
        - head(),Tail(),summary()
    - 2. 변수 중요도 : 사용된 변수의 중요도를 확인(변수 선택법과 유사)
        - klaR 패키지 : 특정 변수에 대한 클래스, 분류, 에러율 계산 및 그래픽화
        - greedy.wilks() : 세분화를 위한 stepwise forward 변수 선택을 위한 패키지, wilk's lambda로 종속변수들을 영향력/중요도 별로 정리

            ㄴ wilk's lambda = 집단 내 분산/총 분산

    - 3. **변수의 구간화** : 연속형 변수를 목적에 맞게 구간화**_binning**과 **의사결정** 나무에 자주 활용됨
        - 보통 5개 구간, 7개 구간 이상은 잘 만들지 않음

            ex) 신용평가모형, 고객 세분화 : 스코어링 방식(각 변수들 구간화 → 구간별 점수 적용)

        - 구간화 방법

            ㄴ binning : 신용평가모형 개발 시 연속형 변수(부채비율)를 범주형으로 구간화 하는데 활용
            50개의 깡통(bin)에 담고, 각 깡통 기준으로 병합하며 5~7개의 구간으로 만듦

            ㄴ 의사결정나무 : 세분화/예측에 활용, 연속변수가 반복적으로 선택될 경우 각 분리 기준값으로 구간화

- #### 3절 기초 분석 및 데이터 관리

    1. 데이터 EDA(탐색적 자료 분석) : 데이터 분석 전에 데이터 특징 파악 및 다각도 접근, Summary() 활용 기초 통계량 확인

    2. 결측값 인식 : NA, 999999999, ' '(공백), Unknown, Not answer 결측값 처리(많은 시간 투자시 비효율적, 결측값 처리는 전체 작업속도에 영향력 큼)

    - 3. 결측값 처리 방법
        1. 단순 대치법(Single Imputation)

            1) Completes Analysis  : 결측값 레코드 삭제

            2) 평균대치법(Mean Imputation) : 결측값을 데이터 평균으로 대치

            ㄴ 비조건부 평균 대치법 : 관측데이터의 평균으로 대치

            ㄴ 조건부 평균 대치법(regression imputation) : 회귀분석을 활용한 대치

            3) 단순 확률대치법(Single Stochastic Imputation) : 평균대치법에서 추정량 표준오차의 과소 추정 문제를 보완(hot-desk 방법, nearest neighbor 방법)

        2. 다중 대치법(Multiple Imputation) : 단순 대치법을 M번하여 M개의 가상적 완전 자료를 만듬

            ㄴ 단계 : (1) 대치(Imputation Step), (2) 분석(Analysis Step), (3) 결합(Combination Step) 

            ex) Amelia-time series cross sectional data set(여러 국가에서 매년 측정된 자료)에서 Bootstrapping based algorithm을 활용한 다중 대치법

    - 4. R에서 결측값 처리
        - 관련함수

            complete.cases() : 데이터 내 레코드에 결측값이 있으면 True, False로 반환

            is.na() : 결측값을 NA로 인식하여 결측값이 있으면 True, 없으면 False 로 반환

            centrallmputation(DMwR 패키지) : 결측값(NA)을 가운데 값으로 대치(숫자 - 중위수, 요인(Factor) - 최빈값)

            knnimputation(DMwR 패키지) : NA값을 k최근 이웃분류 알고리즘을 사용하여 대치, k개 주변 이웃까지의 거리를 고려하여 가중 평균한 값

            amelia(amelia패키지) : 다중 대치법에서 활용, randomForest패키지의 rflmpute()함수 활용하여 NA결측값 대치 후 RF 알고리즘 적용(RF 결측값 있을시 에러)

    - 5. 이상값(Outlier) 인식과 처리
        1. 이상값 : 잘못 입력, 분석 목적 미부합, 의도하지 않은 현상이지만 분석 필요, 의도된 이상값(Fraud, 불량)

            ㄴ 무조건적인 제거가 아닌 상황에 따른 판단 필요

        2. **이상값의 인식 방법**
            - ESD (Extreme Studentized Deviation) : 평균으로부터 3 표준편차 떨어진 값(0.15%)
            - 기하평균-2.5표준편차 < data < 기하평균+2.5표준편차
            - 사분위수 이용 (Q1 - 1.5(Q3 - Q1) < data < Q3 + 1.5(Q3 - Q1))
        3. 극단값 절단(Trimming) 방법
            - 기하평균을 이용 : geo_mean
            - 하/상단 % 이용 : 10% 절단(상하위 5% 데이터 제거)
        4. 극단값 조정(winsorizing) 방법
            - 벗어난 상한, 하한값을 상한 하한값으로 바꿔 활용

### 4장 통계 분석

- #### 1절 통계분석의 이해
    - 1. 통계 : 특정 집단 대상의 조사/실험에 대한 결과가 요약된 형태의 표현

        ㄴ 조사/실험으로 데이터 확보, 조사 대상에 따라 총조사(census)와 표본조사로 구분

    - 2. 통계자료의 획득 방법 : 전수조사, 표본조사
        1. 총 조사/전수조사(census) : 많은 비용과 시간이 소요되어 특별한 경우에만 사용(인구 주택 총 조사)
        2. 표본조사 : 대부분의 설문조사 방법, 모집단에서 샘플을 추출하여 진행하는 조사

            ㄴ 모집단(population) : 대상 집단 전체, 원소(element) :  모집단 구성 개체, 표본(sample) : 모집단의 일부 원소, 모수(parameter) : 모집단에 대한 정보

            ㄴ **표본오차** : 평균의 표본 표준편차, 해석에 의한 오차, **비표본오차** : 표본오차를 제외한 오차,(부주의, 실수, 알수 없는 원인

            ㄴ 중심극한정리 : 표본수가 커질 수록 평균값을 중심으로 한 정규분포에 가까워진다

            ㄴ 표본편의 : 표본추출 과정중 특정 대상이 다른 대상에 비해 우선적으로 추출될때 생기는 오차, 확률화(randomization)에 의해 최소화

        3. 표본 추출 방법 : 모집단을 대표할 수 있도록 선정 필요, 추출 방법에 따라 결과 차이가 큼
            1. 단순 랜덤 추출법(simple random sampling) : 샘플 번호 부여 임의의 n개 추출(비복원, 복원 추출(추출한 element를 다시 집어넣어 추출하는 경우)
            2. 계통추출법(systematic sampling) : 샘플 번호 부여 k개씩 n개의 구간으로 나눠 임의 위치에서 매 k번째 항목을 추출하는 방법
            3. 집략추출법(Cluster random sampling) : 군집 구분 후 군집별로 단순랜덤 추출법 수행 후, 전수 사용 or 샘플링(지역표본추출, 다단계표본추출)
            4. 층화추출법(stratified random sampling) : 유사한 원소를 층(stratum)으로 층에서 랜덤 추출 하는 방식(비례층화추출법, 불비례층화추출법)

            예외) 기브스 표집(Gibbs sampling) : 두개 이상 확률분포에서 표본을 추출

        4. 측정(measurement) : 표본조사, 실험시 주어진 목적에 맞게 관측해 자료 수집
            - 측정 방법 : 명목척도/순서척도/구간척도(등간척도)/비율척도

                ㄴ **명목척도** : 측정대상이 어느 집단에 속하는지 분류 (**성별, 출생지 구분**) → 질적척도(범주형자료, 숫자들의 크기 차이가 계산되지 않음)

                ㄴ **순서척도** : 서열관계를 관측하는 척도 (**만족도, 선호도, 학년, 신용등급**) → 질적척도(범주형자료, 숫자들의 크기 차이가 계산되지 않음)

                ㄴ **구간척도(등간척도)** : 속성의 양을 측정, 구간의 간격이 의미가 있는 자료 (**온도, 지수**) → 양적척도(절대적 크기 측정 불가, 사칙연산은 가능, 비율은 불가능함)

                ㄴ **비율척도** : 간격에 대한 비율이 의미를 가짐, 절대적인 기준 0이 존재, 사칙연산 가능 (**무게, 나이, 시간, 거리**) → 양적척도(수치형자료, 숫자들의 크기 차이를 계산 할 수 있음

                ※ 서열척도 : 숫자의 크기를 위미 있게 활용 가능(명목척도와 다름)(ex. 1등이 2등보다 성적이 높다

    - 3.  통계분석 : 특정집단, 불확실한 현상 관련 자료/정보 수집 후 통계분석 방법을 이용해 의사 결정하는 과정
        1. 기술통계(descriptive statistic) : 주어진 자료들의 여러 특성을 수량화하여 객관적인 데이터 나타냄(판단/예측 등 주관이 섞일 수 있는 과정 배제)
        2. **통계론적 추론**(추측통계, inference statistic) : Sample을 통해 모집단 추정(모집단에 대한 일반적 결론 유도, 본질적으로 불확실성 있음)

            ㄴ 모수측정(평균,분산 모집단 추론)→ 가설검정(옳고/그름, 채택여부) → 예측(불확실성 해결, 효율적 의사결정, 회귀분석, 시계열p분석)

    - 4. 확률 및 확률분포
        1. 확률 :P(E) = n(E)/n(Ω) (E : 사건(event), Ω : 표본공간(실험시 모든 결과들의 집합), E는 Ω의 부분집합)
            - 확률 변수 : 특정값이 나타날 가능성이 확률적으로 주어지는 변수
                - 정의역(domain)이 표본공간, 치역(range)이 실수값(0<y<1)인 함수
                - 이산형 확률변수(discrete random variable), 연속형 확률변수(continuous random variable)
                - 기대값 :

                    ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2022.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2022.png)

                - 모분산 : 2차 중심적률 (2차적률 - 1차적률^2)
                - 덧셈정리(배반, 배반X), 곱셈정리(독립 AnB=A*B), 조건부(독립)
        2. 확률 분포
            - 이산형 확률변수 : 0이 아닌 확률값을 갖는 확률 변수를 셀 수 있는 경우(확률 질량 함수)

                ㄴ 베르누이 확률분포(bernoulli distribution) : 결과가 2개만 나오는 경우(동전 던지기, 승/패, 합/불) _ $P(X=x) = p^x*(1-p)^(x-1)$, $E(x)=p, var(x)=p(p-1)$

                ㄴ 이항분포(Binomial distribution) : 베르누이 시행을 n번 반복 했을때 k번 성공 확률

                $P(X=k)=nCkP^k(1-p)^(n-k), nCk = n!/k!(n-k)!$ ($X : B(n,p), E(x) = np, var(x) = np(1-p)$)

                ㄴ 다항분포(Multinomial distribution) : 이항분포를 확장 세 가지 이상의 결과를 반복시행 발생 확률

                ㄴ 포아송 분포(poisson distribution) : 시간/공간 내에서 사건의 발생 횟수에 대한 확률 분포

                λ=특정 시간 내 사건이 일어날 횟수에 대한 기대값, y=사건이 일어난 수

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2023.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2023.png)

            - 연속형 확률변수 : 실수의 특정 구간 전체에 해당하는 확률 변수(확률 밀도 함수)

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2024.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2024.png)

                - 균일분포(일양분포, Uniform distribution) : 모든 확률 변수가 동일한 확률 $E(x) =(a+b)/2, Var(x) = (b-a)^2$
                - 정규분포(Normal Distribution) : 평균 μ, 표준편차 σ인 확률 밀도 함수

                    ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2025.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2025.png)

                - 지수분포(Exponential distribution) : 어떤 사건이 발생할 때까지 경과 시간에 대한 연속확률분포(전자레인지 수명시간, 콜센터에 전화가 걸려올 때까지 시간, 은행 고객 내방 시간)
                - T-분포(T-distribution) : 평균 0중심, 좌우 대칭(표준정규분포와 동일)_표본이 적을땐 납작, 표본(30개이상 자유도)커지면 ND와 비슷한 분포

                    ㄴ 연속형일때 사용, 두 집단의 평균의 동일여부 확인하기 위한 **검정통계량**으로 활용

                - $x^2$-분포(chi-square distribution, 카이제곱분포) : 모집단의 모분산에 대한 **가설 검정**에 사용(모평균, 모분산이 알려지지 않은 집단)

                    ㄴ 두 집단간 동질성 검정에 활용 (범주형 자료에서 관측값과 기대값 차이 적합성 검정 활용)

                - F-분포(F-distribution) : 두 집단간 분산의 동일성 **검정통계량**

                    ㄴ 확률변수 항상 양의 값, $x^2$분포와 달리 자유도 2개 가지고 있음, 자유도 커질수록 정규분포에 가까워짐(T-분포도 표본이 커지면 ND에 가까워짐)

    - 5. 추정과 가설검정
        - 1. 추정의 개요
            - 확률표본(random sample) : 어떤 확률분포에서 독립적으로 반복해 표본 추출(각 관찰값은 서로 독립, 동일 분포)
            - 추정 : 표본에서 미지의 모수 추측(점추정, 구간추정으로 구분)

                ㄴ **점추정(Point estimation)** : '모수가 특정한 값일 것' 이라고 추정(표본의 평균, 중위수, 최빈값 등을 사용)

                - 점추정량의 조건
                    - 불편성(unbiasedness) : 표본의 추정량의 기댓값과 모집단의 모수와 편차 없음
                    - 효율성(efficiency) : 추정량 분산 작을 수록 좋음
                    - 일치성(consistency) : 표본 크기 커지면, 추정량은 모수로 수렴
                    - 충족성(sufficient) : 추정량은 모수에 대해 모든 정보를 제공
                    - 표본평균(sample mean), 표본분산(sample variance) : 모집단의 모평균/분산 추정하기 위한 추정량

                ㄴ **구간추정(interval estimation)** : 모수는 특정 구간에 있을 것이라 선언(점추정의 정확성 보완)

                → 추정량의 분포 전제가 필요, 신뢰수준 필요(confidence interval, 구간안에 모수가 있을 가능성 크기)

        - 2. 가설검정 : 가설 → 표보관찰 → 채택여부 결정
            - 표본관찰/실험으로 귀무가설 or 대립가설 선택("귀무가설 옳다" 전제 → 검정통계량 값 확인 → **가능성의 크기**에 따라 귀무가설 채택여부 결정)

                ㄴ 귀무가설(Null hypothesis, $H_0$) : **"비교하는 값과 차이가 없다, 동일하다"**를 기본 개념으로 하는 가설(현재상태 or 대립가설을 무효화 하는 주장))

                ㄴ 대립가설(alternative hypothesis, $H_1$) : **뚜렸한 증거가 있을 때** 주장하는 가설(입증하고 싶은 주장)

                ㄴ 검정통계량(test statistic) : 관찰된 표본으로 구하는 통계량, 검정 시 가설의 진위 판단 기준

                ㄴ **유의수준(significance level, α) : 귀무가설이 옳은데도 이를 기각하는 확률의 크기(P-value는 유의확률)**

                ㄴ **기각역**(critical region, C) : 귀무가설이 옳다는 전제 하 검정통계량의 분포에서 확률이 유의수준 α인 부분(반대는 채택역(acceptance region))

                **※ 제1종오류(α)와 제2종오류(β)**

                → 제 1종오류(type 1 error) : 귀무가설 $H_0$가 옳은데도 귀무가설을 기각하게 되는 오류

                → 제 2종 오류(tyep 2 error) : 귀무가설 $H_0$가 옳지 않은데도 귀무가설을 채택하게 되는 오류

                ⇒ 두 가지 오류는 서로 상충 관계, α크기를 0.1,0.05,0.01등으로 고정 β가 최소가 되도록 기각역 설정

    - 6. 비모수 검정 : 모수적 검정/비모수적 검정
        1. **모수적** 방법 : 모집단 분포 가정, 검정통계량과 검정통계량 분포를 유도하여 검정
        2. **비모수적** 방법 : 추출된 모집단의 분포에 제약을 가하지 않고 검정

            ㄴ 사용 : 관측된 자료에 대한 분포를 가정할 수 없는 경우, 관측된 자료수 적음, 개체간의 서열관계를 나타내는 경우

        3. 모수적검정과 비모수적검정의 차이점
            1. 가설의 설정 
                - 모수적 검정 : 가정된 분포의 모수에 대한 가설
                - 비모수 검정 : 가정된 분포가 없음, 분포 형태에 대해 설정(분포의 형태가 동일하다, 하지 않다 등)
            2. 검정 방법 
                - 모수적 검정 : 표본 평균, 분산을 이용하여 검정 실시
                - 비모수 검정 : 관측값들의 순위, 두 관측값 차이의 부호 이용 검정(절대적 크기에 의존 하지 않음)
        4. 비모수 검정의 예
            - 부호검정(sign test), 윌콕슨의 순위합검정(rank sum test), 윌콕슨의 부호순위합검정(wilcoxon signed rank test), 만-위트니의 U검정, 런검정(run test),스피어만의 순위상관계수

                ㄴ 스피어만 : 두 변수 의 순위 사이의 통계적 의존성을 측정(비모수적 상관관계, 순서형 변수, -1~1사이값) 연속형일 경우 순위 척도로 변환 하여 사용

                ↔ 피어슨 : 연속형 변수, 정규성가정, 각 객체의 집합이 직선으로 표현되는 정도 -1~1, 상관계수는 r^2 

- #### 2절 기초 통계분석
    - 1. 기술통계(Descriptive Statistics) : 표, 그림, 통계량으로 자료 특성을 쉽게 정리/요약(기초 통계, 데이터 분석에 앞서 데이터에 대한 대략적인 이해와 통찰력)
        - 1. 통계량에 의한 자료 정리
            1. 중심 위치의 측도 : 자료(데이터)의 중심, 표본평균(sample mean), 중앙값(median)
            2. 산포의 측도 : 산포도(dispersion) - **분산, 표준편차, 범위, 사분위수 범위**, 백분위수, 변동계수, 평균의 표준오차
            3. 분포의 형태에 관한 측도

                ㄴ 왜도 : 분포의 **비대칭정도를 나타내는 측도**($m_3$ 양수 : 오른쪽 긴꼬리(최빈 중앙 평균), 음수 : 왼쪽 긴꼬리, 0 : 좌우대칭(평균 중앙 최빈))

                ㄴ 첨도 : 분포의 **뾰족한 정도를 나타내는 측도**($m_4$ 양수 : 많이 뾰족, 음수 : 덜 뾰족, 0 : 표준정규분포와 유사한 뾰족)

        - 2. 그래프를 이용한  자료 정리
            1. 히스토그램 : 도수분포표를 그래프로 나타낸 것(봉우리가 여러개면 2개 이상의 공정이나 조건데이터가 수집)
            2. 막대그래프와 히스토그램의 비교
                - 막대그래프 : **범주형**(Category)으로 구분된 데이터를 표현, 범주 순서를 의도에 맞춰 바꿀 수 있음(예 : 직업, 종교, 음식)

                    ㄴ**파레토그림**(pareto diagram) : 명목형 자료에서 중요한 소수?를 찾는데 유용

                - 히스토그램 : **연속형**(continuous)으로 표시된 데이터, 임의로 순서 바꿀 수 없음(예 : 몸무게, 성적, 연봉)

                    ㄴ히스토그램 생성 : 데이터 수, 계급수($k=log_2$$n$), 계급간격($(max-min)/k$) 계산 → 도수분포표 → 히스토그램

                    - 그래프

                        ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2026.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2026.png)

                        ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2027.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2027.png)

                - 줄기-잎 그림(stem-and leaf plot) : 데이터를 줄기/잎 모양으로 그린 그림(계산량 적음)
                    - 그래프

                        ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2028.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2028.png)

                - 상자그림(Box plot) : 최소값, Q1, Q2, Q3, 최대값 다섯 숫자 요약
                    - 사분위수 범위(IQR) : Q3-Q1
                    - 안울타리(inner fence) : Q1 - 1.5 X IQR 또는 Q3 + 1.5 X IQR
                    - 바깥 울타리(outer fence) : Q1 - 3 X IQR 또는 Q3 + 3 X IQR
                    - 보통이상점(mild outlier) : 안쪽/바깥 울타리 사이에 있는 자료
                    - 극단이상점(extreme outlier) : 바깥 울타리 밖의 자료
                    - 그래프

                        ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2029.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2029.png)

    - 2. 인과관계의 이해
        1. 종속변수(반응변수, y) : 다른 변수의 영향을 받는 변수
        2. 독립변수(설명변수, x) : 영향을 주는 변수
        3. 산점도(scatter plot) : 좌표평면에 점들로 표현
            - 확인할 사항
                - 두 변수 사이 선형(직선) 관계 성립여부
                - 두 변수 사이 함수관계(직선 or 곡선) 성립 여부
                - 이상값 존재?
                - 몇 개의 집단으로 구분 되는가?
                - 그래프

                    ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2030.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2030.png)

        4. 공분산(covariance) : 두 확률변수 X,Y의 방향 조합(선형성) - $Cov(X,Y) = E[(X-μ_X)(Y-μ_Y)]$_(X,Y가 독립이면 $Cov(X,Y)=0$)

            ㄴ 부호로 두 변수의 방향성 확인 가능(**+ : 양의 방향성, - : 음의 방향성**)

            - 그래프

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2031.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2031.png)

    - 3. 상관분석(Correlation Analysis) : 두 변수 간의 관계의 정도 확인 → **상관계수** 활용
        - 1. 상관관계의 특성

            ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2032.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2032.png)

        - 2. 상관분석의 유형
            - 피어슨 : 등간척도 이상으로 두 변수들의 상관관계 측정 방식(연속형 변수, 정규성 가정, 피어슨 γ(적률상관계수))
            - 스피어만 : 서열철도인 두 변수들의 상관관계 측정 방식(순서형 변수, 비모수적 방법, 순위 기준 상관관계 측정, 순위상관계수(σ, 로우))
        - 3. 상관분석의 가설 검정
            - $r$=0 x와 y사이 관계 없음(귀무가설 $r$=0, 대립가설 $r$≠0)
            - t 검정통계량 p-Value값이 0.05 이하인 경우 대립가설을 채택, 데이터 상관계수 활용 가능
        - 상관분석 예제

            ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2033.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2033.png)

- #### 3절 회귀분석
    - 1. 회귀분석의 개요
        - 1. 회귀분석의 정의
            - 하나 이상의 독립변수들이 종속변수에 미치는 영향을 추정하는 통계 기법
            - 변수들의 인과관계 확인 모형을 적합, 관심있는 변수 예측, 추론
            - 독립변수 1개 : 단순선형회귀분석, 2개이상 : 다중선형회귀분석
        - 2. 회귀분석의 변수
            - 영향을 받은 변수(y) : 반응변수(response variable), 종속변수(dependent variable), 결과변수(outcome variable)
            - 영향을 주는 변수(x) : 설명변수(explanatory variable), 독립변수(independent variable), 예측변수(predictor variable)
        - 3. 선형회귀분석의 가정
            - 선형성 : 입력, 출력 변수의 관계가 선형(선형회귀분석에서 가장 중요한 가정

                설명변수(x), 반응변수(y)는 선형관계 전제

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2034.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2034.png)

            - 등분산성 : 오차의 분산은 입력변수와 무관, 잔차플롯(산점도)를 활용 잔차와 입력변수간 무작위적으로 고루 분포해야 가정을 만족(반대 : 이분산성)
                1. 등분산성 만족 : 잔차 산점도를 그렸을때 변동성이 일정한 경우\

                    ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2035.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2035.png)

                2. 만족 못하는경우

                    ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2036.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2036.png)

            - 독립성 : 입력변수와 오차는 관련 없음, Durbin-watson 통계량으로 자기상관(독립성)확인(주로 시계열데이터에서 많이 활용)
            - 비상관성 : 오차들끼리 상관 없음
            - 정상성(정규성) : 오차는 정규분포를 따름, Q-Q plot, Kolmogolov-smirnov 검정, shaprio-wilk 검정을 통해 정규성 확인(Durbin watson test 는 회귀모형 오차항)

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2037.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2037.png)

        - 4. 가정에 대한 검증
            1. 단순선형회귀분석 : 입출력 변수간 산점도로 선형성 확인
            2. 다중선형회귀분석 : **선형성, 등분산성, 독립성, 정상성(선형회귀분석 가정)** 만족 여부 확인
    - 2. 단순선형회귀분석

        ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2038.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2038.png)

        - 1. 회귀분석에서의 검토사항

            1) 회귀계수들이 유의미 여부 : t 통계량 p-value < 0.05 경우 통계적으로 유의미

            2) 모형이 얼마나 설명력 갖는지 : 결정계수($R^2$) 확인(0~1, 높을 수록 회귀식 설명력 높음)

            3) 모형이 데이터를 잘 적합하고 있지 : 잔차 그래프로 회귀진단

        - 2. 회귀계수의 추정(최소제곱법, 최소자승법)
            - 측정값을 기초로 제곱합을 최소로 하는 값으로 측정결과를 처리하는 방식(잔차제곱이 가장 작은 선을 구함)
        - 3. 회귀분석의 검정

            1) 회귀계수의 검정 : 회귀계수 $β_1$= 0, 입출력 변수사이 인과관계 없음

            2) 결정계수  : $SSR/SST, 0≤R^2≤1$ (여기서 SST=SSR+SSE) _ 전체 데이터를 회귀모형으로 설명할 수 있는 설명력 의미

            - 전체제곱합(SST, Total sum of square), 회귀제곱합(SSR, Regression sum of square), 오차제곱합(SSE, residuals sum of square)

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2039.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2039.png)

                **결정계수$(R^2)$ $=$ $(A)/((A)+(B))$**

            3) 회귀직선의 적합도 검토

            - 결정계수($R^2$)의 타당성 검토
            - 독립변수가 종속변수 변동의 몇 %를 설명하는지 나타내는 지표
            - 다변량 회귀분석에서 독립변수가 많아지면, 유의성과 상관없이 결정계수($R^2$)가 높아지는 단점이 있음
            - 단점을 보완한 수정된 결정계수($R^2_a$ : adjusted $R^2$)_결정계수보다 작은값을 가짐

                ㄴ 수정된**결정계수$(R^2)$ $=$ $(A)/((A)+(B))$*(n-p+1)**

            ※ 오차(error)와 잔차(residual)의 차이  

            ㄴ오차 : 모집단에서 실제값과 회귀선의 차이(정확치와 관측치 차이)

            ㄴ잔차 : 표본에서 관측값과 회귀선의 차이 ⇒ 회귀모형에서 오차항 측정불가, 잔차로 오차항에 대한 가정 확인

    - 3. 다중선형회귀분석
        - 1. **다중선형회귀분석**(다변량회귀분석)
            1. 다중회귀식 : $Y=β_0  + β_1X_1+β_2X_2 + ... + β_kX_k+ε$
            2. 모형의 통계적 유의성
                - F 통계량으로 확인, 유의수준 5%하에서 F통계량의 p-value < 0.05 이면 통계적으로 유의한 회귀식
                - F통계량이 크면, P-value <0.05이 되고 귀무가설 기각
                - 참고

                    ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2040.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2040.png)

            3. 회귀계수의 유의성 : t통계량으로 확인, 모든 회귀계수의 유의성이 통계적으로 검증되어야 모형으로 활용 가능
            4. 모형의 설명력 : 수정된 결정계수($R^2_a$ : adjusted $R^2$)나 결정계수를 사용
            5. 모형의 적합성 : 잔차와 종속변수의 산점도 확인
            6. 데이터 전제 가정 : 선형성, 독립성, 등분산성, 비상관성, 정상성
            7. 다중공선성(Multicollinearity) : 다중회귀분석에서 설명변수(x)들 사이에 선형관계 존재시 회귀계수 정확한 추정 어려움

                ㄴ 다중산공성 검사 방법 

                1) **분산팽창요인(VIF)** : > 4 →다중공선성 존재, >10 → 심각한 문제 

                2) **상태지수** : ≥10 → 문제가 있음, ≥30 → 심각한 문제(다중선형회귀분석에서 **다중공선성 문제 발생시, 변수 제거 or 주성분회귀, 능형회귀 모형 적용**)

    - 4. 회귀분석의 종류

        ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2041.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2041.png)

    - 5. 회귀분석 사례
    - 6. 최적회귀방정식
        - 1. 최적회귀방정식의 선택

            1) 설명변수 선택 : 필요한 변수만 상황에 따라 타협을 통해 선택, y값에 영향이 있는 모든 x값을 참여시킴, 가능한 적은 수의 x포함

            2) 모형선택(exploratory analysis) : 분석 데이터에 가장 잘 맞는 모형 찾는 방법(모든 조합을 회귀분석 후 가장 적합한 회귀분석 선택)

            3) 단계적 변수선택(stepwise variable selection) 

            - 전진선택법(forward selection) : 절편만 있는 상수모형으로 시작, 중요한 설명변수들 차례로 모형에 추가(단점 : 변수값의 작은 변동에도 결과가 달라짐)
            - 후진제거법(backward elimination) :  독립변수 후보 모두 포함한 모형에서 가장 영향력이 적은 변수부터 제거함(단점 : 개수가 많은 경우 사용이 어려움)
            - 단계선택법(stepwise method) : 전진선택법에 의해 변수를 추가로 기존 변수의 중요도가 떨어지면 제거
        - 2. 벌점화된 선택기준 : 모형의 복잡도에 벌점을 주는 방식 (AIC, BIC 방법이 사용됨)
            - AIC(Akaike information criterion) : parameter*2의 벌점, 표본의 크기와 상관이 없음, 실제와 예측 분포의 차이를 나타냄(예측분포 확인에 쓰임)
            - BIC(bayesian information criterion) : Log(n)의 벌점, 표본의 크기에 비례하여 커짐(실제 데이터 분포 확인에 쓰임)
            - 설명

                → 모든 후보 모형 중 **AIC, BIC가 최소가 되는 모형 선택**

                - 모형테이터의 일치성(consistency inselection) : 자료의 수가 늘어날때 참인 모형이 주어진 모형 선택 기준 최소값을 갖게되는 성질
                - 이론적으로 AIC는 일치성 성립하지 않지만  BIC는주요 부품에서 이러한 성질이 성립
                - AIC를 활용하는게 보편하된 방법
                - 그 외 벌점화 선택 기준 RIC(risk inflation criterion), CIC(Covariance inflation criterion), DIC(deviation information criterion)

    - 7. 로지스틱 회귀 모형
        - 어떤 사건이 발생여부 예측이 아니라 그 사건이 발생할 확률을 예측
        - 설명변수 대비 반응변수의 발생 증가율 → 승산비(오즈비)(사건 발생/사건발생 X)
        - 종속변수 : 범주형(이분형(0,1값만 가짐)), 독립변수 :  범주, 연속 모두 가능,
- #### 4절 시계열 분석
    - 1. 시계열 자료 : 시간의 흐름에 따라 관찰된 값(미래값 예측, 경향, 주기, 계절성 파악), ETS(Error, Trend, Seasonality)모델

        **17세기** : 태양흑점/밀가격지수 변동에  sin,cos활용, **1926(Yule)** : ARMA 개념, **1937(Walker)** : ARMA 모형, **1960/1970(Durbin/Box&jenkins)** : ARMA 모형 추정, **1957(holt)** : 지수평활법(Exponential soomthing)제시, **1960(Winter)** : 계절성(seasonal) 지수평활법 제시

        - 정상성  시계열 자료 : 다루기 쉬운 자료 / 비정상성 시계열 자료 : 다루기 어려운 자료
    - 2. 정상성 : 동일 평균, 동일 분산, 동일 시차에 대한 동일 공분산_정상시계열
        - **모든 시점 동일 평균**(평균 일정치 않을 경우 차분(diference)을 통해 정상화)
        - **모든 시점 동일 분산**(분산 일정치 않을 경우 변환(Transformation)을 통해 정상화)
        - **공분산은 시차에만 의존**(실제 특정 시점 t, s에는 의존 하지 않음)

        차분 : 현 시점 자료에서 전시점 자료 뺀 값(일반 차분 : 바로 전 시점의 자료를 뺌, 계절차분 : 여러시점 전 자료를 뺌)

    - 3. 시계열자료 분석방법 : 회귀분석(계량경제)방법, Box-jenkins방법, 지수평활법, 시계열분해법
        - 1. 분석방법
            - 수학적 이론 모형 : 회귀분석(계량경제)방법, Box-Jenkins 방법
            - 직관적 방법 : 지수평활법, 시계열 분해법(시간에 따른 **변동이 느린 데이터 분석**에 활용)
            - 장기 예측 : 회귀분석방법 활용
            - 단기 예측 : Box-jenkins 방법, 지수평호라법, 시계열분해법 활용
        - 2. 자료 형태에 따른 분석 방법
            - 일변량 시계열분석 : 시간(t)을 설명변수로 한 회귀모형주가, 소매물가지수 등 하나의 변수(Box-jenkins 방법, 지수평호라법, 시계열분해법)
            - 다중 시계열분석 : 여러개의 시간(t) 변수들을 활용(계량경제모형, 전이함수모형, 개입분석, 상태공간분석, 다변량 ARIMA)
        - 3. 이동평균법 : 현재까지 일정기간별 이동 평균 계산, 다음 기간의 추세를 파악하여 예측

            ㄴ 시계열 자료에서 계절변동, 불규칙변동을 제거, 추세/순환변동안 가진 시계열로 변환하는 방법

            1) 간단하게 미래 예측 가능(자료 수 많고, 안정된 패턴의 경우 예측 품질 높음)

            2) 특정 기간 안에 속하는 시계열에 동일한 가중치 부여

            3) 추세 강하고 불규칙 변동 없는 경우 **짧은 기간**의 이동평균, 추세가 희미하고 불규칙변동이 심한경우 **긴 기간**의 평균 사용

            4) **적절한 n의 개수**를 정하는게 중요함

        - 4. 지수평활법 : 모든 시게열 자료로 평균을 구하고, 최근 시계열에 더 가중치를 부여하여 미래 예측(이동평균법은 일정 기간 평균, 특정 기간 가중치)

            1) 단기간 불규칙 변동을 평활하는 방법으로 사용

            2) 자료 수 多, 안정된 패턴 → 예측 품질 좋음

            3) 지수평활계수(α) : 가중치 역할(불규칙 변동 큼 → 작은 α, 불규칙 변동 작음 → 큰 α)

            4) 지수평활계수는 예측오차(잔차제곱합)이 작은 값을 선택

            5) 과거로 갈수록 지수평활계수 감소

            6) 불규칙 변동의 영향을 제거 효과, 중기 예측이상에 주로 사용(**단순지수평활법**의 경우 **장기 추세**, **계절변동에 적합하지** **않음**)

    - 4. 시계열 모형
        - 1. 자기회기 모형(AR 모형, Autoregressive Model) : p 시점 전의 자료가 현재 자료에 영향을 주는 모형

            $Z_t=Φ_1Z_(t-1)+Φ_2Z_(t-2)+...+Φ_pZ_(t-p)+α_t$

            $Z_t$ : 현재 시점의 시계열, $Z_(t-1), Z_(t-2), Z_p$ : 시점p의 시계열 자료, $Φ_p$ : p시점의 영향력을 나타내는 모수, 

            $α_t$ : 백색 잡음(오차항) ⇒ 평균 0 분산 $σ^2$, 자기분산이 0인 경우 (시계열간 확률적 독립 → 강 백색잡음 과정, 정규분포 →가우시안 백색잡음과정)

            - **AR(1) 모형** : 직전 시점 데이터로 분석, **AR(2) 모형** : 연속된 3시점 정도의 데이터로 분석
            - 자기상관함수(ACF) 빠르게 감소, 부분자기함수(PACF)는 어느 시점에서 절단점을 가짐
        - 2. 이동평균 모형(MA 모형, Moving Average Model) : 유한 개수의 백색잡음의 결합, 언제나 정상성 만족
            - 1차 이동평균모형(MA1 모형) : 같은 시점의 백색잡음 바로 전 시점의 백색잡음의 결합
            - 2차 이동평균모형(MA2 모형) : 바로 전 시점의 백색잡음과 시차가 2인 백색잡음의 결합
            - ACF에서 절단점, PACF 빠르게 감소(AR은 PACF 절단점, ACF 빠르게 감소)
        - 3. 자기회귀누적이동평균모형(ARIMA(p,d,q) 모형, autoregressive integrated moving average model)
            - 시계열 {$Z_t$}의 d번 차분한 시계열이 ARMA(p,q)모형일 경우, ARIMA(p,d,q)모형을 갖음
            - 비정상시계열 모형
            - 차분이나 변환을 통해 AR/MA 모형 둘을 합친 ARMA모형으로 정상화 가능
            - p는 AR 모형, q는 MA 모형 관련 차수
            - d=0일 경우 ARMA(p,q)모형, 이모형은 정상성을 만족(ARMA(0,0) 일 경우 정상화 불필요)
        - 4. 분해 시계열 : 시계열에 영향을 주는 일반적인 요인을 시계열에서 분리하여 분석, 회귀분석적인 방법을 주로 사용(요소분해법)

            $Z_t = f(T_t, S_t, C_t, I_t)$

            - $T_t$ : 경향(추세)요인 : 자료의 오르 내리는 추세(선형, 이차, 지수적 형태)_경향
            - $S_t$ : 계절요인 : 요일, 월, 사계절 각 분기에 의한 변화, 고정된 주기에 따라 변화_계절변동
            - $C_t$ : 순환요인 : 알려지지 않은 주기를 가지고 변화하는 자료(경제적이나, 자연적인 이유가 아닌) _ 순환변동
            - $I_t$  : 불규칙요인 : 위 세 가지를 제외한 설명할수 없는 오차 요인 _ 급격한 환경변화, 천재지변 등
- #### 5절 다차원척도법
    - 1. 다차원척도법(Multidimensional Scaling) : 개체 사이 유사성/비유사성 측정하여, 근접성을 공간상에 시각화하는 분석 방법
        - 개체들을 2,3차원 공간상 점으로 표현, 개체들 사이의 집단화를 시각적으로 표현
        - 목적
            1. 데이터속 **잠재된 패턴/구조**를 찾아냄
            2. **소수 차원 공간**에 기하학적으로 표현
            3. **데이터 축소(data Reduction)**의 목적으로 사용
        - 방법
            1. **유클리드 거리행렬**을 활용하여 개체간 거리 계산
            2. 관측대상들의 상대적 거리의 정확도를 높이기 위해 **스트레스 값**으로 나타냄
            3. 개체를 공간에 표현하기 위해 부적합도 기준으로 STRESS나 S-STRESS 사용
            4. **최적모형의 적합**은 **부적합도를 최소**로 하는 **반복 알고리즘**을 이용(부적합도가 일정 수준 이하가 될때 까지)
            - Stress

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2042.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2042.png)

        - 종류

            1) 계량적 MDS(Metric MDS) : 구간척도, 비율척도 데이터에 활용, N개 케이스중 p개의 특성변수에 대해 각 개체들간의 유클리드 거리행렬을 계산, 비유사성S(거리제곱 행렬의 선형함수)를 공간상에 표현

            2) 비계량적 MDS(Non-Metric MDS) : 순서척도 데이터에 활용, 순서척도를 거리의 속성과 같도록 변환(monotone transformation)하여 거리를 생성한 후 적용

- #### 6절 주성분 분석
    - 1. 주성분 분석(Principal Component Analysis) : 상관성이 높은 변수들 선형결합하여, 변수를 요약/축소(1st 주성분 : 전체변동 설명, 2nd 주성분 : 1st 주성분과 상관성 낮도록 설정)
        - 목적
            1. 데이터 이해 및 관리 쉽게 해줌(변수간 내재하는 상관관계, 연관성 활용 소수의 주성분으로 차원 축소)
            2. 다중공선성(독립변수들 강 강한 상관관계) 존재할 경우, **상관성이 없는 주성분**으로 **변수 축소**
            3. 군집화 결과 및 연산속도 개선
            4. 기계의 고장(fatal failure)징후를 사전 파악 가능함
        - 주성분의 선택법
            1. 분석 결과 누적기여율(cumulative proportion)이 85%이상
            2. scree plot 활용, 고유값(eigenvalue)이 수평을 유지하기 전단계로 주성분의 수 선택
            - Scree plot(100%-y값이 85%이상이 처음 나온 곳(통상 85%))

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2043.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2043.png)

    - 2. 주성분분석 vs 요인분석
        - 요인분석(Factor Analysis) : 등간척도(비율척도)데이터에서 두 개 이상의 변수들에 공통인자를 찾아내는 기법
        - 공통점 : 데이터 축소
        - 차이점 :

            1) 변수의 수 : PCA - 3개 이하, FA - 여러개

            2) 변수의 이름 : PCA -  제1주성분, 제2주성분.. , FA - 분석자 직접 명명

            3) 변수들 간의 관계 : PCA - 제1주성분이 가장 중요, FA - 중요도 없음(단 분류/예측 다음단계에서 중요성의 의미가 부여됨)

            4) 분석 방법의 의미 : PCA - 목표변수 고려하여 주성분 탐색, FA - 목표변수 고려 X, 비슷한 성격으로 새로운 잠재변수 만듬

### 5장 정형 데이터 마이닝

- #### 1절 데이터 마이닝 개요
    - 1. 데이터 마이닝 : 대용량 데이터에서 의미있는 패턴 파악, 예측하여 의사결정에 활용
        - 데이터 마이닝 - 다양한 알고리즘 통하여 데이터로부터 의미 있는 정보를 찾아냄, **가정, 가설이 없음**(기존 통계분석 - 가설, 가정에 따른 분석이나 검증)
        - 정보를 찾아내는 벙법론
            - 인공지능(Artificial Intelligence)
            - 의사결정나무(Decision Tree)
            - k-평균군집화(K-means Clustering)
            - 연관분석(Association Rule)
            - 회귀분석(Regression)
            - 로짓분석(Logit Analysis)
            - 최근접이웃(Nearest Neighborhood)
        - 분석대상, 활용목적, 표현에 따른 분류
            - 시각화분석(Visualization Analysis)
            - 분류(Classification)
            - 군집화(Clustering)
            - 포케스팅(Forecasting)
        - 사용분야 : 환자별 발생 가능한 병 예측, 기존 환자 응급실 왔을시 어떤 조치부터 해야할지 결정, 대출 적격여부 판단, 입국자이력 데이터 관세물품 반입여부 예측
        - 알고리즘에 대한 깊은 이해가 없어도 분석에 큰 영향성 없으나, 과제 복잡성, 중요도가 높을경우 **결과 품질**을 위해 **전문가**에 의뢰 필요
    - 2. 데이터 마이닝 종류 :
        - 지도학습 모델(Supervised Data Prediction)
            - 의사결정나무(Decision Tree)
            - 인공신경망(ANN, Antificial Neural Network)
            - 일반화 선형모형(GLM, Generalized Linear Model)
            - 회귀분석(regression Analysis)
            - 로지스틱 회귀분석(Logistic Regression Analysis)
            - 사례기반 추론(Case-Based Reasoning)
            - 최근접 이웃법(KNN, K-Nearest Neigbor)
        - 비지도학습 모델(UnSupervised Data Prediction)
            - OLAP(On-Line Analysis Processing)
            - 연관성 규칙발견(Association Rule discovery, Market basket)
            - 군집분석(K-Means Clustering)
            - SOM(Self Organizing Map)
    - 3. 분석에 따른 작업 유형과 기법
        - 예측(Predictive Modeling)
            - 분류규칙(Classification) : 가장 많이 사용, 과거 고객 데이터에서 분류모형을 만들어 새로운 레코드의 결과값을 예측 목표(마케팅, 고객신용평가 모형에 활용)

                ㄴ 회귀 분석, 판별분석, 신경망, 의사결정나무

        - 설명(Descriptive Modeling)
            - 연관규칙(Association) : 데이터 항목간 종속관계를 찾아내는 작업(교차판매, 매장진열, 첨부우편, 사기적발)

                ㄴ 동시발생 매트릭스

            - 연속규칙(Sequence) : 연관규칙에 시간관련 정보가 포함, 고객의 구매이력 속성이 반드시 필요(목표마케팅, 일대일 마케팅)

                ㄴ 동시발생 매트릭스

            - 데이터 군집화(Clustering) : 고객 레코드를 유사한 특성을 지닌 소그룹화, 분류규칙(Classification)과 유사하나 분석대상 데이터에 결과값이 없음(판촉활동, 이벤트 대상)

                ㄴ K-Means Clustering

    - 4. 추진 단계
        1. 목적 설정 : 무엇을 왜 하는지 명확한 목적, 전문가와 사용할 모델, 필요 데이터 정의
        2. 데이터 준비 : 정형, 비정형 다양한 데이터 활용, IT 부서와 데이터 접근 부하 유의하여 준비, 데이터 정제하여 품질 확보, 충분한 양의 데이터 확보
        3. 가공 : 목적에 따라 목적변수 정의, 데이터 마이닝 소프트웨어에 적용 가능하도록 가공
        4. 기법적용 : 1단계에서 선정한 데이터마이닝 기법 적용하여 정보를 추출
        5. 검증 : 추출된 정보 검증, 테스트 데이터와 과거 데이터를 활용하여 최적 모델 선정, 검증 후 IT 부서와 협의해 업무에 적용 및 보고서 작성, 기대효과 전파
    - 5. 데이터 분할
        - 평가용 데이터와 구축용 데이터로 분할, 구축용 데이터로 모형을 생성, 테스트 데이터로 모형 적합성 판단
        - 구축용(training data, 50%) : 추정용, 훈련용 데이터, 데이터 마이닝 모델을 만드는데 활용
        - 검정용(validation data, 30%) : 구축된 모델의 과대추정, 과소추정을 미세조정에 활용
        - 시험용(test data, 20%) : 테스트 데이터, 과거 데이터를 활용하여 모델의 성능을 검증

        ※ 데이터 양이 충분하지 않거나, 입력 변수에 대한 설명이 충분한 경우

        1) 홀드아웃(hold-out) 방법 : 랜덤하게 두 개의 데이터로 구분하여 학습용(Training data), 시험용(test data)로 분리 사용

        2) 교차확인(Cross-validation) 방법 : 데이터를 k개의 하부 집단으로 구분, k-1개의 집단을 학습용, 나머지 하부집단으로 검증용으로 설정하여 학습, **k번 반복하여 결과의 평균을 최종값**으로 사용

        ㄴ 10-fold 교차분석을 많이 활용

    - 6. 성과분석
        - 과적합(Overfitting), 과소적합(Underfitting)
            - 과적합,과대적합(Overfitting) : 학습용 데이터를 과하게 학습하여, 학습데이터 내에서 높은 정확도를 갖지만, 테스트 데이터 or 다른 데이터 적용 시 성능 떨어짐
            - 과소적합(Underfitting) : 데이터 특징을 정확하게 설명하지 못하고, 지나치게 일반화
            - 일반화(generalization) : 데이터 특성을 잘 설명하면서도 지나치게 학습되지 않음, 새로운 데이터 입력에도 좋은 성능 나타남
        - 오분류에 대한 추정치
            - 정분류율(Accuracy) = (**T**N+**T**P)/(TN+TP+FN+FP) → 정확 분류/Total

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2044.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2044.png)

            - 오분류율(Error Rate) = 1-Accuracy = (FN+FP)/(TN+TP+FN+FP) → 틀린 분류/Total

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2045.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2045.png)

            - 특이도(Specificity) = TN/(TN+FP) → N 정확하게 구분/실제 N (TNR : True Negative Rate)

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2046.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2046.png)

            - 민감도(Sensitivity) = TP/(TP+FN) → P 정확하게 구분/실제 P(TPR : True Positive Rate)

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2047.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2047.png)

            - 정확도(Precision) = TP/(TP+FP) → 실제 P/P예측

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2048.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2048.png)

            - 재현율(Recall) = 민감도 = TP/(TP+FN)

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2047.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2047.png)

            - F1 Score = 2*정확도*재현율/(정확도+재현율) = 2TP/(2TP+FP+FN)

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2049.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2049.png)

        - ROCR 패키지 성과 분석
            - 1. ROC Curve(Receiver Operating Characteristic Curve) : 가로축(FPR, 1-특이도), 세로축(TPR, 민감도) 값으로 시각화 그래프
                - 2진 분류(binary classification) 모형 성능 평가 척도
                - 그래프상 왼쪽 상단에 가깝게 그려질수록 올바른 예측 비율 높고, 잘못 예측 낮음(ROC 곡석 아래의 면적이 높을수록 성능 좋음)_AUROC(Area Under ROC)
                - FPR(False Positive Rate), 1-특이도(1-TN/(TN+FP)=FP/(TN+FP)) : 0인 케이스를 1로 잘못 예측
                - TPR(True Positive Rate), 민감도 : 1인 케이스를 1로 예측
                - AUROC를 이용한 정확도 판단 기준

                    ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2050.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2050.png)

            - 2. 이익도표(Lift Chart)
                - 분류모형 성능 평가 척도로 활용
                - 분류된 관측치에 대한 예측 정도를 나타내기 위해 임의로 나눈 각 등급별로 반응검출율, 반응률, 리프트등 정보를 산출한 도표
                - 평가 방법 : 1단계(예측확률을 내림차순 정리) → 2단계(k개 구간으로 구분, 각 구간의 반응률(%, response)산출, → 3단계(향상도 계산 = 반응율/기본향상도(Baseline list))

                    ㄴ 향상도가 빠르게 감소할 수록 좋은 모델, 기본향상도(=대상/total)

- #### 2절 분류분석
    - 1. 분류분석과 예측분석
        - 1. 분류분석 정의 : 데이터가 어떤 그룹에 속하는지 알아맞힘
            - Clustering과 비슷하지만 분류분석은 각 그룹이 정의되어 있음
            - 교사학습(Supervise Learning)에 해당하는 예측기법
        - 2. 예측분석 정의  : 시계열 분석과 비슷하게 시간에 따른 값 두 개만을 이용해 매출, 온도 예측
            - 입력하는 데이터에 따라 특성이 다름
            - 한 개의 설명변수
        - 3. 분류분석 vs 예측분석
            - 공통점 : 레코드의 특정 속성 값을 미리 알아맞힘
            - 차이점 :

                분류 - 레코드의 범주형 속성 값을 예측

                예측 - 레코드의 연속형 속성 값을 예측

        - 4. 예시
            - 분류 : 학생들의 국,영, 수 점수를 통해 내신등급 알아맞힘, 카드회사에서 회원 가입정보를 통해 1년 후 신용등급 알아맞힘
            - 예측 : 학생들의 여러 가지 정보를 입력하여 수능점수 알아맞힘, 카드회사 회원 가입정보를 통해 연 매출액 알아맞힘
        - 5. 분류 모델링 : 신용평가모형(우량,불량), 사기방지모형(사기, 정상), 이탈모형(이탈, 유지), 고객세분화(VVIP, VIP, Gold, Silver, Bronze)
        - 6. 분류 기법 :
            - 회귀분석, 로지스틱 회귀분석(Logistic Regression)
            - 의사결정나무(Decision Tree), CART(Classification and Regression Tree), C5.0
            - 베이지안 분류(Bayesian Classification), Naive Bayesian
            - 인공신경망(ANN, artificial Neural Network)
            - 지지도벡터기계(SVM, Support Vector Machine)
            - k 최근접 이웃(KNN, K-Nearest Neighborhood)
            - 규칙기반의 분류와 사례기반추론(Case-Based Reasoning)
    - 2. 로지스틱 회귀분석(Logistic Regression)
        - 범주형 반응변수에 사용되는 회귀분석모형
        - 설명변수(예측변수) → 반응변수의 범주(집단)에 속할 확률 추정(예측모형) → 추정확률 기준으로 분류
        - 사후확률(Posterior Probability) : 모형 적합을 통해 추정된 확률
        - 식

            ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2051.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2051.png)

            ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2052.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2052.png)

            ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2053.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2053.png)

            ㄴLogit 변환(선형화됨)

        - $β_1$>0 → S자 모양, $β_1$<0 → 역 S자 모양
        - 로지스틱 분포의 누적분포함수(cumulative distribution function)를 F(x)라 할때
            - $P(y)=F(a+β_1x_1+...+β_kx_k)$ → 성공확률을 추정

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2054.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2054.png)

        - 선형회귀분석 vs 로지스틱 회귀분석
            - 종속변수 : 선형회귀 - 연속성, 로지스틱 -(0,1)
            - 모형 검정 : 선형회귀 - F-검정, T-검정, 로지스틱 - 카이제곱 검정($x^2$-test)
            - 계수추정법  : 선형회귀 - 최소제곱법, 로지스틱 - 최대우도추정법(MLE, Maximum Likelihood Estimation)

                ㄴ 최대우도추정법 : 표본을 바탕으로 미지의 모수 θ 추정, 우도(likelihood) : 주어진 표본을 비추어보고 모수θ 추정이 그럴듯한 정도를 말함

                우도 L(θ|x) ∝ p(x|θ) 비례함

        - glm(종속변수 ~ 독립변수1+..+독립변수k, family=binomial, data=데이터셋명)

            ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2055.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2055.png)

            ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2056.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2056.png)

    - 3. 의사결정나무
        - 1. 정의
            - 분류함수를 의사결정 규칙으로 이뤄진 나무 모양으로 그림
            - 연속적으로 발생하는 의사결정을 시각화해 의사결정 시점과 성과 확인 가능_해석 간편
            - 주어진 입력값에 대해 출력값을 예측하는 모형
            - 분류나무모형, 회귀나무모형
            - 구성요소
                - 뿌리마디(root node) : 시작되는 마디로 전체자료를 포함
                - 자식마디(child node) : 하나의 마디로부터 분리되어 나간 2개 이상의 마디들
                - 부모마디(Parent node) : 주어진 마디의 상위 마디
                - 끝마디(Terminal node) : 자식마디가 없는 마디
                - 중간마디(Internal node) : 부모마디와 자식마디가 모두 있는 마디
                - 가지(branch) : 뿌리마디로부터 끝마디까지 연결된 마디들
                - 깊이(depth) : 뿌리마디부터 끝마디까지의 중간마디들의 수
        - 2. 예측력과 해석력
            - 고객의 유치방안 예측 → 예측력에 치중
            - 결과에 대한 이유 설명 → 해석력 치중
        - 3. 활용
            1. 세분화 : 비슷한 특성을 갖은 그룹으로 분할해 그룹별 특성을 발견
            2. 분류 : 관측개체의 목표변수 범주를 몇 개의 등급으로 분류하고자 하는 경우 사용
            3. 예측 : 자료에서 규칙을 찾아 미래의 사건 예측
            4. 차원 축소 및 변수 선택 : 많은 예측 변수 중 목표변수에 큰 영향을 미치는 변수들을 골라냄
            5. 교호작용효과의 파악 : 예측변수들을 결합해 목표변수에 작용하는 규칙 파악, 범주의 병합 or 연속형 변수의 이산화

        - 4. 특징
            - 장점
                - 결과 설명이 용이
                - 모형 만들때 계산 복잡하지 않음
                - 대용량 데이터에서도 속도 빠름
                - 비정상 잡음 데이터에 대해서도 민감하지 않게 분류
                - 한 변수와 상관성이 높은 변수가 있어도 크게 영향 X
                - 설명변수, 목표변수에 수치형, 범주형 변수 모두 사용가능
                - 모형 분류 정확도가 높음
            - 단점
                - 새로운 자료에 대한 과대적합 발생 가능성 높음
                - 분류 경계선 부근 자료값에 대한 오차가 큼
                - 설명변수간 중요도 판단 어려움
        - 5. 분석과정 : 성장 → 가지치기 → 타당성평가 → 해석 및 예측
            1. 성장단계 : 각 마디에서 최적의 분리규칙(Splitting Rule)을 찾아 나무를 성장시키는 과정, 적절한 정지규칙(Stopping Rule)을 만족하면 중단
            2. 가지치기단계 : 오차 높일 위험이 있거나 부적절한 추론규칙을 가지고 있는가지, 불필요한 가지를 제거
            3. 타당성평가단계 : 이익도표(Gain Chart), 위험도표(Risk Chart), 시험자료를 이용하여 의사결정나무를 평가
            4. 해석 및 예측단계 : 구축된 나무모형을 해석, 예측모형 설정하여 적용
        - 6. 나무의 성장
            - 1. 분리규칙(Splitting Rule)
                - 분리변수(Split Variable)가 연속형인 경우 : $A=x-j≤s$
                - 분리변수 법주형 {1,2,3,4}인 경우, $A=1,2,4$와 $A^c=3$으로 나눌수 있다
                - 최적 분할의 결정은 불순도 감소량을 가장 크게하는 분할

                    $Δi(t)=i(t)-P_Li(t_L)-P_Ri(t_R), i(t) = ∑(y_i-y_t^-)^2$

                - 마디별로 동일한 과정을 반복

            - 2. 분리기준(Splitting Criterion)
                - 이산형 목표변수
                    - 카이제곱 통계량 p값 : P값이 가장 작은 예측변수와 그 때의 최적분리에 의해서 자식마디를 형성
                    - 지니 지수 : 지니 지수를 감소시켜주는 예측변수와 그때의 최적분리에 의해서 자식마디 선택
                    - 엔트로피지수 : 엔트로피 지수가 가장 작은 에측변수와 이 때의 최적분리에 의해 자식마디를 형성
                - 연속형 목표변수
                    - 분산분석에서 F-통계량 : P값이 가장작은 예측변수와 그 때의 최적분리에 의해서 자식마디 형성
                    - 분산의 감소량 : 분산의 감소량을 최대화 하는 기준의 최적분리에 의해 자식마디를 형성
            - 3. 정지규칙(Stopping Rule)
                - 더 이상 분리가 일어나지 않고 현재의 마디가 끝마디가 되도록 하는 규칙
                - 정지기준(Stopping criterion) : 의사결정나무의 깊이(depth)를 지정, 끝마디의 레코드 수의 최소 개수를 지정
        - 7. 나무의 가지치기(pruning)
            - 큰 나무모형 → 과대적합 가능성 높아짐, 작은 나무모형 → 과소적합 위험
            - 마디에 속하는 자료 수로 분할을 정지
            - 비용-복잡도 가지치기(Cost-Complexity pruning)을 이용하여 성장시킨 나무를 가지치기함
    - 4. 분순도의 여러 가지 측도
        - 목표변수가 범주형 변수인 의사결정 나무의 분류규칙을 선탣하기 위해서 카이제곱 통계량, 지니지수, 엔트로피 지수 활용

        1) 카이제곱 통계량 : ((실제도수-기대도수)의 제곱/기대도수)의 합으로 구함,(기대도수 = 열의 합계 * 합의 합계 / 전체합계)

        - 2) 지니지수 : 노드의 불순도 나타냄, (클수록 이질적이며, 순수도가 낮음)

            ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2057.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2057.png)

        - 3) 엔트로피 지수 : 클수록 순수도가 낮음, 엔트로피지수가 가장 작은 예측변수와 이때의 최적분리 규칙에 의해 자식마디 형성

            ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2058.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2058.png)

        - 예제

            ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2059.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2059.png)

            ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2060.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2060.png)

    - 5. 의사결정나무 알고리즘
        - CART(Classification and Regression Tree) : 많이 활용되는 의사결정나무 알고리즘, 불순도의 측도로 출력변수가 범주형일때 지니지수, 연속형일때 이진분리

            ㄴ 개별 입력변수 뿐만 아니라 입력변수들의 선형결합들중 최적의 분리 찾을 수 있음

        - C4.5와 C5.0 : CART와 다르게 각 마디에서 다지분리(Multiple split)가 가능, 범주형 입력변수에 대해서 범주의 수 만큼 분리가 일어남, 불순도는 엔트로피지수
        - CHAID(CHI-Squared Automatic Interaction Detection) : 가지치기 안하고 적당한 크기에서 나무모형을 중지, 입력변수는 반드시 범주형, 불순도는 카이제곱
- #### 3절 앙상블 분석
    - 1. 앙상블(Ensemble)
        - 정의
            - 여러 개의 예측모형을 만들고, 조합하여 하나의 최종 예측 모형을 만드는 방법
            - 다중 모델 조합(Combining multiple models), 분류기 조합(Classifier combination)
            - 앙상블 알고리즘

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2061.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2061.png)

        - 학습방법의 불안전성
            - 학습자료의 작은 변화에 예측 모형이 크게 변하는 경우, 그 학습방법은 불안정함
            - 가장 안정적인 방법으로는 1-Nearest Neighbor, 선형회귀모형
            - 가장 불안정한 방법은 의사결정나무
        - 앙상블 기법의 종류
            - 1. 배깅
                - breiman(1994)에 제안됨
                - 주어진 자료에서 여러개의 Bootstrap(붓스트랩)자료를 생성, 각 자료에 예측 모형을 만든 후 최종 예측모형 만듦
                - BootStrap : 주어진 자료에서 동일한 크기로 표본을 랜덤 복원 추출한 자료_한번도 선택되지 않는 원데이터 발생(약36.8%)
                - Voting : 여러 개의 모형으로 산출된 결과를 다수결에 의해 최종 결과 선정하는 과정
                - 최대로 성장한 의사결정 나무를 활용(의사결정나무에서는 가지치기가 가장 어렵지만 배깅에서는 가지치기 안함)
                - 훈련자료를 모집단으로 생각하고 평균예측모형을 구하여 분산을 줄이고 예측력 향상(실제 문제에서는 훈련자료의 모집단의 분포를 모르기 때문에 평균예측모형 구할 수 없음
            - 2. 부스팅
                - 예측력이 약한 모형(weak learner)들을 결합, 강한 예측모형을 만듦
                - Adaboost(Freund & Schapire가 제안) : 이진분류에서 랜덤분류기보다 더 좋은 분류기 n개에 가중치를 선정/결합하여 최종 분류기를 만듦
                - 훈련오차를 빠르고 쉽게 줄임
                - 배깅보다 예측오차 향상, 배깅보다 성능이 뛰어난 경우가 많음
            - 3. 랜덤 포레스트(Random Forest)
                - breiman(2001)에 개발됨
                - 의사결정나무의 분산이 크다는 점을 고려, 배깅과 부스팅보다 더 많은 무작위성을 주어 약한 학습기들을 생성, 선형결합하여 최종 학습기를 만듦
                - 무작위 입력에 따른 예측나무(forest of tree)를 이용한 분류
                - 랜덤한 Forest에는 많은 트리가 생성됨
                - 변수제거없이 실행되어 정확도 측면에서 좋음
                - 이론적 설명, 최종결과 해석에 어려움
                - 배깅, 부스팅과 비슷하거나 좋은 예측력
- #### 4절 인공신경망 분석
    - 1. 인공 신경망 분석(ANN, Artificial Nerual Network)
        - 1. 인간의 뇌를 기반으로 한 추론 모델, 뉴런은 가장 기본적인 처리 단위
        - 2. 인공신경망 연구
            - 1943년 매컬럭(McCulloch)과 피츠(Pitts) : 인간의 뇌를 수많은 신경세포가 연결된 하나의 디지털 네트워크 모형으로 간주, 신경세포의 신호처리 과정을 모형화하여 단순 패턴분류 모형을 개발
            - 헵(Hebb) : 신경세포(뉴런) 사이 연결강도(weight)를 조정하여 학습규칙 개발
            - 로젠블럿(Rosenblat,1955) : 퍼셉트론(Perceptron)이라는 인공세포 개발
            - 비선형성의 한계점 발생 -XOR(Exclusive OR)문제를 풀지 못하는 한계를 발견
            - 홉필드(Hopfild), 러멜하트(Rumelhart), 맥클랜드(McClenlland) : 역전파알고리즘(Backpropagation)을 활용하여 비선형성을 극복한 다계층 퍼셉트론으로 새로운 인공신경망 모형이 등장
        - 3. 인간의 뇌를 형상화한 인공신경망
            - 1. 인간뇌의 특징
                - 100억개의 뉴런과 6조개의 시냅스의 결합체,
                - 어떤 컴퓨터보다 빠르고, 매우 복잡, 비선형적, 병렬적 정보처리 시스템
                - 잘못된 답에 대한 뉴런들 사이 연결은 약화, 올바른 답에 대한 연결이 강화
            - 2. 인간의 뇌 모델링
                - 뉴런은 가중치가 있는 링크들로 연결됨
                - 뉴런은 여러 입력 신호를 받지만 출력은 오직 하나만 생성
        - 4. 인공신경망의 학습
            - 신경망은 가중치를 반복적으로 조정하며 학습
            - 뉴런은 링크로 연결되어 있고, 각 링크는 수치적인 가중치가 있음
            - 가중치를 초기화 하고, 훈련 데이터를 통해 가중치를 갱신하여 신경망의 구조를 선택, 활용할 학습 알고리즘을 결정한 후 신경망을 훈련시킴
        - 5. 인공신경망의 특징
            - 1. 구조
                - 입력 링크에서 여러 신호를 받아 새로운 활성화 수준을 계산, 출력 링크로 출력 신호를 보냄
                - 입력신호는 미가공 데이터 또는 다른 뉴런의 출력이 될 수 있다.
                - 출력 신호는 문제의 최종적인 해(Solution)가 되거나 다른 뉴런에 입력 될 수 있다.
            - 2. 뉴런의 계산
                - 뉴런은 전이함수, 즉 활성화 함수(Activation Function)를 사용
                - 활성화 함수를 이용해 출력을 결정하며 입력신호의 가중치 합를 계산하여 입계값과 비교
                - 가중치 합이 입계값보다 작으면 뉴런 출력은 -1, 크거나 같으면 +1출력
                - 함수

                    ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2062.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2062.png)

            - 3. 뉴런의 활성화 함수
                - 시그모이드 함수는 로지스틱 회귀분석과 유사 0~1의 확률값을 가짐
                - 관련 함수

                    ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2063.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2063.png)

                - Softmax함수 : 표준화지수로 불림, 출력값이 여러개로 주어지고, 목표치가 다범주인 경우 각 범주에 속할 사후확률을 제공하는 함수
                - Relu함수 : 입력값이 0 이하는 0, 0이상은 x값을 가지는 함수, 최근 딥러닝에서 많이 활용하는 활성화 함수
            - 4. 단일 뉴런의 학습(단층 퍼셉트론)
                - 퍼셉트론은 선형 결합기와 하드 리미터로 구성
                - 초평면(hyperplane)은 n차원 공간을 두개의 영역으로 나눔
                - 초평면을 선형 분리 함수로 정의

                    ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2064.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2064.png)

                    ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2065.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2065.png)

        - 6. 신경망 모형 구축시 고려사항
            - 1. 입력변수
                - 자료 선택에 매우 민감
                - 범주형 입력 변수 : 모든 범주에서 일정 빈도 이상의 값을 갖고 각 범주의 빈도가 일정
                - 연속형 입력 변수 : 입력 변수 값들의 범위가 변수간의 큰 차이가 없을 때
                - 연속형 변수의 경우 분포가 평균을 중심으로 대칭이 아니면  결과가 안좋음
                - 보완 방법 : 변환(대부분 평균 미만, 소수 매우큼 → 로그변환), 범주화(각범주 비슷하도록 설정)
                - 범주형 변수의 경우 가변수화 하여 적용(남녀 : 1,0)하고 가능하면 모든 범주형 변수는 같은 범위를 갖도록 가변수화 하는 것이 좋음
                - 
            - 2. 가중치의 초기값과 다중 최소값 문제
                - 역전파 알고리즘은 초기값에 따라 결과가 많이 달라짐, 초기값 선택 매우 중요
                - 가중치가 0이면 시그모이드 함수는 선형이 되고 신경망 모형은 근사적으로 선형모형됨
                - 일반적으로 초기값은 0 근처로 랜덤하게 선택하여 초기 모형은 선형모형에 가깝고, 가중치 값이 증가할 수록 비선형 모델이됨
            - 3. 학습모드
                - 1. 온라인 학습모드(Online learning Mode)
                    - 각 관측값을 순차적으로 하나씩 신경망에 투입하여 가중치 추정값이 매번 바뀐다
                    - 일반적으로 속도가 빠름(훈련자료에 유사한 값이 많은 경우 더 빨라짐)
                    - 비정상성(Nonstationarity) 같은 특이한 성질을 가진 경우가 좋음
                    - 국소최솟값에서 벗어나기 더 쉬움
                - 2. 확률적 학습모드(Probabilistic Learning Mode)
                    - 투입되는 관측값 순서가 랜덤(나머진 온라인 학습모드와 동일)
                - 3. 배치 학습 모드(Batch Learning Mode)
                    - 전체 훈련자료를 동시에 신경망에 투입

                ※ 학습률 : 처음에 큰 값으로 정하고 반복 수행과정을 통해 해에 가까울 수록 학습률이 0에 수렴

            - 4. 은닉층(Hidden Layer)와 은닉 노드(Hidden Node)의 수
                - 신경망을 적용할 때 가장 중요한 부분이 모형의 선택이다(은닉층의 수와 은닉노드의 수 결정)
                - 은닉충과 은닉노드가 많으면 가중치가 많아져서 과대 적합문제 발생
                - 은닉충과 은닉노드가 적으면 과소적합 문제 발생
                - 은닉충의 수가 하나인 신경망은 범용 근사자(universal approximator)로 모든 매끄러운 함수를 근사적으로 표현할 수 있어, 가능하면 1개로 선정
                - 은닉노드의 수는 적절히 큰 값으로 놓고 가중치를 감소(weight decay) 시키며 적용하는 것이 좋다
            - 5. 과대 적합문제
                - 신경망에서는 많은 가중치를 추정해야 하므로 과대적합 문제가 빈번함
                - 알고리즘 조기 종료와 가중치 감소 기법으로 해결 가능
                - 모형이 적합하는 과정에서 검증오차가 증가하기 시작하면 반복을 중지하는 조기종료를 시행
                - 선형모형의 능형회귀와 유사한 가중치 감소라는 벌점화 기법 활용
        - 딥러닝(Deep learning) : 인공신경망의 한계를 극복하기 위해 제안된 심화 신경망(Deep Neural Network)를 활용한 방법
        - 딥러닝소프트웨어 : Tensorflow, caffe, theano, MXnet
        - 활용 :  음성인식, 이미지 인식, 자연어 처리, 헬스케어

- #### 5절 군집분석
    - 1. 군집분석
        - 1. 개요
            - 객체의 유사성을 측정, 유사성이 높은 집단을 분류, 군집내 객체들의 유사성과, 다른 군집에 속한 객체간의 상이성을 규명하는 분석 방법
            - 특성에 따라 여러 개의 배타적인 집단으로 나눔
            - 군집분석 방법에 따라 결과 차이 발생
            - 군집의 개수나 구조에 대한 가정 없이 데이터 사이의 거리를 기준으로 군집화
            - 소비자들의 상품구매행동이나 Life style에 따른 소비자군 분류
            - 군집분석 개요도

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2066.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2066.png)

        - 2. 특징
            - 요인분석과의 차이점 : 요인분석은 유사한 변수를 묶어주는 것이 목적
            - 판별분석과의 차이점 : 판별분석은 사전에 집단이 나누어져 있는 자료를 통해 새로운 데이터를 기존의 집단에 할당 하는것이 목적
            - 밀도기반 군집 : DBSCAN, DENCLUE 기법등 임의적인 모양의 군집탐색에서 효과적인 방법
            - 품질 평가 : 실루엣(Shilouette)지표 사용- 군집내 데이터 응집도 군집간 데이터 분리도를 계산하여 응집도가 크고 분리도가 크면 큰값을 갖음 완벽한 분리시 1값
    - 2. 거리 : 유사성이나 근접성을 측정해 어느 군집으로 묶을 수 있는지 판단
        - 1. 연속형 변수의 경우
            - 유클리디안 거리(Enclidean Distance) : 데이터간 유사성을 측정시 사용, 통계적 개념 포함안됨, 변수들의 산포정도가 전혀 감안 안됨

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2067.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2067.png)

            - 표준화 거리(statistical distance) : 해당변수의 표준편차로 척도 변환 후 유클리드안 거리를 계산하는 방법, 표준화하면 척도/분산의 차이로 인한 왜곡 피할 수있다

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2068.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2068.png)

            - 마할라노비스(Mahalanobis) 거리 : 통계적 개념 포함, 변수간 산포 고려하여 표준화한 거리, 두 벡터사이의 거리를 산포를 의미하는 표본공분산으로 나눠줌, 그룹에 대한 사전 지식 없이는 표본공분산S를 계산할 수 없어 사용하기 곤란

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2069.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2069.png)

            - 체비셰프(chebychav) 거리

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2070.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2070.png)

            - 맨하탄(Manhattan) 거리 : 유클리디안 거리와 함께 많이 사용됨, 건물에서건물을 가기위한 최단거리를 구하기 위해 고안(맨하탄도시에서)

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2071.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2071.png)

            - 캔버라(Canberra) 거리

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2072.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2072.png)

            - 민코우스키(Minkowski) 거리 : 맨하탄 거리와 유클리디안 거리를 한번에 표현한 공식,L1거리(맨하탄거리), L2거리(유클리디안 거리)

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2073.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2073.png)

        - 2. 범주형 변수의 경우
            - 자카드 거리 : Boolean 속성으로 이루어진 두 객체 간 유사도 측정

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2074.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2074.png)

            - 자카드 계수

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2075.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2075.png)

            - 코사인 거리 : 문서를 유사도 기준으로 분류, 혹은 그룹핑 할 때 유용하게 사용

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2076.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2076.png)

            - 코사인 유사도 : 두 개체의 벡터 내적의 코사인 값을 이용하여 측정된 벡터간 유사한 정도

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2077.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2077.png)

            - 코사인 거리 예제

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2078.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2078.png)

    - 3. 계층적 군집분석 : n개의 군집으로 시작, 점차 군집의 개수를 줄여나감, 합병형 방법(Agglomerative:bottom-up), 분리형 방법(Divisive-down)
        - 최단연결법(Single Linkage, Nearest Neighbor)
            - n*n거리 행렬에서 거리가 가장 가까운 데이터를 묵어 군집화
            - 군집과 군집 또는 데이터와의 거리를 계산,최단 거리를 거리로 계산하여 거리행렬 수정
            - 수정된 거리행렬에서 거리에 가까운 데이터 또는 군집을 새로운 군집으로 형성
            - 예시

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2079.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2079.png)

        - 최장연결법(Complete linkage, Farthest neighbor)
            - 군집과 군집, 데이터와 거리를 계산시 최장거리로 계산하여 거리행렬 수정
            - 예시

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2080.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2080.png)

        - 평균연결법(Average Linkage)
            - 군집과 군집 또는 데이터와 거리를 계산시 평균(mean)을 거리로 계산하여 거리행렬 수정
            - 예시

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2081.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2081.png)

        - 와드연결법(Ward Linakge)
            - 군집내 편차들을 제곱합을 고려
            - 군집 간 정보손실 최소화를 위해 군집화 진행
        - 군집화
            - 거리를 통해 객체들간의 관계를 구명, 덴드로그램을 그림
            - 덴드로그램을 보고 군집의 개수를 변화해 가며 적절한 군집 수 선정
            - 군집의 수는 분석 목적에 따라 선정가능, but 5개 이상의 군집은 잘 활용하지 않음
            - 군집화 단계

                1) 거리행렬을 기준으로 덴드로그램을 그린다. 

                2) 덴드로그램의 최상단부터 세로축의 개수에 따라 가로선을 그어 군집의 개수를 선택한다.

                3) 각 객체들의 구성을 고려해 적절한 군집수를 선정한다. 

    - 4. 비계측정 군집분석 : n개의 개체를 g개의 군집으로 나눌 수 있는 모든 가능한 방법을 점검해 최적화한 군집을 형성
        - K-평균 군집분석(K-Means Clustering)의 개념
            - 주어진 데이터를 K개의 클러스터로 묶은 알고리즘, 각 클러스터와 거리 차이의 분산을 최소화 하는 방식으로 동작
        - K-평균 군집분석(K-Means Clustering) 과정
            - 원하는 군집의 개수와 초기값들을 정해 Seed중심으로 군집을 형성
            - 각 데이터를 거리에 가장 가까운 Seed가 있는 구집으로 분류
            - 각 군집의 seed값을 다시 계산한다.
            - 모든 개체가 군집으로 할당된 때까지 위 귀정을 반복한다.

        - K-평균 군집분석 특징
            - 거리 계산을 통해 연속형 변수에 활용이 가능
            - k개의초기 중심값은 임의로 선택이 가능, 가급적이면 멀리 떨어지는게 바람직
            - 초기 중심값을 임의로 선택 시, 일렬로 선택하면 군집이 혼합되지 않고 층으로 나눠질 수 있음
            - 초기 중심으로부터의 오차제곱합을 최소화하는 방향으로 군집이 형성되는 탐욕적 알고리즘으로 안정된 굽집은 보장하나 최적은 보장하지 않음
            - 장점
                - 알고리즘 단순, 빠르게 수행
                - 계측적 군집분석에 비해 많은 데이터를 다룰 수 있음
                - 내부구조에 대한 사전정보가 없어도 의미있는 자료구조를 다 찾을 수 있음
                - 다양한 형태의 데이터에 적용이 가능
            - 단점
                - 군집의 수 가중치와 거리 정의 어롭다.
                - 사전에 주어진 목적이 없으므로 결과 해석 어려움
                - 잡음이나 이상값 영향을 많이 받음 → 극복을 위한 k-median 군집의 PAM(Partitoning Around Medoids) 함수 사용 → 평균값이 아니라 중앙값을 이용
                - 볼록형태가 아닌 (Nov-convex) 군집이(예를 들어 U 형태의 군집) 존재할 경우 성능이 떨어짐,초기 군집수 결정에 어려움이 있다.
                - 
    - 5. 혼합 분포 군집(Mixture Distribution Clustering)
        - 개요
            - 모형기반(Model-Based)의 군집방법
            - 데이터가 k개의 모수적 모형(흔히 정규분포, 다변량 정규분포를 가정)의 가중합으로 표현되는 모집잔 모형으로 나왔다는 가정하에서 모수와 함께 가중치를 자료로부터 추정하는 방법 사용
            - k개의 각 모형은 군집을 의미하며, 각 데이터는 추정된 k개의 모형 중 어느 모형으로부터 나왔을 확률이 높은지에 따라 군집의 분류가 이뤄짐
            - 흔히 혼합모형에서의 모수와 가중치의 추정(최대가능도추정)에는 EM 알고리즘이 사용됨
        - 혼합 분포모형으로 설명할 수 있는 데이터의 형태

            ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2082.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2082.png)

            (a) 다봉형 형태로 단일 분포로의 적합은 적절하지 않음, 대략 3개 정도의 정규분포 결합을 통해 설명 가능

            (b) 여러 개의 이변량 정규분포의 결합을 통해 설명 가능

            ⇒ (a), (b) 모두 정규분포로 제한할 필요는 없다

        - EM(Expectation-Maximization) 알고리즘의 진행 과정
            - k개의 각 모형의 가중합으로 되어있다는 가정 하에서 이 혼합모형의 모수와 가중치의 최대가능도(Maximum likelihood)를 추정하는 알고리즘
            - 각 자료에 대해 Z의 조건부 분포로 부터 조건부 기대값 구할 수 있음
            - 관측변수X와 잠재변수Z를 포함하는 (X,Z)에 대한 로그-가능도함수에 Z 대신 상수값인 Z의 조건부 기댓값을 대입하면, 로그-가능도함수를 최대로 하는 모수를 찾을 수 있음, (M-단계)
            - 갱신된 모수 추정치에 대해 위 과정을 반복한다면 수렴하는 값을 얻게 되고, 이는 최대 가능도 추정치로 사용될 수 있음
            - E-단계 : 잠재변수 Z의 기대치 계산
            - M-단계 : 잠재변수 Z의 기대치를 이용하여 파라미터 추정
            - EM 알고리즘 순서도

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2083.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2083.png)

        - EM 알고리즘의 진행 과정
            - 진행과정

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2084.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2084.png)

            - 혼합분포 군집모형의 특징
                - K-평균군집의 절차와 유사하지만, 확률분포를 도입하여 군집을 수행
                - 군집을 몇 개의 모수로 표현가능, 서로 다른 크기나 모양의 군집을 찾을 수 있음
                - EM 알고리즘을 이용한 모수 추정에서 데이터가 커지면 수렴에 시간이 걸릴 수 있음
                - 군집의 크기 너무 작으면 추정의 정도가 떨어지거나 어려울 수 있음
                - K-평균군집과 같이 이상치 자료에 민감함으로 사전에 조치가 필요
    - 6. SOM(Self Organizing Map)
        - 개요
            - 코호넨(Kohonen)에 의해 제시, 개발되어 코호넨 맵이라고도 알려짐
            - 비지도 신경망
            - 고차원 데이터를 이해하기 쉬운 저차원 뉴런으로 정렬, 지도형태로 형상화
            - 입력변수의 위치 관계를 그대로 보존(실제 공간에 입력변수가 가까이 있으면 지도상에도 가까운 위치에 있음)
        - 구성
            - 2개의 인공 신경망 층으로 구성(입력층, 경쟁층)
            - 입력층(Input layer : 입력벡터를 받는 층)
                - 입력 변수의 개수와 동일하게 뉴런의 수가 존재
                - 입력층의 자료는 학습을 통해 경쟁층에 정렬됨 → 지도(Map)
                - 입력층과 경쟁층의 각각의 뉴런들과 연결되어 있음 → 완전 연결(fully connected)
            - 경쟁층(Competitive Layer : 2차원 격자(grid)로 구성된 층)
                - 입력 벡터의 특성에 따라 벡터가 한점으로 클러스터링 되는 층
                - SOM은 경쟁학습으로 각각의 뉴런이 입력벡터와 얼마나 가까운가를 계산, 연결강도(Connection weight)를 반복적으로 재조정하여 학습
                - 연결강도는 입력 패턴과 가장 유사한 경쟁층 뉴런이 강해짐
                - 입력층의 표본벡터에 가장 가까운 프로토타입 벡터를 선택해 BMU(Best-Matching-Unit)라고 불림
                - 코호넨의 승자 독점의 학습규칙에 따라 위상학적 이웃(Topological neighbors)에 대한 연결강도 조정
                - 승자 독식구조로 경쟁층은 승자 뉴런만 나타남, 승자와 유사한 연결 강도를 갖는 입력 패턴이 동일한 경쟁 뉴런으로 배열됨
                - 모식도

                    ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2085.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2085.png)

        - 특징
            - 고차원의 데이터를 저차원의 지도형태로 형상화하여 이해하기 쉬움
            - 입력 변수의 위치관계를 그대로 보존, 실제 데이터가 유사하면 지도상에서 가깝게 표현(패턴발견, 이미지 분석 등 에서 뛰어난 성능을 보임)
            - 역전파(Back propagation)알고리즘 등을 이용하는 인공신경망과 달리 단 하나의 전방 패스(feed-forward flow)를 사용하여 속도가 매우 빠름,
            - 실시간 학습처리 할 수 있는 모형
        - SOM과 신경망 모형의 차이점
            - 학습방법 : 신경망 - 오차역전파법, SOM -경쟁학습방법
            - 구성 : 신경망 - 입력층, 은닉층, 출력층, SOM - 입력층, 2차원 격자형태의 경쟁층
            - 기계학습 방법의 분류 : 신경망 - 지도학습, SOM - 비지도학습
    - 7. 최신 군집분석 기법들
        - Hierarchical clustering
        - k-means clustering
- #### 6절 연관분석
    - 1. 연관규칙
        - 1. 연관규칙분석(Association Analysis)의 개념
            - 연관성 분석은 장바구니분석(Market basket Analysis) 또는 서열 분석(Sequence Analysis)라고 불림
            - 기업의 데이터베이스에서 상품의 구매, 서비스등 일련의 거래 또는 사건들 간의 규칙 발견을 위해 적용
            - 장바구니 분석 : 장바구니에 무엇이 같이 들어있는지에 대한 분석
            - 서열분석 : A를 산 다음에 B를 산다
        - 2. 연관규칙의 형태
            - 조건과 반응의 형태(if-then)로 이루어져 있다(아메리카노를 마시는 손님 중 10%가 브라우니를 먹는다)
        - 3. 연관규칙의 측도
            - 산업의 특성에 따라 지지도, 신뢰도, 향상도 값을 잘 보고 규칙을 선택
            - 지지도(Support) : 전체 거래중 항목 A와 항목 B를 동시에 포함하는 거래의 비율로 정의

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2086.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2086.png)

            - 신뢰도(Confidence) : 항목 A를 포함한 거래중, 항목 A와 항목 B가 같이 포함될 확률, 연관성의 정도 파악 가능

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2087.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2087.png)

            - 향상도(Lift) : A가구매되지 않았을 때 품목 B의 구매확률에 비해 A가 구매 됐을때 품목 B의 구매 확률의 증가비

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2088.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2088.png)

            - 예제

                ![ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2089.png](ADsP%20%E1%84%82%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2081e03de5bd9d480e87c0df3f7fc07cdd/Untitled%2089.png)

        - 4. 연관규칙의 절차
            - 최소 지지도보다 큰 집합만을 대상으로 높은 지지도를 갖는 품목 집합을 찾는 것
            - 초기 5%로 잡고 규칙이 충분히 도출되는지 확인, 다양하게 조절하며 시도
            - 처음부터 너무 낮은 최소 지지도를 선정하는 것은 많은 리소스 소모
            - 절차 : 1) 최소지지도 결정 → 2) 품목중 최소 지지도 넘는 품목 분류 → 3) 2가지 품목 집합 생성 → 4) 반복적으로 수행, 빈발품목 집합을 찾음
        - 5. 연관규칙의 장점과 단점
            - 장점
                - 탐색적인 기법을 사용하여 조건반응으로 표현되는 연관성 분석의 결과를 쉽게 이해
                - 강력한 비목적성 분석 기법, 분석 방향이나 목적이 특별히 없는 경우 목적변수가 없으므로 유용하게 사용가능
                - 사용이 편리한 분석데이터의 형태로 거래 내용에 대한 데이터 변환없이 그 자체로 이용
                - 분석을 위한 계산이 간단
            - 단점
                - 품목수가 증가하면 분석에 필요한 계산 기하급수적으로 늘어남 →  개선방법 : 유사품목을 한 범주로 일반화,신뢰도 하한 새로 정의하여 의미가 적은 연관규칙은 제외
                - 너무 세분화한 품목에 대한 연관성 규칙은 의미없는 분석이 될 수 있음 → 적절히 구분이 되는 큰 범주로 구분해 전체 분석에 포함시킨 후 결과 중 세부적으로 연관규칙을 찾는 작업을 수행
                - 거래량이 적은 품목은 규칙 발견시 제외되기 쉬움 → 중요품목일 경우 유사한 품목들과 함께 범주로 구성하여 포함
        - 6. 순차패턴(Sequence Analysis)
            - 동시에 구매될 가능성이 큰 상품군을 찾아내는 연관성 분석에 시간이라는 개념을 포함시켜 순차적으로 구매 가능성이 큰 상품군 찾아냄
            - 연관성분석에서 데이터 형태에서 각각의 고객으로부터 발생한 구매 시점에 대한 정보가 포함
    - 2. 기존 연관성분석의 이슈
        - 대용량 데이터에 대한 연관성 분석이 불가
        - 시간이 많이 걸리거나 시스템 다운 현상 발생
    - 3. 최근 연관성분석 동향
        - 1세대 알고리즘인 Apriori나 2세대인 FP-Growth에서 발전하여 3세대의 FPV를 이용해 메모리 효율을 높여  SKU레벨의 연관성분석을 성공적으로 적용
        - 품목 개수가 n개 일때 품목 전체집합에서 추출할 수 있는 부분집합의 개수는 $2^n-1$(공집합제외)개이다. 그리고 가능한 연관규칙 개수는 $3^n-2^(n+1)+1$ 개이다
        - Apriori는 품목 부분집합개수를 줄이는 방식으로 작동하는 알고리즘,
        - FP-Growth는 거래내역 안에 포함된 품목의 개수를 줄여 비교하는 횟수를 줄이는 방식
        - Apriori 알고리즘
            - 빈발항목집합(frequenct item set) : 최소 지지도보다 큰 지지도 값을 갖는 품목의 집합
            - 최소 지지도 이상의 빈발항목집합을 찾은 후 그것들에 대해서만 연관규칙을 계산
            - Apriori는 1994년 발표된 1세대 알고리즘으로 구현과 이해하기가 쉽다는 장점이 있음
            - 지지도가 낮은 후도 집합 생성 시 아이템의 개수가 많아지면 계산복잡도가 증가한다는 문제점이 있음
        - FP-Growth 알고리즘
            - FP-Growth 알고리즘 : 후보 빈발항목집합 생성X, FP-Tree(frequent Pattern Tree)를 만든 후 분할정복방식을 통해 Apriori알고리즘보다 빈발항목집합을 추출할 수 있는 방법
            - Apriori 알고리즘의 약점을 보완하기 위해 고안된 것
            - 데이터 베이스 스캔하는 횟수가 작고, 빠른속도 분석 가능
    - 4. 연관성분석 활용방안
        - 장바구니 분석의 경우 실시간 상품추천을 통한 교차판매에 응용
        - 순차패턴 분석은 A를 구매한 사람인데 B를 구매하지 않은 경우 B를 추천하는 교차판매 캠페인에 사용